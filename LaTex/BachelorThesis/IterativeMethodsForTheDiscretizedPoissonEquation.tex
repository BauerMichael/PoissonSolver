\input{IterativeMethodsForTheDiscretizedPoissonEquation_settings.tex} % Importiere die Einstellungen aus der Präambel
% hier beginnt der eigentliche Inhalt

% \usepackage{geometry}
% \geometry{tmargin=3cm,bmargin=3cm,lmargin=4cm,rmargin=2cm}

\begin{document}
\pagenumbering{Roman} % große Römische Seitenummerierung
\pagestyle{empty}

% Titelseite
\clearscrheadings\clearscrplain

\begin{center}
\includegraphics[width=6cm]{images/uniR.png}

\begin{huge}
UNIVERSITÄT REGENSBURG
\vspace{10mm}
\end{huge}

{\Large \textbf{Fakultät für Mathematik}}
\vspace{0mm}

{\Large Bachelor of Science Computational Science}

\vspace{10mm}
\begin{huge}
Bachelorarbeit
\end{huge}

\textit{zur Erlangung des akademischen Grades eines Bachelor of Science (B. Sc.)}

\vspace{10mm}

\begin{Large}
Iterative Verfahren zur Lösung der diskretisierten Poisson Gleichung
\end{Large}

\begin{large}
in der angewandten Mathematik.  
\end{large}

\vspace{5mm}
\begin{small}
von
\end{small}

\begin{large}
Michael Bauer
\end{large}

\begin{small}
Matrikelnummer: 152 8558
\end{small}

\vspace{1cm}
\begin{tabular}{ll}
{\bf Betreuer:} &Prof. Dr. Harald Garcke\\
{\bf Eingereicht am:} &20.03.2014\\
\end{tabular}

\end{center}
\clearpage

\chapter*{Erklärung}

\textbf{Eidesstattliche Erklärung zur Bachelorarbeit}

Ich habe die Arbeit selbstständig verfasst, keine anderen als die angegebenen Quellen und Hilfsmittel benutzt und bisher keiner anderen Prüfungsbehörde vorgelegt. Außerdem bestätige ich hiermit, dass die vorgelegten Druckexemplare und die vorgelegte elektronische Version der Arbeit identisch sind, dass ich über wissenschaftlich korrektes Arbeiten und Zitieren aufgeklärt wurde und dass ich von den in § 26/27 Abs. 5 vorgesehenen Rechtsfolgen Kenntnis habe.

\vspace{3cm}
Ort, Datum \hspace{5cm} Unterschrift\\


\pagestyle{useheadings} % normale Kopf- und Fußzeilen für den Rest

\tableofcontents
\listoffigures
\listoftables

% richtiger Inhalt
\chapter{Einleitung}\label{c.Einleitung}
\pagenumbering{arabic} % ab jetzt die normale arabische Nummerierung

Viele Prozesse in den Naturwissenschaften, wie Biologie, Chemie und Physik, aber auch der Medizin und Wirtschaft lassen sich auf partielle Differentialgleichungen zurückführen. Das Lösen solcher Gleichungen ist allerdings nicht immer möglich, oder aufwendig. \\
Eine partielle Differentialgleichung, die vor allem in der Physik häufige Verwendung findet, ist die Poisson-Gleichung. Sie stellt eine elliptische partielle Differentialgleichung zweiter Ordnung dar. So genügt beispielsweise das elektrostatische Potential $u$ zu gegebener Ladungsdichte $f$, oder das Gravitationspotential $u$ zu gegebener Massendichte $f$ dieser Gleichung. Wie man sieht, hängt diese Gleichung mit ganz elementaren Dingen unseres Lebens zusammen. \\
Methoden aus der numerischen Mathematik ermöglichen das Lösen von partiellen Differentialgleichungen mittels computerbasierten Algorithmen. Es wird jedoch nicht die Lösung direkt bestimmt, sondern versucht eine exakte Approximation der Lösung zu erhalten. Dabei ist es wichtig, dass der zugrunde liegende Algorithmus effizient, also durch Stabilität und geringem Rechenaufwand gekennzeichnet ist. \\
Unser Ziel ist es nun, solche Algorithmen herzuleiten und in Programmiercode umzusetzen, damit wir die oben genannte Poisson Gleichung im zweidimensionalen Raum lösen können. Bevor man die Lösung einer partiellen Differentialgleichung berechnen kann, versucht man zunächst die Gleichung auf ein lineares Gleichungssystem $\mat{A} u = f$ zurückzuführen. Eine der zentralen Methoden der Numerik sind Finite-Differenzen. Hierbei diskretisiert man das Gebiet, auf dem die partielle Differentialgleichung definiert ist und kann dann die Gleichung auf ein lineares Gleichungssystem der Form $\mat{A} u = f$ zurück führen. \\
Da es eine Reihe von Methoden zur Lösung von linearen Gleichungssystemen gibt, stellt sich natürlich die Frage, welche die effizienteste für die gegebene Problemstellung ist. Eine Möglichkeit, ein lineares Gleichungssystem zu lösen, sind iterative Verfahren. Sie zeichnen sich dadurch aus, dass sie die approximierte Lösung schrittweise nähern. Dabei werden pro Iterationsschritt nur endlich viele Rechenoperationen benötigt. Beispiele, die in dieser Arbeit diskutiert werden, sind das Jacobi-Verfahren oder das Verfahren der konjugierten Gradienten. Letzteres liefert ein großartiges Werkzeug, um eine Lösung innerhalb weniger Iterationsschritte zu berechnen. \\
Wie wir sehen werden, ist das Jacobi-Verfahren zwar ein iteratives Verfahren, jedoch als solches ungeeignet. Durch eine Modifikation erhalten wir das Jacobi-Relaxationsverfahren, welches ebenfalls nicht als ein iteratives Verfahren geeignet ist, allerdings die sogenannte Glättungseigenschaft besitzt. Diese Eigenschaft findet ihre Anwendung in den Mehrgittermethoden, welche sich durch schnelle Konvergenz und geringe Rechenzeit auszeichnen. Gerade für die Poisson-Gleichung liefern die Mehrgittermethoden innerhalb weniger Sekunden eine sehr gute Approximation der Lösung.\\
Um einen Vergleich dieser Verfahren zu erhalten, wollen wir abschließend diese miteinander vergleichen. Dabei betrachten wir, basierend auf einem selbst entwickelten C++-Algorithmus, die Iterationsschritte für die iterativen Verfahren und die Anzahl der Zyklen bei den Mehrgittermethoden. Zusätzlich soll uns die Rechenzeit als Referenz beim Vergleich der Verfahren dienen und uns deutliche Unterschiede erkennen lassen. So werden wir sehen, dass speziell für die Poisson Gleichung die Mehrgittermethoden die stärksten Algorithmen darstellen.


\chapter{Diskretisierung der Poisson-Gleichung im $\mathbb{R}^{2}$} \label{c.Diskretisierte Poisson-Gleichung}

\section{Definition (Poisson-Gleichung)}\label{s.Poisson-Gleichung}

Sei $\Omega = (0,1)\times(0,1) \in \mathbb{R}^{2}$ ein beschränktes, offenes Gebiet. Gesucht wird eine Funktion $u(x,y)$, die das Randwertproblem
\begin{eqnarray}
-\Delta u(x,y) &=& f(x,y) \textnormal{ in } \Omega, \\
u(x,y) &=& g(x,y) \textnormal{ auf } \partial \Omega\label{eq.Dirichlet1}
\end{eqnarray}
löst.
Dabei seien $f: \Omega \rightarrow \mathbb{R}$ und $g: \partial\Omega \rightarrow \mathbb{R}$ stetige Funktionen und es bezeichnet $\Delta = \sum\limits_{k=1}^{2} \frac {\partial^{2}} {\partial x_{k}^{2}}$ den Laplace-Operator. Für die Poisson-Gleichung im $\mathbb{R}^{2}$ gilt dann:
\begin{eqnarray}
-\Delta u(x,y) &=& - \left( \frac {\partial^{2} u(x,y)} {\partial x^{2}} + \frac {\partial^{2} u(x,y)} {\partial y^{2}} = f(x,y) \right) \textnormal{ in } \Omega, \\
u(x,y) &=& g(x,y) \textnormal{ auf } \partial \Omega.\label{eq.Dirichlet2}
\end{eqnarray}
\autoref{eq.Dirichlet1} bzw. \autoref{eq.Dirichlet2} nennt man Dirichlet-Randbedingung.\\

\textbf{Beachte:} $\partial_{xx}u(x,y) = \frac {\partial^{2}u(x,y)} {\partial x^{2}}$ und $\partial_{x}u(x,y) = \frac {\partial u(x,y)} {\partial x}$.

Um diese (elliptische) partielle Differentialgleichung nun in $\Omega$ zu diskretisieren, bedarf es der Hilfe der Finiten-Differenzen-Methode.

\section{Finite-Differenzen-Methode und Diskretisierung von $\Omega$}\label{s.Finite Differenzen}

Zunächst wollen wir den zentralen Differenzenquotienten (zweiter Ordnung) einführen, Mithilfe dessen wir bei der Diskretisierung eine Matrix $\mat{A}$ erhalten werden. Diese wollen wir auf ihre Eigenschaften untersuchen.

\subsection{Zentraler Differenzenquotient zweiter Ordnung}\label{ss.Differenzenquotient zweiter Ordnung}

In diesem Abschnitt folgen wir \cite{ALO1}.\\
Wir betrachten ein $(x,y) \in \Omega$ beliebig. Dann gilt für $u(x,y)$ mit $h > 0$ und der Taylorformel

\begin{equation}
u(x+h,y) = u(x,y) + h \partial_{x} u(x,y) + \frac {h^{2}} {2!} \partial_{xx} u(x,y) + \frac{h^{3}}{3!} \partial_{xxx} u(x,y) + \frac{h^{4}}{4!} \partial_{xxxx} u(\xi,y),\label{eq.Partiall x+h}
\end{equation}
\begin{equation}
u(x-h,y) = u(x,y) - h \partial_{x} u(x,y) + \frac {h^{2}} {2!} \partial_{xx} u(x,y) - \frac{h^{3}}{3!} \partial_{xxx} u(x,y) + \frac{h^{4}}{4!} \partial_{xxxx} u(\zeta,y),\label{eq.Partiall x-h}
\end{equation}
wobei $\xi$ und $\zeta$ jeweils unbekannte Zwischenstellen zwischen $x$ und $x \pm h$ sind. Addieren wir beide Gleichungen und teilen durch $h^{2}$, so ergibt dies:
\begin{equation}
\partial_{xx} u(x,y) + O(h^{2}) = \frac {u(x-h,y) - 2u(x,y) + u(x+h,y)} {h^{2}}.
\end{equation}

Analog können wir für $u(x,y+h)$ und $u(x,y-h)$ vorgehen und erhalten:
\begin{equation}
\partial_{yy} u(x,y) + O(h^{2}) = \frac {u(x,y-h) - 2u(x,y) + u(x,y+h)} {h^{2}}.
\end{equation}

Diese Näherungen nennt man den \textbf{zentralen Differenzenquotienten der zweiten Ableitung}. $O(h^{2})$ ist ein Term zweiter Ordnung und wird vernachlässigt.

Somit erhalten wir für $-\Delta u(x,y)$ die Näherung
\begin{equation}
-\Delta u(x,y) = \frac {u(x-h,y) - u(x+h,y) + 4u(x,y) - u(x,y-h) - u(x,y+h)} {h^{2}}.\label{eq.Differenzenquotienten}
\end{equation}

\subsection{Diskretisierung von $\Omega$}\label{ss.Diskretisierung}


Mit einem zweidimensionalen Gitter, der Gitterweite $h$, wobei $h \in \mathbb{Q}$ mit $h = \frac {1} {m}$ und $m \in \mathbb{N}_{>2}$, wird nun das Gebiet $\Omega$ diskretisiert. Die Zahl $N = (m-1)$ gibt an, wie viele innere Gitterpunkte es jeweils in x- bzw. y-Richtung gibt.\\
Die folgenden Aussagen sind weitestgehend in \cite{DR1} wieder zu finden.\\

Für $i,j = 1,...,N$ fassen wir $u(x,y)$ auf als:
\begin{equation}
u(x_{i},y_{j}) = u(ih,jh).
\end{equation}

$\Omega$ schreiben wir als $\Omega_{h}$, so dass gilt:
\begin{equation}
\Omega_{h} = \{(ih, jh) | 1 \le i,j \le N\}.
\end{equation}

\bild{diffStar}{8cm}{5-Punkt-Differenzenstern im Gitter, wobei für alle $1 < i,j < N$ jeder Punkt $(x,y) \in \Omega_{h}$ genau vier Nachbarn in $\Omega_{h}$ besitzt.}{5-Punkt-Differenzenstern bei der Diskretisierung von $\Omega$.}\label{img.5-Point-Star}

Mit \autoref{eq.Differenzenquotienten} fassen wir $\Delta u(x,y) = \Delta_{h} u(x,y)$ für alle $(x,y) \in \Omega_{h}$ in diskretisierter Form auf als:
\begin{equation}
-\Delta_{h} u(x,y) = \frac {u(x-h,y) - u(x+h,y) + 4u(x,y) - u(x,y-h) - u(x,y+h)} {h^{2}}.\label{eq.5-Point-Star}
\end{equation}
Man stellt $\Delta_{h} u(x,y)$ auch als 5-Punkt-Differenzenstern (Abbildung 2.1) in der Form
\begin{equation}
\left[-\Delta_{h}\right]_{\xi} = \frac {1} {h^{2}}
\begin{pmatrix}
  & -1 & \\
-1 & 4 & -1 \\
  & -1 & 
\end{pmatrix}_{\xi}
\textnormal{, } \xi \in \Omega_{h}
\end{equation}
dar.

\bild{gitter}{12cm}{Nummerierung der Punkte von $\Omega = (0,1)^{2}$ mit $m=5$. In x- bzw. y-Richtung gibt es jeweils $N=4$ Punkte.}{Lexikographische Nummerierung der Punkte auf dem Gitter im Einheitsquadrat.}\label{img.gridWithNumbers}

Es werden nun alle Gitterpunkte des Gitters fortlaufend von links unten nach rechts oben (Abbildung 2.2) nummeriert (Lexikographische Nummerierung). Stellt man nun \autoref{eq.5-Point-Star} für jeden Punkt auf, so führt dies auf eine $N^{2} \times N^{2}$-Matrix, in der die Koeffizienten der Gleichung abgespeichert werden. Die vierfache Gewichtung der Funktion $u(x,y)$ steht in der Diagonale. Der linke bzw. der rechte Nachbar im Gitter sind auf der unteren bzw. oberen Nebendiagonalen zu finden, so fern der Nachbar in $\Omega_{h}$ liegt. Die oberen bzw. unteren Nachbarn, falls diese in $\Omega_{h}$ liegen, werden jeweils auf der $N-ten$ Nebendiagonalen der Matrix gespeichert.

\begin{equation}
\mat{A} = \frac{1}{h^{2}}
\begin{pmatrix}
A_{1} & -Id \\
-Id & A_{2} & \ddots \\
 & \ddots & \ddots & -Id \\
 & & -Id & A_{n}
\end{pmatrix},\label{eq.2D Matrix}
\end{equation}
wobei $\mat{Id} \in \mathbb{R}^{N \times N}$ die Identität meint und für alle $i = 0,..,N$ gilt:

\begin{equation}
A_{i} =
\begin{pmatrix}
4 & -1\\
-1 & 4 & -1\\
   & -1 & 4 & -1\\
   &    & \ddots & \ddots & \ddots\\
   &    &        & -1 & 4 & -1\\
   &    &        &    & -1 & 4 & -1\\
   &    &        &    &    & -1 & 4
\end{pmatrix},
\end{equation}
$A_{i} \in \mathbb{R}^{N \times N}$.

Nun wissen wir, wie $-\Delta u_{h}$ in Matrixform aussieht. Man nennt eine Matrix bei der viele Einträge gleich Null sind dünn besetzt oder sparse. Die rechte Seite $f$ der partiellen Differentialgleichung kann nun ebenfalls in diskretisierter Form $f_{h}$ geschrieben werden. Zu jeder Komponente von $f_{h}$, die einen Randpunkt als Nachbarn hat, wird dieser dazu addiert. Hat eine Komponente von $f_{h}$ zwei Nachbarn auf $\partial \Omega_{h}$, werden beide addiert. Dies führt uns auf folgende rechte Seite, wobei wir diese nun als $f$ auffassen wollen:

\begin{equation}
f =
\begin{pmatrix}
f_{1} \\ f_{2} \\ \vdots \\ f_{N}
\end{pmatrix},
\end{equation}

wobei gilt

\begin{equation}
f_{1} = 
\begin{pmatrix}
f(h,h) + h^{-2}(g(h,0)+g(0,h)) \\
f(2h,h) + h^{-2} g(2h,0) \\
\vdots \\
f(1-2h,h) + h^{-2} g(1-2h,0) \\
f(1-h,h) + h^{-2}(g(1-h,0)+g(1,h))
\end{pmatrix},
\end{equation}

\begin{equation}
f_{j} = 
\begin{pmatrix}
f(h,jh) + h^{-2} g(0,jh) \\
f(2h,jh) \\
\vdots \\
f(1-2h,jh) \\
f(1-h,jh) + h^{-2} g(1,jh)
\end{pmatrix}
2 \le j \le N-1,
\end{equation}

\begin{equation}
f_{N} = 
\begin{pmatrix}
f(h,1-h) + h^{-2}(g(h,1)+g(0,1-h)) \\
f(2h,1-h) + h^{-2} g(2h,1) \\
\vdots \\
f(1-2h,1-h) + h^{-2} g(1-2h,1) \\
f(1-h,1-h) + h^{-2}(g(1-h,1)+g(1,1-h))
\end{pmatrix}.
\end{equation}

Somit ergibt sich das lineare Gleichungssystem $\mat{A}u = f$, wobei $\mat{A}$ der diskretisierte Laplace-Operator ist, $f$ die rechte Seite der Gleichung darstellt und $u$ die approximierte Lösung der partiellen Differentialgleichung enthält.\\
Wir bezeichnen ab jetzt die zweidimensionale Poisson Matrix aus \autoref{eq.2D Matrix} mit $\mat{A}_{2D}$. \\
Außerdem gilt für die Schrittweite stets $h = \frac{1}{m}$, wobei $m \in \mathbb{N}_{m > 2}$. Zudem setzen wir $N = m-1$ und $n = N^{2}$, so dass wir die Poisson Matrix als eine Matrix $\mat{A}_{2D} \in \mathbb{R}^{n \times n}$ auffassen werden.

\section{Eigenschaften der Matrix $\mat{A}_{2D}$}\label{s.Eigenwerte und Eigenvektoren}

$\mat{A}_{2D} \in \mathbb{R}^{n \times n}$ ist symmetrisch, da $\mat{A}_{2D} = \mat{A}_{2D}^{T}$. Somit existiert eine Orthogonalbasis aus Eigenvektoren für die gilt:
\begin{equation}
\mat{A}_{2D}v_{i} = \lambda_{i} v_{i},
\end{equation}
wobei für alle $1 \le i \le n$ gilt: $\lambda_{1},...,\lambda_{n} \in \mathbb{R}$ und $v_{i} \in \mathbb{R}^{n}$.

\subsection{Eigenwerte und Eigenvektoren von $\mat{A}_{2D}$}\label{ss.Eigenwerte und Eigenvektoren}

Aus \cite{SAAD1} sei für $k,l \in \mathbb{N}$, $(x_{k},y_{l}) \in \Omega_{h}$ mit $x_{k} = k \cdot h, y_{l} = l \cdot h$. Zudem sind $\theta_{k}, \theta_{l} \in \mathbb{R}$ mit $\theta_{k} = k \pi h$ bzw. $\theta_{l} = l \pi h$. Dann gilt für die Eigenwerte:
\begin{equation}
\lambda_{k,l} = 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2}) \right) \textnormal{ für } 1 \le k,l \le N.\label{eq.Eigenwerte}
\end{equation}
Zudem gilt für einen Eigenvektor am Punkt $(x_{k},y_{l})$ gilt:
\begin{equation}
(v_{k,l})_{i,j} = \sin(i \pi x_{k}) \sin(j \pi y_{l}) = \sin(i\theta_{k}) \sin(j\theta_{l}) \textnormal{ für } 1 \le i,j,k,l \le N.
\end{equation}

\textbf{Bemerkung:}

Ein Eigenvektor im Punkt $(x_{k},y_{l})$ lässt sich auch in der folgenden Form darstellen:
\begin{equation}
v_{k.l} = 
\begin{pmatrix}
\sin(\theta_{k})\sin(\theta_{l})\\
\sin(\theta_{k})\sin(2 \theta_{l})\\
\vdots \\
\sin(\theta_{k})\sin(N \theta_{l})\\
\sin(2 \theta_{k})\sin(\theta_{l})\\
\sin(2 \theta_{k})\sin(2 \theta_{l})\\
\vdots\\
\sin(N \theta_{k})\sin((N-1) \theta_{l})\\
\sin(N \theta_{k})\sin(N \theta_{l})
\end{pmatrix}.
\end{equation}

\textbf{Beispiel:}

Um sich die Eigenvektoren besser vorstellen zu können, wollen wir zunächst drei Eigenvektoren des eindimensionalen Poisson Problems betrachten. Es wird das Gebiet $\Omega_{h} = (0,1)$ diskretisiert und es gilt $u(x)'' = f(x)$ mit Randbedingung $u(x) = g(x)$. Dann erhält man auf einem Gitter $(0,1)$ mit $h = \frac{1}{m}$ wie oben, die Eigenvektoren des diskretisierten Problems in den Punkten 1,7 und 13:

\bild{sin1}{10cm}{Dieser Graph stellt den Eigenvektor am ersten Punkt, also $h = \frac{1}{15}$ dar. Die Sinuswellen sind zwar eher kurzwellig, allerdings nicht stark oszillierend.}{Eigenvektor mit kurzwelligen, schwach oszillierenden Sinuswellen.}
\bild{sin7}{10cm}{Bei diesen stark oszillierenden Sinuswellen handelt es sich um den Eigenvektor im Punkt $\frac{7}{15}$.}{Stark oszillierender Eigenvektor für die eindimensionale Poisson Gleichung.}
\bild{sin13}{10cm}{Die Sinuswellen sind für den Eigenvektor im Punkt $\frac{13}{15}$ kaum mehr erkennbar, wegen ihrer starken Oszillation.}{Sehr stark oszillierender Eigenvektor.}\label{img.1D Langwelle}



Man sieht deutlich, dass die Oszillation der einzelnen Eigenvektoren unterschiedlich ist. Manche der Eigenvektoren sind langwellig, andere kurzwellig. Wenn wir nun zurück zu unserer Ausgangssituation des zweidimensionalen Poisson Problems gehen und das Gitter aus Abbildung 2.2 als Basis nehmen, erhalten wir für den Punkt 11 des Gitters folgende Darstellung des Eigenvektors:

\bild{EV}{14cm}{Es sind die Sinuswellen des Eigenvektors im Gitterpunkt $(\frac {3} {5},\frac {3} {5})$ zu sehen. Sie sind langwellig und nur schwach oszillierend.}{Ein Beispiel für einen langwelligen Eigenvektor für die zweidimensionale Poisson Gleichung.}

Zur besseren Darstellung wurde für den Plot der Eigenvektor das kontinuierlichen Problem herangezogen. Man konnte hier einen langwelligen Eigenvektor erkennen. Von besonderem Interesse sind in späteren Abschnitten jedoch die kurzwelligen, stark oszillierenden Eigenvektoren.

\textbf{Beweis zu \autoref{ss.Eigenwerte und Eigenvektoren}:}

Wir wollen zunächst die $\mat{A}_{i} \in \mathbb{R}^{N \times N}$ von $\mat{A} \in \mathbb{R}^{n \times n}$ genauer betrachten.\\
Behauptung: Für eine Matrix $\mat{B} \in \mathbb{R}^{N \times N}$ mit
\begin{equation}
\mat{B} = 
\begin{pmatrix}
a & b\\
c & a & b\\
  & \ddots & \ddots & \ddots\\
  &		   & c & a & b\\
  &		   &  & c & a
\end{pmatrix}
\end{equation}

gilt für die Eigenwerte $\lambda_{k} = a + 2b\left(\frac{c}{b}\right)^{\frac{1}{2}} \cos(\theta_{k})$ und die Eigenvektoren\\
$(v_{k})_{i} = \left(\frac{c}{b}\right)^{\frac{i}{2}} \sin(i \theta_{k})$ für alle $1 \le i \le N$, $\lambda_{k} \in \mathbb{R}, v_{k} \in \mathbb{R}^{N}$.

\textit{Beweis:}

Da $\lambda_{k}, v_{k}$ Eigenwerte bzw. Eigenvektoren von $\mat{B}$ sind, gilt folgende Gleichung:
\begin{equation}
(\mat{B} - \lambda_{k} \mat{Id}) v_{k} = 0
\end{equation}

\begin{equation}
\Longleftrightarrow
\begin{pmatrix}
a - \lambda_{k} & b\\
c & a - \lambda_{k} & b\\
  & \ddots & \ddots & \ddots\\
  &		   & c & a - \lambda_{k} & b\\
  &		   &  & c & a - \lambda_{k}
\end{pmatrix}
v_{k} = 0
\end{equation}

\begin{equation}
\Longleftrightarrow
\begin{pmatrix}
(a - \lambda_{k}) v_{k_{1}} + b v_{k_{2}}\\
c v_{k_{1}} + (a - \lambda_{k}) v_{k_{2}} + b v_{k_{3}}\\
\vdots\\
c v_{k_{N-2}} + (a - \lambda_{k}) v_{k_{N-1}} + b v_{k_{N}}\\
c v_{k_{N-1}} + (a - \lambda_{k}) v_{k_{N}}
\end{pmatrix}
= 0.
\end{equation}

Wir wollen zunächst die einzelnen Summanden ausrechnen bzw. vereinfachen. Dabei benutzen wir u.a. die Additionstheoreme:

\begin{description}
\item[1.] Löse $(a - \lambda_{k}) (v_{k})_{i}$
\begin{eqnarray}
(a - \lambda_{k}) (v_{k})_{i} &=& (a - (a + 2b\left(\frac{c}{b}\right)^{\frac{1}{2}} \cos(\theta_{k}))) \left(\frac{c}{b}\right)^{\frac{i}{2}} \sin(i \theta_{k})\notag \\
&=& -2b\left(\frac{c}{b}\right)^{\frac{i+1}{2}} \cos(\theta_{k})\sin(i \theta_{k}) \notag
\end{eqnarray}
\item[2.] Löse $b (v_{k})_{i+1}$
\begin{eqnarray}
b (v_{k})_{i+1} &=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} \sin((i+1) \theta_{k})\notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) + \cos(i \theta_{k}) \sin(\theta_{k})) \notag
\end{eqnarray}
\item[3.] Löse $c (v_{k})_{i-1}$
\begin{eqnarray}
c (v_{k})_{i-1} &=& c \left(\frac{c}{b}\right)^{\frac{i-1}{2}} \sin((i-1) \theta_{k}) = \left(c^{2}\frac{c}{b}\right)^{\frac{i-1}{2}} \sin((i-1) \theta_{k})\notag \\
&=& \left(\frac{1}{b^{-2}} \frac{c}{b}\right)^{\frac{i+1}{2}} \sin((i-1) \theta_{k}) = b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} \sin((i-1) \theta_{k}) \notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) - \cos(i \theta_{k}) \sin(\theta_{k})) \notag
\end{eqnarray}
\end{description}

Nun rechnen wir jede Zeile unseres Vektors aus und nutzen dabei wieder die Additionstheoreme:

\begin{description}
\item[1.] $i = 1$
\begin{eqnarray}
&& -2b\left(\frac{c}{b}\right)^{\frac{1+1}{2}} \cos(\theta_{k})\sin(\theta_{k}) + b \left(\frac{c}{b}\right)^{\frac{1+1}{2}} (\cos(\theta_{k}) \sin(\theta_{k}) + \cos(\theta_{k}) \sin(\theta_{k})) \notag \\
&=& c (-2\cos(\theta_{k})\sin(\theta_{k}) + 2\cos(\theta_{k})\sin(\theta_{k})) = 0. \notag
\end{eqnarray}
\item[2.] für alle $2 \le i \le N-1$
\begin{eqnarray}
&& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) - \cos(i \theta_{k}) \sin(\theta_{k})) - 2b\left(\frac{c}{b}\right)^{\frac{i+1}{2}} \cos(\theta_{k})\sin(i \theta_{k}) \notag \\
&+& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) - \cos(i \theta_{k}) \sin(\theta_{k})) \notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (-2\cos(\theta_{k})\sin(i\theta_{k}) + 2\cos(\theta_{k})\sin(i\theta_{k}) \notag \\
&+& \cos(i\theta_{k})\sin(\theta_{k}) - \cos(i\theta_{k})\sin(\theta_{k})) = 0. \notag
\end{eqnarray}
\item[3.] $i = N$
\begin{eqnarray}
&& b \left(\frac{c}{b}\right)^{\frac{N+1}{2}} (\cos(\theta_{k}) \sin(N \theta_{k}) - \cos(N \theta_{k}) \sin(\theta_{k})) - 2b\left(\frac{c}{b}\right)^{\frac{N+1}{2}} \cos(\theta_{k})\sin(N \theta_{k}) \notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{N+1}{2}}(-\cos(\theta_{k})\sin(N\theta_{k}) - \cos(N\theta_{k})\sin(\theta_{k})) \notag \\
&=& -b \left(\frac{c}{b}\right)^{\frac{N+1}{2}}(\cos(\theta_{k})\sin(N\theta_{k}) + \cos(N\theta_{k})\sin(\theta_{k})). \notag \\
&=& -b \left(\frac{c}{b}\right)^{\frac{N+1}{2}} \sin((N+1)\theta_{k}) \overset{h = \frac{1}{N+1}}{=} -b \left(\frac{c}{b}\right)^{\frac{N+1}{2}} \sin(k\pi) \overset{k \in \mathbb{N}}{=} 0. \notag
\end{eqnarray}
\end{description}
\begin{flushright}
$\blacksquare$
\end{flushright}
Für die Matrizen $A_{i}$ erhalten wir mit $a = 4, b = c = -1$ für die Eigenwerte $\lambda_{k} = 4 (1 - \frac{1}{2} \cos(\theta_{k}))$ und die Eigenvektoren $v_{k} = \sin(i\theta_{k})$ für alle $1 \le i \le N$.\\
Offensichtlich hat auch $-\mat{Id}$ die selben Eigenvektoren wie $A_{i}$, mit den Eigenwerten $\mu_{k} = -1$, denn
\begin{equation}
(-\mat{Id} - \lambda_{k}\mat{Id}) v_{k} = 0.
\end{equation}
Nun wollen wir diese Erkenntnisse für die Matrix $\mat{A}_{2D} \in \mathbb{R}^{n \times n}$ verwenden. Da diese Matrix ebenfalls eine Tridiagonalmatrix ist, folgt für $a = A_{j}$, wobei $1 \le j \le N$ und $b = c = -Id$.\\
Der gesuchte Eigenvektor in einem Punkt $(x_{k},y_{l})$ war gegeben durch $(v_{k,l})_{i,j} = \sin(i\theta_{k}) \sin(j\theta_{l})$ für alle $1 \le i,j,k,l \le N$. Diese Eigenvektoren können wir auch auffassen als $(v_{k,l})_{i,j} = \sin(j\theta_{l})(v_{k})_{i}$ für alle $1 \le i,j,k,l \le N$. Wegen $\mat{A}_{2D}$ symmetrisch folgt:
\begin{eqnarray}
\mat{A}_{2D}v_{k,l} &=& 
\begin{pmatrix}
A_{1} & -Id\\
-Id & A_{2} & -Id\\
    & \ddots & \ddots & \ddots\\
   	&		 & -Id    & A_{N-1} & -Id\\
   	&		 &		  & -Id    & A_{N}
\end{pmatrix}
\begin{pmatrix}
\sin(\theta_{l}) v_{k}\\
\sin(2\theta_{l}) v_{k}\\
\vdots\\
\sin((N-1)\theta_{l}) v_{k}\\
\sin(N\theta_{l}) v_{k}\\
\end{pmatrix}\notag\\
&=& \lambda_{k,l}
\begin{pmatrix}
\sin(\theta_{l}) v_{k}\\
\sin(2\theta_{l}) v_{k}\\
\vdots\\
\sin((N-1)\theta_{l}) v_{k}\\
\sin(N\theta_{l}) v_{k}\\
\end{pmatrix}\notag
\end{eqnarray}

Für $1 \le l,k \le N$ werden wir nun $\mat{A}_{2D} v_{k,l}$ explizit ausrechnen und müssen dafür wieder drei Fälle unterscheiden, bei denen wir die Additionstheoreme benutzen:
\begin{description}
\item[1.] $j = 1$
\begin{eqnarray}
&&\underbrace{A_{1} v_{k}}_{=\lambda_{k} v_{k}} \sin(\theta_{l}) + (\underbrace{-Id v_{k}}_{= \mu_{k} v_{k}}) \underbrace{\sin(2\theta_{l})}_{= \sin(\theta_{l} + \theta_{l}) = 2\cos({\theta_{l})\sin({\theta_{l}})}} \notag \\
&=& \lambda_{k}v_{k}\sin(\theta_{l}) + 2\mu_{k}v_{k}\cos(\theta_{l})\sin(\theta_{l}) \notag \\
&=& (\lambda_{k} + 2\mu_{k}\cos(\theta_{l})) \underbrace{v_{k}\sin(\theta_{l})}_{= (v_{k,l})_{j=1}} = (\lambda_{k} + 2\mu_{k}\cos(\theta_{l}))(v_{k,l})_{j=1} \notag \\
&=& (4(1 - \frac{1}{2}\cos{\theta_{k}}) - 2\cos(\theta_{l})) (v_{k,l})_{j=1} \notag \\
&=& (4 - 2\cos(\theta_{k}) - 2\cos(\theta_{l})) (v_{k,l})_{j=1} \notag \\
&=& (2 - 2\cos(\theta_{k}) + 2 - 2\cos(\theta_{l})) (v_{k,l})_{j=1} \notag \\
&=& \underbrace{(4(\frac{1}{2} - \frac{1}{2}\cos(\theta_{k})) + 4(\frac{1}{2} - \frac{1}{2}\cos(\theta_{l}))}_{\textnormal{mit } \frac{1}{2}(1 - \cos(x)) = \sin^{2}(\frac{x}{2})} (v_{k,l})_{j=1} \notag \\
&=& 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) (v_{k,l})_{j=1}\notag
\end{eqnarray}
\item[2.] für alle $2 \le j \le N-1$
\begin{eqnarray}
&&\underbrace{-Id v_{k}}_{= \mu_{k} v_{k}} \sin((j-1) \theta_{l}) + \underbrace{A_{j} v_{k}}_{=\lambda_{k} v_{k}} \sin(j\theta_{l}) \underbrace{- Id v_{k}}_{= \mu_{k} v_{k}} \sin((j+1) \theta_{l}) \notag \\
&=& \mu_{k} v_{k} (\sin(j\theta_{l})\cos(\theta_{l}) - \cos(j\theta_{l})\sin(\theta_{l})) + \lambda_{k} v_{k} \sin(j\theta_{l}) \notag \\
&+& \mu_{k} v_{k} (\sin(j\theta_{l})\cos(\theta_{l}) + \cos(j\theta_{l})\sin(\theta_{l})) \notag \\
&=& 2 \mu_{k} v_{k} \sin(j\theta_{l})\cos(\theta_{l}) + \lambda_{k} v_{k} \sin(j\theta_{l}) = (\lambda_{k} + \mu_{k} \cos(\theta_{l})) \underbrace{(v_{k,l})_{j=N}}_{= \sin(j\theta_{l}) v_{k}} \notag \\
&=& (\lambda_{k} + \mu_{k} \cos(\theta_{l})) \sin(j\theta_{l}) (v_{k,l})_{j=N} \notag \\
&\overset{\textnormal{ wie oben }}{=}& 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) (v_{k,l})_{j=N} \notag
\end{eqnarray}
\item[3.] $j = N$
\begin{eqnarray}
&&\underbrace{-Id v_{k}}_{= \mu_{k} v_{k}} \sin((N-1)\theta_{l}) + \underbrace{A_{Ń} v_{k}}_{=\lambda_{k} v_{k}} \sin(N \theta_{l}) \notag \\
&=& \mu_{k} v_{k} (\sin((N-1)\theta_{l}) + \underbrace{\sin((N+1)\theta_{l})}_{= 0}) + \lambda_{k} v_{k} \sin(N\theta_{l}) \notag \\
&=& \mu_{k} v_{k} (\sin(N\theta_{l})\cos(\theta_{l}) - \cos(N\theta_{l})\sin(\theta_{l}) + \sin(N\theta_{l})\cos(\theta_{l}) \notag \\
&+& \cos(N\theta_{l})\sin(\theta_{l})) + \lambda_{k} v_{k} \sin(N \theta_{l}) \notag \\
&=& 2\mu_{k} v_{k} \sin(N\theta_{l})cos(\theta_{l}) + \lambda_{k} v_{k} \sin(N\theta_{l}) \notag \\
&=& (\lambda_{k} + 2\mu_{k}\cos(\theta_{l})) \underbrace{(v_{k,l})_{j=N}}_{= \sin(N\theta_{l}) v_{k}} \overset{\textnormal{ wie oben }}{=} 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) (v_{k,l})_{j=N} \notag
\end{eqnarray}
\end{description}
Also sind die $\lambda_{k,l}$ Eigenwerte von $\mat{A}_{2D}$ zu den Eigenvektoren $v_{k,l}$.
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Folgerung ($\mat{A}_{2D}$ ist s.p.d.)}

Für die Eigenwerte $\lambda_{k,l} \in \mathbb{R}$ von $\mat{A}_{2D}$ mit $1 \le k,l \le N$ gilt:
\begin{equation}
\lambda_{k,l} > 0,
\end{equation}
d.h. die Poisson Matrix ist symmetrisch positiv definit.

\textbf{Beweis:}

Da $\sin^{2}(x) \in (0,1)$ streng monoton steigend ist für $x \in (0,\frac{\pi}{2})$ und $0 < \frac{\pi h}{2} < \frac{\pi}{2}$ gilt:
\begin{equation}
\lambda_{k,l} = 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) > 0.
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Definition (Kondition einer symmetrischen Matrix)}\label{ss.Definition Kondition einer Matrix}

Sei $\mat{A}$ eine symmetrische Matrix des $\mathbb{R}^{n \times n}$. Dann ist die euklidische Kondition der Matrix definiert als:
\begin{equation}
\kappa_{2} (A) = \frac {\lambda_{max}} {\lambda_{min}} \ge 1.\label{eq.Kondition}
\end{equation}
Je kleiner die Konditionszahl $\kappa$, desto besser ist eine Matrix konditioniert.

\subsection{Lemma (Kondition von $\mat{A}_{2D}$)}\label{ss.Matrixkondition}

In \cite{DR2} wird gezeigt, dass für die Kondition der Matrix $\mat{A}_{2D}$ gilt:
\begin{equation}
\kappa_{2} (A_{2D}) = \frac {\cos^{2}(\frac{\pi h}{2})} {\sin^{2}(\frac{\pi h}{2})}.
\end{equation}

\textbf{Beweis:}

Es gilt mit $h = \frac{1}{m}$, $N = m - 1$, weil $sin^{2}(x) \in (0,1)$ streng monoton steigend ist für $x \in (0,\frac{\pi}{2})$ und den Additionstheoremen:
\begin{eqnarray}
\lambda_{min} &=& \lambda_{1,1} = 4 (\sin^{2}(\frac{\pi h}{2}) + \sin^{2}(\frac{\pi h}{2})) = 8\sin^{2}(\frac{\pi h}{2}),\label{eq.Lambda min}\\
\lambda_{max} &=& \lambda_{N,N} = 8\sin^{2}(\frac{N\pi h}{2}) \overset{\substack{h=\frac {1} {m}, \\N=m-1}}{=} 8\sin^{2}(\frac{(m-1)\pi}{2m}) \notag \\
&=& 8\sin^{2}(\frac{\pi}{2} - \frac{\pi}{2m}) = 8\sin^{2}(\frac{\pi}{2} - \frac{\pi h}{2}) \notag \\
&=& 8(\sin(\frac{\pi}{2})\cos(\frac{\pi h}{2})) - \sin(\frac{\pi h}{2})\cos(\frac{\pi}{2}))^{2} \notag \\
&=& 8\cos^{2}(\frac{\pi h}{2}). \label{eq.Lambda max}
\end{eqnarray}

Somit folgt aus \autoref{eq.Lambda min} und \autoref{eq.Lambda max}:
\begin{equation}
\kappa_{2} (A_{2D}) = \frac {\lambda_{N,N}} {\lambda_{1,1}} = \frac {\cos^{2} (\frac {\pi h} {2})} {\sin^{2} (\frac {\pi h} {2})}. \notag
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}
In \cite{DR3} wird außerdem gezeigt, dass sich $\kappa_{2}(\mat{A}_{2D})$ wie folgt nähern lässt:

\begin{equation}
\frac {\cos^{2} (\frac {\pi h} {2})} {\sin^{2} (\frac {\pi h} {2})} = \left( \frac {2} {\pi h} \right)^{2} (1 + \mathcal{O}(h^{2})).
\end{equation}

Natürlich wollen wir die Funktion $u(x,y)$ so gut wie möglich in $\Omega_{h}$ approximieren. Wir sind daher bestrebt das Gitter so fein als möglich zu wählen. Daraus ergibt sich jedoch die negative Eigenschaft von $\mat{A}_{2D}$. \\
Je größer $m$ gewählt wird, also je feiner das Gitter wird, desto schlechter wird die Kondition der Matrix.
\begin{equation}
\left( \frac {2} {\pi h} \right)^{2} (1 + \mathcal{O}(h^{2})) = \left( \frac {2} {\pi \frac {1} {m}} \right)^{2} (1 + \mathcal{O}(\frac {1} {m^{2}})) \approx \frac {4m^{2}} {\pi^{2}}. \label{eq.kondition von PMatrix}
\end{equation}

Man sieht, dass die Kondition der Matrix quadratisch mit $m$ wächst.

\chapter{Iterative Lösungsverfahren für lineare Gleichungssysteme}\label{c.IterativeVerfahren}

Gleichungssysteme, die partielle Differentialgleichungen lösen, sind im Allgemeinen sehr groß. Aus diesem Grund sind direkte Verfahren, wie z.B. der Gauß-Algorithmus oder die LR-Zerlegung nicht geeignet. Ihr Rechenaufwand beläuft sich für voll besetzte Matrizen im Allgemeinen auf $\mathcal{O}(n^{3})$ und ist zu langsam. Nun ist unser System zwar nicht voll besetzt, jedoch kann es durch die Struktur der Matrix zur Auslöschung von Nullen kommen. Dadurch ist ebenfalls keine große Effizienz gegeben.\\
Wesentlich besser geeignet für diese Problemstellung sind iterative Verfahren. Sie zeichnen sich durch eine schnelle Konvergenz und einen geringeren Rechenaufwand aus.\\
Ein Großteil der Definitionen, Sätze und Lemmata in diesem Kapitel sind sinngemäß aus \cite{DR4}. Der Abschnitt über das Jacobi-Relaxationsverfahren wurde in Teilen von \cite{SAAD2} übernommen. Stellen, an denen andere Literatur verwendet wurde, sind deutlich gekennzeichnet.

\section{Grundbegriffe}\label{s.Grundbegriffe}

\subsection{Definition (Iterationsmatrix)}\label{ss.Iterationsmatrix}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$. Sei außerdem $\mat{C} \in \mathbb{R}^{n \times n}$ eine nichtsinguläre Matrix. Für die iterative Lösung eines linearen Gleichungssystems der Form $Au = f$ ist die Iterationsmatrix $\mat{T} \in \mathbb{R}^{n \times n}$ definiert als:
\begin{equation}
\mat{T} = (\mat{Id} - \mat{C} \mat{A}),
\end{equation}
wobei die Iterationsvorschrift für $k=1,...,n$ gegeben ist durch:
\begin{equation}
u^{k+1} = (\mat{Id} - \mat{C} \mat{A})u^{k} + \mat{C} f.\label{eq.Iteratives Verfahren}
\end{equation}

\subsection{Definition (Spektralradius)}\label{s.Spektralradius}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$. Und seien für alle $i=1,...,n$: $\lambda_{i} \in \mathbb{R}$. Dann gilt:
\begin{equation}
\rho(A) = \max_{1 \le i \le n} | \lambda_{i} |.
\end{equation}
Ist $\mat{A}$ symmetrisch positiv definit, so gilt auch $\rho(A) = \| A \|_{2}$ und $\lambda_{i} \in \mathbb{R}_{>0}$ für alle $i=1,...,n$.

\subsection{Satz (Konvergenz iterativer Verfahren)}\label{ss.Konvergenz iterativer Verfahren}

Ein iteratives Verfahren mit beliebigem Startvektor $u^{0} \in \mathbb{R}^{n}$ konvergiert genau dann gegen die exakte Lösung $u^{*} \in \mathbb{R}^{n}$, wenn gilt:
\begin{equation}
\rho(\mat{T}) = \rho(Id - CA) < 1.
\end{equation}
Je kleiner $\rho$ ist, desto schneller konvergiert das System.

Einen Beweis hierzu findet man z.B. in \cite{DR5}.

\subsection{Definition (Residuum und Fehler)}\label{ss.Residuum und Fehler}

Sei $u^{*} \in \mathbb{R}^{n}$ die exakte Lösung des linearen Gleichungssystems $Au = f$. Sei weiterhin $u^{k} \in \mathbb{R}^{n}$ die Approximation der Lösung im $k-ten$ Iterationsschritt. Dann gilt für das Residuum:
\begin{equation}
r^{k} = f - \mat{A} u^{k}.\label{eq.Residuum}
\end{equation}
Der Fehler, also die Diskrepanz zwischen exakter und approximierter Lösung, ist definiert als:
\begin{equation}
e^{k} = u^{*} - u^{k}.\label{eq.Fehler}
\end{equation}
Durch Multiplikation mit der Matrix $\mat{A}$ ergibt sich:
\begin{eqnarray}
&&e = u^{*} - u \Leftrightarrow \mat{A}e = \mat{A}(u^{*} - u) \notag \\
&\Leftrightarrow& \mat{A}e = \mat{A} u^{*} - \mat{A} u \Leftrightarrow \mat{A}e = f - \mat{A} u \notag \\
&\Leftrightarrow& \mat{A}e = r.\label{eq.Residuumsgleichung}
\end{eqnarray}
$\mat{A}e = r$ nennen wir Residuumsgleichung.

\section{Das Jacobi-Verfahren (Gesamtschrittverfahren)}\label{s.Das Jacobi-Iterationsverfahren}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$ und $f,u \in \mathbb{R}^{n}$, wobei $u$ die Lösung des linearen Gleichungssystems $\mat{A}u = f$ ist. Dann lässt sich $\mat{A}$ wie folgt zerlegen:

\begin{equation}
A = D - L - U.
\end{equation}

Dabei sind $\mat{D},\mat{L},\mat{U} \in \mathbb{R}^{n \times n}$, mit $\mat{D} = diag(a_{1,1},...,a_{n,n})$, $\mat{L}$ strikte untere und $\mat{U}$ strikte obere Dreiecksmatrix. \\
Somit ergibt sich für $\mat{A}u = f$:

\begin{equation}
Au = f \Leftrightarrow (D-L-U)u = f \Leftrightarrow Du = (L+U)u + f.
\end{equation}

Ist nun $\mat{D}$ nicht singulär, so gilt für das Jacobi-Verfahren folgende Iterationsvorschrift:
\begin{equation}
Du^{k+1} = (L+U)u^{k} + f \Leftrightarrow u^{k+1} = D^{-1}(L+U)u^{k} + D^{-1}f.
\end{equation}

\subsection{Algorithmus (Jacobi-Verfahren)}\label{ss.Allgemeines Jacobi-Verfahren}

In \cite{ALO2} wird gezeigt, wie man den Rechenaufwand verbessern kann, indem man die Struktur von $\mat{A}_{2D}$ optimal ausnutzt.\\
Mit einem Startvektor $u^{0} \in \mathbb{R}^{n}$ \textit{beliebig} und $k=1,2,...$, berechne für $i=1,...,n$:

\begin{equation}
u^{k+1}_{i} = \frac {1} {a_{ii}} (f_{i} - \sum_{\substack{j = 1 \\ j \ne i}}^{n} a_{ij}u^{k}_{j}).
\end{equation}

In jedem Schritt zur Berechnung von $u^{k+1}$ muss im Algorithmus die Information seines Vorgängers $u^{k}$ bekannt sein. Der Rechenaufwand pro Iterationsschritt beträgt $\mathcal{O}(n^{2})$ und entspricht somit einer Matrix-Vektor-Multiplikation.

\subsection{Satz (Iterationsmatrix des Jacobi-Verfahrens)}\label{ss.Iterationsmatrix Jacobi}

Für die Iterationsmatrix des Jacobi-Verfahrens gilt:
\begin{equation}
\mat{T}_{J} = (\mat{Id} - \mat{D}^{-1} \mat{A})
\end{equation}
Hier ist also $\mat{C} = \mat{D}^{-1}$

\textbf{Beweis}:

Mit der Iterationsvorschrift folgt:
\begin{eqnarray}
u^{k+1} &=& D^{-1}(L+U)u^{k} + D^{-1}f \overset{(L+U)=(D-A)}{=} D^{-1}(D-A)u^{k}+D^{-1}f \notag \\
&=& (Id-D^{-1}A)u^{k} + D^{-1}f. \notag
\end{eqnarray}
Also $\mat{T}_{J} = (\mat{Id} - \mat{D}^{-1} \mat{A})$.
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Satz (Eigenwerte von $\mat{T}_{J}$ bzgl. $\mat{A}_{2D}$)}\label{ss.EW Jacobi}

Man sieht leicht ein, dass die Eigenvektoren von $\mat{T}_{J}$ gleich denen von $\mat{A}_{2D}$ sind. Dann gilt für die Eigenwerte der Iterationsmatrix:
\begin{equation}
\lambda_{i,j}(\mat{T}_{J}) = 1 - \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right), \label{eq.Jacobi-Eigenwerte}
\end{equation}
für $1 \le i,j \le N$ und $\theta_{i}$, $\theta_{j}$ wie in \autoref{ss.Eigenwerte und Eigenvektoren}.

\textbf{Beweis:}\label{b.EW Jacobi}

Dieser folgt direkt mit dem Beweis aus \autoref{ss.EW Relax} für $\omega = 1$.
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Lemma (Spektralradius von $\mat{T}_{J}$ bzgl. $\mat{A}_{2D}$)}\label{ss.Spektralradius Jacobi}

Das Jacobi-Verfahren konvergiert für die diskretisierte Poisson Gleichung und es gilt für den Spektralradius:
\begin{equation}
\rho(\mat{T}_{J}) = \cos(\pi h) < 1.
\end{equation}

\textbf{Beweis:}\label{b.Spektral Jacobi}

Folgt mit Beweis aus \autoref{ss.Spektralradius Jacobi Relax} und $\omega = 1$.
\begin{flushright}
$\blacksquare$
\end{flushright}

Wir können für das Jacobi-Verfahren zwar die Konvergenz nicht verbessern, allerdings ist es möglich den Rechenaufwand zu verbessern. Dies gelingt uns, indem wir die Dünnbesetztheit von $\mat{A}_{2D}$ gezielt ausnutzen. Es sind pro Zeile maximal $5$ Einträge ungleich Null. Oder anders formuliert, hat jeder Gitterpunkt in $\Omega_{h}$ höchstens 4 Nachbarn. Nutzt man diese Struktur aus, so erhält man den folgenden Algorithmus:

\subsection{Algorithmus (Jacobi-Verfahren für $\mat{A}_{2D}$)}\label{ss.Jacobi für Poisson}

Dieser Abschnitt wurde sinngemäß aus \cite{ALO2} übernommen. Der Algorithmus wurde auf unser Problem hingehend optimiert.
Berechne für $k = 1,2,...$ mit Startvektor $u^{0} \in \mathbb{R}^{n}$ \textit{beliebig}:\\
Für $i = 1,...,N$ und für $j = 1,...,N$:
\begin{equation}
u^{k+1}_{i,j} = \frac{h^{-2}}{4} (u^{k}_{i-1,j} + u^{k}_{i+1,j} + u^{k}_{i,j-1} + u^{k}_{i,j+1}) - \frac{1}{4} f_{i,j}
\end{equation}
Bei diesem Algorithmus gehen wir über allen inneren Punkte des Gitters in x- und y-Richtung. Sobald ein Zeilenindex $i$ (bezeichnet die y-Komponente) Null wird, ignorieren wir diesen Wert. Das Gleiche gilt für die Spalteninidzes $j$, die die x-Komponente repräsentieren.\\
Der Rechenaufwand pro Iterationsschritt beträgt lediglich $\mathcal{O}(N \cdot N)=\mathcal{O}(n)$ Schritte.

\section{Das Jacobi-Relaxationsverfahren}\label{s.Jacobi Relaxation}

Wir wollen nochmal die Iterationsvorschrift des Jacobi-Verfahrens betrachten:

\begin{equation}
u^{k+1} = (Id - D^{-1}A)u^{k} + D^{-1}f.
\end{equation}

Durch Umformung erhalten wir:
\begin{eqnarray}
u^{k+1} &=& (Id - D^{-1}A)u^{k} + D^{-1}f \notag \\
&=& u^{k} - D^{-1}Au^{k} + D^{-1}f \notag \\
&=& u^{k} + D^{-1} \underbrace{(f - Au^{k})}_{= r^{k}}.
\end{eqnarray}

Wir addieren also zu $u^{k}$ das Residuum. Die Idee ist nun das Residuum mit einem Parameter $\omega \in \mathbb{R}$ zu multiplizieren, so dass wir neue Eigenschaften erhalten:
\begin{eqnarray}
u^{k+1} &=& u^{k} + D^{-1} \omega r^{k} = u^{k} + \omega D^{-1} (f - Au^{k}) \notag \\
&=& u^{k} - \omega D^{-1} Au^{k} + \omega D^{-1}f = (Id - \omega D^{-1} A)u^{k} + \omega D^{-1}f \notag \\
&=& (Id - \omega Id + \omega Id - \omega D^{-1} A)u^{k} + \omega D^{-1}f \notag \\
&=& ((1 - \omega)Id + \omega(Id - D^{-1}A)) u^{k} + \omega D^{-1}f.\label{eq.Jacobi-Relaxation}
\end{eqnarray}
\autoref{eq.Jacobi-Relaxation} stellt die Iterationsvorschrift für das Jacobi-Relaxationsverfahren dar. Die Iterationsmatrix ist gegeben durch:
\begin{equation}
\mat{T}_{J_{\omega}} = (1 - \omega) \mat{Id} + \omega(\mat{Id} - \mat{D}^{-1} \mat{A}) = (\mat{Id} - \omega \mat{D}^{-1} \mat{A}).
\end{equation}

Letztlich erweitert man den Jacobi-Algorithmus also mit dem Parameter $\omega$ und addiert $(1 - \omega)u^{k}$ zu $u^{k+1}$.

\subsection{Algorithmus (Jacobi-Relaxations-Verfahren)}\label{ss.Algorithmus Jacobi Relax}

Sei $u^{0} \in \mathbb{R}^{n}$ ein \textit{beliebiger} Startvektor und $k=1,2,...$. Berechne für $i=1,...,n$:

\begin{equation}
u^{k+1}_{i} = (1 - \omega)u^{k}_{i} + \frac {\omega} {a_{ii}} (f_{i} - \sum_{\substack{j = 1 \\ j \ne i}}^{n} a_{ij}u^{k}_{j}).
\end{equation}

Beachte: Für $\omega = 1$ erhalten wir das Jacobi-Verfahren. Der Rechenaufwand des Jacobi-Relaxationsverfahrens beträgt ebenfalls $\mathcal{O}(n^{2})$.

\subsection{Satz (Eigenwerte von $\mat{T}_{J_{\omega}}$ bzgl. $\mat{A}_{2D}$)}\label{ss.EW Relax}

Die Eigenvektoren entsprechen denen von $\mat{A}_{2D}$ und für die Eigenwerte von $\mat{T}_{J_{\omega}}$ gilt:
\begin{equation}
\lambda_{i,j}(\mat{T}_{J_{\omega}}) = 1 - \omega \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right) \textnormal{ für } 1 \le i,j \le N,\label{eq.Jacobi-Relaxation-Eigenwerte}
\end{equation}
$\theta_{i}$, $\theta_{j}$ wie in \autoref{ss.Eigenwerte und Eigenvektoren}.

\textbf{Beweis:}\label{sss.EW JacobiRelax}

Für $\mat{D} = 4 \cdot \mat{Id}$ folgt $\mat{D}^{-1} = \frac {1} {4} \mat{Id}$. Für die Iterationsmatrix $\mat{T}_{J_{\omega}}$ angewandt auf einen Eigenvektor $u$ gilt:
\begin{eqnarray}
\mat{T}_{J_{\omega}}u &=& (\mat{I}d - \omega \mat{D}^{-1}\mat{A})u = \mat{Id} u - \omega \mat{D}^{-1} \mat{A} u \notag \\
&=&\mat{Id} u - \frac {\omega} {4} \mat{Id} \underbrace{\mat{A}u}_{=\lambda_{i,j}(A) u} = u (1 - \frac {\omega} {4} \lambda_{i,j}(\mat{A})). \notag
\end{eqnarray}
Die Eigenwerte von $\mat{T}_{J_{\omega}}$ lassen sich also einfach durch $(1 - \frac {\omega} {4} \lambda_{i,j}(A))$ berechnen:
\begin{equation}
\lambda_{i,j}(\mat{T}_{J_{\omega}}) = 1 - \omega \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right) \textnormal{ für } 1 \le i,j \le N. \notag
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Lemma (Spektralradius von $\mat{T}_{J_{\omega}}$ bzgl. $\mat{A}_{2D}$)}\label{ss.Spektralradius Jacobi Relax}

Das Jacobi-Relaxations-Verfahren konvergiert für die diskretisierte Poisson Gleichung und es gilt für den Spektralradius:
\begin{equation}
\rho(\mat{T}_{J_{\omega}}) = 1 - \omega(1 - \cos (\pi h)) < 1.
\end{equation}

\textbf{Beweis:}\label{b.Spektral JacobiRelax}

Hier nutzen wir nochmals aus, dass $\sin^{2}(x) \in (0,1)$ monoton steigend ist für $x \in (0, \frac{\pi}{2})S$. Zudem ist $\cos(x) \in (0,1)$ für $x \in (0,\frac{\pi}{2})$ und $\sin^{2}(\frac{x}{2}) = \frac{1}{2} (1 - \cos(x))$.
Für \autoref{eq.Jacobi-Relaxation-Eigenwerte} folgt also:
\begin{eqnarray}
\rho(\mat{Id}-\omega\mat{D}^{-1}\mat{A}_{2D}) &=& \max_{1 \le i,j \le N} | \lambda_{i,j} | \notag \\
&=& \max_{1 \le i,j \le N} | 1 - \omega \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right) | \notag \\
&=& 1 - 2 \omega \sin^{2} (\frac {\theta_{1}} {2}) = 1 - 2 \omega \sin^{2} (\frac {\pi h} {2}) \notag \\ \notag \\
&=& 1 - 2 \omega (\frac{1} {2} (1 - \cos (\pi h)) \notag \\
&=& 1 - \underbrace{\omega(1 - \underbrace{\cos (\pi h)}_{< 1})}_{< 1} < 1, \notag
\end{eqnarray}
für $\omega \in (0,1)$.
\begin{flushright}
$\blacksquare$
\end{flushright}

Da mit $h = \frac{1}{m}$ und $m \in \mathbb{N}_{>2}$ für den Spektralradius des Jacobi-Verfahrens ($\omega = 1$) gilt:
\begin{equation}
  \rho(\mat{T}) = \cos(\pi h) \in (0,1),
\end{equation}
ist die Kondition der Matrix für $h \longrightarrow 0$ mit $\kappa_{2}(\mat{T}_{J}) = \frac{\lambda_{max}}{\lambda_{min}} \approx \frac{1}{\cos(\frac{\pi}{3})}$

 $\cos(\pi h) \in [0,1)$ ist die Konditionszahl der Iterationsmatrix, die gegeben war durch:
\begin{equation}
  \kappa_{2}(\mat{T}_{J_{\omega}}) = \frac{\lambda_{max}}{\lambda_{min}},
\end{equation}
für große $m$ bzw. somit für kleine $h$

Für ein $\omega \in (0,1)$ 

Der Spektralradius ist für ein $\omega \in (0,1)$ näher an eins, als der des Jacobi-Verfahrens ($\omega = 1)$. Das Jacobi-Relaxationsverfahren konvergiert also langsamer gegen die approximierte Lösung $u^{*}$, als das Jacobi-Verfahrens. Es wird daher nicht als iteratives Verfahren eingesetzt. Wie wir in \autoref{s.Glättungseigenschaft} sehen werden, besitzt dieses Verfahren die Glättungseigenschaft und findet in den Mehrgitter-Algorithmen seine Anwendung. Für die zweidimensionale Poisson Gleichung stellen $\omega = \frac{1}{2}$ und $\omega = \frac{4}{5}$ die optimalen Parameter als Glätter dar [Saad].

\subsection{Algorithmus (Jacobi-Relaxations-Verfahren für $\mat{A}_{2D}$)}\label{ss.Algorithmus Jacobi Relax Poisson}

Mit derselben Vorgehensweise wie in \autoref{ss.Jacobi für Poisson} verbessern wir noch den Rechenaufwand bzgl. $\mat{A}_{2D}$.

Berechne für $k = 1,2,...$ mit Startvektor $u^{0} \in \mathbb{R}^{n}$ \textit{beliebig}\\
für $i = 1,...,N$ und $j = 1,...,N$:
\begin{equation}
u^{k+1}_{i,j} = (1 - \omega) u^{k}_{i,j} + \frac{h^{-2} \omega}{4} (u^{k}_{i-1,j} + u^{k}_{i+1,j} + u^{k}_{i,j-1} + u^{k}_{i,j+1}) - \frac{\omega}{4} f_{i,j}
\end{equation}

mit Rechenaufwand $\mathcal{O}(n)$.

\section{Glättungseigenschaft}\label{s.Glättungseigenschaft}

Für die Iterationsmatrix des Jacobi-Relaxationsverfahrens gilt:
\begin{equation}
u^{k+1} = (Id - \omega D^{-1}A)u^{k} + \omega D^{-1}f.
\end{equation}
Die Eigenvektoren sind gegeben durch:
\begin{equation}
(v_{k,l})_{i,j} = sin(i \theta_{k})sin(j \theta_{l}) \textnormal{ für } 1 \le i,j,k,l \le N,
\end{equation}
$\theta_{k}, \theta_{l}$ wie oben.\\
Als Eigenvektoren der Matrizen $\mat{T}_{J}$ bzw. $\mat{T}_{J_{\omega}}$ bilden diese Vektoren eine Basis des $\mathbb{R}^{n}$, wobei $n = N^{2}$.
Für den Fehlerterm im $k-ten$ Iterationsschritt gilt:
\begin{equation}
e^{k} = u^{*} - u^{k}.
\end{equation}

Betrachten wir nun den $(k+1)-ten$ Fehler und formen geschickt um, so gilt:
\begin{eqnarray}
e^{k+1} &=& u^{*} - u^{k+1} = u^{*} - \left( (Id - \omega D^{-1} A)u^{k} + \omega D^{-1}f \right) \notag \\
&=& \underbrace{u^{*} - u^{k}}_{= e^{k}} + \omega D^{-1} A u^{k} - \omega D^{-1}f = e^{k} + \omega D^{-1} (Au^{k} - f) \notag \\
&=& e^{k} + \omega D^{-1} (Au^{k} - Au^{*}) = e^{k} + \omega D^{-1} A(u^{k} - u^{*}) \notag \\
&=& e^{k} - \omega D^{-1} A e^{k} = (Id - \omega D^{-1} A) e^{k}.
\end{eqnarray}

\bild{omegaeins}{14cm}{Dies ist das Eigenwertspektrum für $\omega = 1$. Das Gesamtschrittverfahren hat keine Glättungseigenschaft. Es liegen nur wenige der Eigenwerte um die Null.}{Eigenwertspektrum der Jacobi Iterationsmatrix, bzw. der Iterationsmatrix des Jacobi-Relaxationsverfahrens mit Parameter $\omega = 1$.}

Da die $n$ Eigenvektoren $v_{i,j}$ eine Basis bilden, lässt sich der Fehler $e$ als Linearkombination der $v_{i,j}$ darstellen:
\begin{eqnarray}
e^{k+1} &=& \sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k+1}_{i,j} v_{i,j} = (Id - \omega D^{-1}A) \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} v_{i,j}\right) \notag \\
&=& \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} (Id - \omega \underbrace{D^{-1}}_{=\frac{1}{4} Id} A) v_{i,j}\right) = \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} (v_{i,j} - \frac {\omega} {4} \underbrace{A v_{i,j}}_{= \lambda_{i,j}(A)v_{i,j}})\right) \notag \\
&=& \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} \underbrace{(1 - \frac {\omega} {4} \lambda_{i,j}(A))}_{= \lambda_{i,j}(\mat{T}_{J_{\omega}})} v_{i,j}\right) = \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} \lambda_{i,j}(\mat{T}_{J_{\omega}}) v_{i,j}\right).
\end{eqnarray}

\bild{omegaeinhalb}{14cm}{Für $\omega = \frac{1}{2}$ ist gut zu erkennen, dass für $i,j$ nahe $N$ viele Eigenwerte nahe Null liegen.}{Eigenwertspektrum der Iterationsmatrix des Jacobi-Relaxationsverfahrens mit dem Parameter $\omega = \frac{1}{2}$.}\label{img.Jacobi2}

Da wir im Folgenden eine Fehlerglättung erreichen wollen, wählen wir nun $\omega = \frac {1} {2}$ (\subfigureautorefname{ 3.2}). Wir stellen fest, dass die Eigenwerte der Iterationsmatrix zwischen $0$ und $1$ liegen. Sind $i,j > \frac {N} {2}$ so werden die Fehleranteile wesentlich besser gedämpft, da die Eigenwerte hier nahe Null liegen, d.h. zugehörige Eigenvektoren werden nahezu eliminiert. Diese Eigenschaft nennt man die \textit{Glättungseigenschaft}. Wie die Abbildungen 3.4, 3.5 und 3.6 illustrieren, werden beim Glätten des Fehlers kurzwellige, stark oszillierende (hochfrequente) Fehleranteile nahezu ausgelöscht. Zwar heißt das nicht, dass der Fehler verschwindet oder kleiner wird, jedoch wird er glatt.\\
Es stellt sich z.B. heraus, dass der optimale Relaxationsparameter, der unabhängig von der Schrittweite $h$ gewählt werden kann, $\omega = \frac {4} {5}$ ist \cite{SAAD4}. In \subfigureautorefname{ 3.3} ist gut zu sehen, dass ein Großteil des Spektrums nahe Null liegt.
Betrachten wir nun die Eigenwerte der Iterationsmatrix für $\omega = 1$ (Jacobi-Verfahren), sieht man, dass die Eigenwerte für $i,j$ nahe Null oder $i,j$ nahe N den Fehler schlecht bis gar nicht dämpfen (\subfigureautorefname{ 3.1}). Diese liegen nahe 1 bzw. -1 und verändern die zugehörigen Eigenvektoren kaum. Als iteratives Verfahren ist $\omega = 1$ der optimale Parameter \cite{SAAD3}. Das Jacobi-Verfahren besitzt wegen seines Eigenwertspektrums keine Glättungseigenschaft.\\

\bild{omeganullacht}{14cm}{Für den optimalen Parameter $\omega = \frac{4}{5}$ ist gut zu erkennen, dass mehr Eigenwerte um die Null liegen, als beim Jacobi-Verfahren oder für den Parameter $\omega = \frac{1}{2}$ (orange Fläche) und somit eine gute Glättungseigenschaft besteht.}{Plot der Eigenwerte für den optimalen Parameter $\omega = \frac{4}{5}$ für das Jacobi-Relaxationsverfahren.}\label{img.Jacobi3}

Zum Abschluss dieses Abschnitts wollen wir ein konkretes Beispiel zur Glättungseigenschaft anführen. Angenommen wir wählen unseren Startvektor $u^{0} = \sin(\theta_{i})$, mit $i = 1,...,n$ und $\theta_{i}$ wie in \autoref{s.Eigenwerte und Eigenvektoren}. Außerdem sei die Poisson Gleichung aus \autoref{s.Beispiel einer Poisson Gleichung} gegeben. Wir wählen den Parameter $\omega = \frac{4}{5}$ für das Jacobi-Relaxationsverfahren. Und wir berechnen den Fehlerterm $e^{0} = u^{*} - u^{0}$ und plotten diesen. Der Fehler sieht zu Beginn wie folgt aus:

\bild{startError}{12cm}{Die Oszillation der Sinuswellen im Startfehler ist hier gut zu erkennen.}{Plot des Startfehlers mit stark oszillierenden Sinuswellen.}

Nun wollen wir uns noch die Plots nach einem und drei Iterationsschritten des Jacobi-Relaxationsverfahrens ansehen. Nach drei Schritten sind die starken Oszillationen verschwunden.

\bild{oneStepError}{12cm}{Nach einem Iterationsschritt ist bereits eine Glättung des Fehlers zu beobachten.}{Plot des Fehlers nach einem Glättungsschritt.}
\bild{threeStepError}{12cm}{Man sieht, dass bereits nach 3 Iterationsschritten der Fehler glatt ist.}{Geglätteter Fehler nach drei Iterationsschritten des Jacobi-Relaxationsverfahrens mit optimalen Parameter.}

\section{Das Verfahren der konjugierten Gradienten}\label{s.Das Verfahren der konjugierten Gradienten}

Das Verfahren der konjugierten Gradienten wurde 1952 von Hestenes und Stiefel erstmals vorgestellt. Es zeichnet sich durch Stabilität und schnelle Konvergenz aus. \\
Das CG-Verfahren (conjugate gradient) - wie es auch genannt wird - ist eine Projektions- und Krylow-Raum-Methode, worauf wir allerdings im Folgenden nicht näher eingehen werden.

\subsection{Definition (A-orthogonal)}\label{ss.A-orthogonal}
Sei $\mat{A}$ eine symmetrische, nicht singuläre Matrix. Zwei Vektoren $x,y \in \mathbb{R}^{n}$ heißen \textbf{konjugiert} oder \textbf{A-orthogonal}, wenn $x^{T}Ay = 0$ gilt.\\

Die A-Orthogonalität ist spezifisch für das CG-Verfahren. Bei der Aufstellung der Teilräume sind die zugehörigen Basisvektoren alle A-orthogonal, wie wir nach dem folgenden Satz sehen werden.

\subsection{Satz (Minimierungsfunktion)}
Sei $A\in\mathbb{R}^{n \times n}$ s.p.d. und
\begin{equation}
h(u) = \frac {1} {2} u^{T}Au - f^{T}u,
\end{equation}
wobei $f,u \in \mathbb{R}^{n}$. Dann gilt:
\begin{center}
h hat ein eindeutig bestimmtes Minimum und
\end{center}
\begin{equation}
Au^{*} = f \Longleftrightarrow h(u^{*}) = \underset{u\in\mathbb{R}^{n}}{\min} h(u)
\end{equation}

Einen kurzen Beweis hierzu findet man z.B. in \cite{DR6}.\\
Es ist also äquivalent, die Funktion $h(u)$ zu minimieren und das Gleichungssystem $Au = f$ zu lösen. Betrachtet man nun den Gradienten von $h(u)$, so gilt:
\begin{equation}
\nabla h(u) = Ax - f = -r.
\end{equation}

Da wir bei diesem Verfahren stets das Minimum in einem Teilraum $U_{k} \in \mathbb{R}$ suchen, also die Funktion $h(u)$ minimieren wollen, wird uns folgendes Lemma hilfreich sein:

\subsection{Lemma - (A-orthogonaler) Projektionssatz}\label{s.Projektionssatz}

Sei $U_{k}$ ein k-dimensionaler Teilraum des $\mathbb{R}^{n} \hspace{1mm} (k \le n)$, und $p^{0}, p^{1},...,p^{k-1}$ eine $\textit{A-orthogonale Basis}$ dieses Teilraums, also $\langle p^{i}, p^{j} \rangle _{A} = 0 \hspace{2mm} f\ddot{u}r \hspace{2mm} i \ne j$. Sei $v \in \mathbb{R}^{n}$, dann gilt für $u^{k} \in U_{k}$:

\begin{equation}
\|u^{k} - v\|_{A} = \underset{u \in U_{k}}{\min} \|u - v\|_{A}
\end{equation}

genau dann, wenn $u^{k}$ die $\textit{A-orthogonale Projektion}$ von $v$ auf $U_{k} = span\{p^{0},...,p^{k-1}\}$ ist. Außerdem hat $u^{k}$ die Darstellung

\begin{equation}
P_{U_{k,\langle \cdot,\cdot \rangle}}(v) = \textbf{u}^{k} = \sum_{j=0}^{k-1} \frac {\langle v, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j}
\end{equation}

Der Beweis zu diesem Lemma wird in \cite{DR7} erläutert.

\subsection{Allgemeiner Algorithmus der konjugierten Gradienten}\label{ss.Allgemeiner CG-Algorithmus}

Zur Erzeugung der Lösung von $u^{*}$ durch Näherungen $u^{1}, u^{2},...$ definieren wir folgende Teilschritte:

\begin{description}

\item[0.] Definiere Teilraum $U_{1}$ und bestimme $r^{0}$ mit beliebigen Startvektor $u^{0}$
\begin{equation}
U_{1} = span\{r^{0}\} \textnormal{, wobei } r^{0} = f - Au^{0}
\end{equation}

\item[1.] Bestimme eine A-orthogonale Basis
\begin{equation}
p^{0},...,p^{k-1} \textnormal{ von } U_{k}.
\end{equation}

\item[2.] Bestimme eine Näherungslösung $u^{k}$, so dass gilt:
\begin{equation}
\|u^{k} - u^{*}\|_{A} = \underset{u \in U_{k}}{\min} \|x - u^{*}\|_{A}.
\end{equation}
Wir berechnen also:
\begin{equation}
u^{k} = \sum_{j=0}^{k-1} \frac {\langle u^{*}, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j}.\label{eq.Projektion}
\end{equation}

\item[3.] Berechne erneut das Residuum und erweitere den Teilraum $U_{k}$
\begin{equation}
U_{k+1} = span\{p^{0},...,p^{k-1},r^{k}\} \textnormal{ wobei } r^{k} = f - Au^{k}.
\end{equation}

\end{description}

Nachdem man ein Residuum berechnet hat, startet der erste Iterationsschritt. Man erweitert seinen Teilraum um das Residuum und bestimmt daraufhin eine A-orthogonale Basis dieses Teilraumes. Ein gängiges Verfahren ist das Gram-Schmidt-Orthonormalisierungsverfahren. Die neue Näherungslösung bzgl. $U_{k}$ kann dann über den (A-orthogonalen) Projektionssatz bestimmt werden. Nachdem erneut ein Residuum berechnet wurde, startet der nächste Iterationsschritt.\\
Wegen \autoref{eq.Projektion} könnte man vermuten, dass die Lösung des Gleichungssystems $u^{*}$ zur Durchführung des Algorithmus bekannt sein muss. Die folgenden Lemmata werden zeigen, dass dem nicht so ist.

\subsection{Lemma}
Sei $u^{*} \in \mathbb{R}^{n}$ die Lösung von $Au = f$. Dann gilt für ein $y \in U_{k}$:
\begin{equation}
\langle u^{*}, y \rangle _{A} = \langle f, y \rangle
\end{equation}

\textbf{Beweis:}

Wir nutzen die Eigenschaften des A-Skalarproduktes aus:
\begin{equation}
\langle u^{*}, y \rangle _{A} = u^{{*}^{T}}Ay = y^{T}Au^{*} = y^{T}f = f^{T}y = \langle f, y \rangle. \notag
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

Nun wollen wir \autoref{eq.Projektion} neu formulieren.

\subsection{Lemma}
Sei $u^{*} \in \mathbb{R}^{n}$ die Lösung von $Au = f$ und $u^{k} \in \mathbb{R}^{n}$ die optimale Approximation von $u^{*}$ in $U_{k}$. Dann kann $u^{k}$ wie folgt berechnet werden:
\begin{equation}
u^{k} = u^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ mit } \alpha_{k-1} = \frac {\langle r^{0}, p^{k-1} \rangle} {\langle {p^{k-1}, Ap^{k-1}} \rangle}.\label{eq.Alpha}
\end{equation}

Für einen Beweis möchte ich auf \cite{DR8} verweisen.

\textbf{Bemerkung:}
$u^{k}$ kann dadurch mit wenig Aufwand aus $u^{k-1}$ und $p^{k-1}$ berechnet werden.

\subsection{Lemma}
Das Residuum $r^{k} \in \mathbb{R}^{n}$ kann einfach berechnet werden durch:
\begin{equation}
r^{k} = r^{k-1} - \alpha_{k-1}Ap^{k-1},
\end{equation}
wobei $\alpha_{k-1}$ wie in \autoref{eq.Alpha}.

\textbf{Beweis:}
\begin{eqnarray}
&&u^{k} = u^{k-1} + \alpha_{k-1}p^{k-1} \notag \\
&&\Longleftrightarrow Au^{k} = Au^{k-1} + \alpha_{k-1}Ap^{k-1} \notag \\
&&\Longleftrightarrow b - Au^{k} = b - Au^{k-1} - \alpha_{k-1}Ap^{k-1} \notag \\
&&\Longrightarrow r^{k} = r^{k-1} - \alpha_{k-1}Ap^{k-1}. \notag
\end{eqnarray}
\begin{flushright}
$\blacksquare$
\end{flushright}

Da wir nun $u^{k}$ und $r^{k}$ recht komfortabel bestimmen können, wollen wir im Folgenden noch eine Möglichkeit zeigen, wie die $p^{k}$ schnell zu berechnen sind.

\subsection{Satz (Bestimmung einer A-orthogonalen Basis)}
Durch
\begin{equation}
p^{k-1} = r^{k-1} - \sum_{j=0}^{k-2} \frac {\langle r^{k-1}, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j},
\end{equation}
wird die A-orthogonale Basis des Unterraums (Krylow-Raums) $U_{k} = span\{p^{0}, ..., p^{k-2},r^{k-1}\}$ zum Vektor $r^{k-1}$ bestimmt, wobei $p^{k-1},r^{k-1} \in \mathbb{R}^{n}$.

\textbf{Beweis:}

Der Beweis folgt direkt aus dem Gram-Schmidt-Orthonormalisierungsverfahren, welches allerdings einen hohen Rechenaufwand vorweist.\\
Wir wollen ohne Herleitung und Beweis (siehe dazu beispielsweise \cite{DR9}) angeben, wie man die $p^{k}$ effizienter bestimmen kann.

\subsection{Satz}
Für die Berechnung von $p^{k}$ gilt:
\begin{equation}
p^{k-1} = r^{k-1} - \frac {\langle r^{k-1}, Ap^{k-2} \rangle} {\langle p^{k-2}, Ap^{k-2} \rangle} p^{k-2}.
\end{equation}

Substituiert man geschickt einige Werte in den Skalarprodukten (ohne Beweis), führt das auf den Algorithmus der konjugierten Gradienten.

\subsection{Algorithmus der konjugierten Gradienten}\label{ss.Numerisches CG}

Gegeben ist eine symmetrisch positiv definite Matrix $\mat{A} \in \mathbb{R}^{n}$. Bestimme die Lösung Mithilfe eines \textit{beliebigen} Startvektors $u^{0} \in \mathbb{R}^{n}$ zu einer gegebenen rechten Seite $f \in \mathbb{R}^{n}$. Setze $\beta_{-1} = 0$ und berechne das Residuum $r^{0} = f - Au^{0}$. \\
Für $k = 1,2,...$, falls $r^{k-1} \ne 0$ berechne:

\begin{eqnarray}
p^{k-1} &=& r^{k-1} + \beta_{k-2}p^{k-2}, \textnormal{ wobei } \beta_{k-2} = \frac {\langle r^{k-1}, r^{k-1} \rangle} {\langle r^{k-2}, r^{k-2} \rangle} \textnormal{ mit } (k \ge 2) \notag \\
u^{k} &=& u^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ wobei } \alpha_{k-1} = \frac {\langle r^{k-1}, r^{k-1} \rangle} {\langle p^{k-1}, Ap^{k-1} \rangle} \notag \\
r^{k} &=& r^{k-1} - \alpha_{k-1}Ap^{k-1} \notag
\end{eqnarray}

Man muss in diesem Algorithmus pro Iterationsschritt lediglich zwei Skalarprodukte ausrechnen (die $r^{k-1}$-Skalarprodukte können für die $r^{k-2}$ nach der Berechnung des neuen Residuums wieder verwendet werden!) und eine Matrix-Vektor-Multiplikation durchführen. Somit erhält man einen Rechenaufwand von $\mathcal{O}(n^{2})$. Angewandt auf $\mat{A}_{2D}$ kann man - durch das Ausnutzen der Dünnbesetztheit und der Symmetrie - einen Aufwand von $\mathcal{O}(n)$ erreichen. Diesen erhält man durch eine geschickt gewählte Matrix-Vektor-Multiplikation, die die Struktur von $\mat{A}_{2D}$ ausnutzt.

An dieser Stelle soll noch ein kurzer Satz über die Konvergenz des Verfahrens folgen. Einen Beweis zur Konvergenz findet man u.a. in [GL].

\subsection{Satz (Konvergenz des CG-Algorithmus)}\label{ss.Konvergenz CG}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$ symmetrisch, positiv definit und seien $u,f,u^{*},u^{k} \in \mathbb{R}^{n}$, wobei $u^{*}$ die exakte Lösung des Gleichungssystems $Au = f$ und $u^{k}$ die approximierte Lösung durch das CG-Verfahren ist. Dann gilt für $k = 1,2,...$:
\begin{equation}
\| u^{k} - u^{*} \|_{A} \le 2 \left( \frac {\kappa_{2} (A) - 1} {\kappa_{2} (A) + 1} \right)^{k} \| u^{0} - u^{*} \|_{A} \label{eq.konvergenzabsch.}
\end{equation}
Da stets $\frac {\kappa_{2} (A) - 1} {\kappa_{2} (A) + 1} < 1$ gilt, sichert dieser Satz die Konvergenz des Algorithmus. Man sieht also, dass die Konvergenz des Verfahrens von der Kondition der Matrix $\mat{A}_{2D}$ abhängt.

\textbf{Beispiel:}

Für die zweidimensionale Poisson Matrix haben wir in \autoref{eq.kondition von PMatrix} gesehen, dass die Kondition der Matrix schlechter wird, je feiner das Gitter ist. Geht $h = \frac{1}{m}$ gegen Null, gilt also $m \longrightarrow \infty$, so folgt:
\begin{equation}
  \frac {\kappa(\mat{A}_{2D}) - 1} {\kappa(\mat{A}_{2D}) + 1} = 1 - \frac {2} {\kappa(\mat{A}_{2D}) + 1} \longrightarrow 1 \textnormal{ für } \kappa(\mat{A}_{2D}) \longrightarrow \infty.
\end{equation}
Für große $m$ und mit \autoref{eq.konvergenzabsch.} sieht man, dass das CG-Verfahren nur langsam konvergiert. Das Ziel muss es also sein, eine Ausgangsmatrix für unser System zu finden, die besser konditioniert ist als $\mat{A}_{2D}$. 

\textbf{Bemerkung:}

Das Verfahren der konjugierten Gradienten konvergiert im Allgemeinen nicht, wenn $\mat{A}$ nicht symmetrisch positiv definit ist.

\section{Vorkonditioniertes Verfahren der konjugierten Gradienten (PCG)}\label{s.PCG}

Das PCG-Verfahren (preconditioned conjugate gradient) ist eine optimierte Version des CG-Verfahrens. Wie wir in \autoref{ss.Matrixkondition} gezeigt haben, ist $\mat{A}_{2D}$ für feinere Gitter schlecht konditioniert. Und in \autoref{ss.Konvergenz CG} haben wir gesehen, dass die Konvergenz des CG-Verfahrens von eben dieser Matrix abhängt. Dies mindert natürlich die Effizienz des Verfahrens. Die Idee ist nun, die bei der Iteration zu Grunde liegende Matrix $\mat{A}$ durch eine ähnliche Matrix mit besserer Kondition zu ersetzen, damit sich das Konvergenzverhalten verbessert.

\subsection{Satz}

Sei $\mat{W} \in \mathbb{R}^{n \times n}$ s.p.d. dann gilt:
\begin{equation}
\mat{A}u = f \Longleftrightarrow \mat{W}^{-1} \mat{A} u = \mat{W}^{-1} f.
\end{equation}
Es macht also keinen Unterschied, ob wir $Au = f$ oder das äquivalente System lösen.

\subsubsection{Beweis:}

\begin{eqnarray}
\mat{A}u = f &\Longleftrightarrow& u = \mat{A}^{-1} \mat{Id} f \Longleftrightarrow u = \mat{A}^{-1} \mat{W} \mat{W}^{-1} f \notag \\
&\Longleftrightarrow& u = (\mat{W}^{-1} \mat{A})^{-1} \mat{W}^{-1} f \Longleftrightarrow \mat{W}^{-1} \mat{A} u = \mat{W}^{-1} f. \notag
\end{eqnarray}
\begin{flushright}
$\blacksquare$
\end{flushright}

Die Konditionszahl dieses Problem ist nun durch $\kappa_{2}(\mat{W}^{-1}\mat{A})$ bedingt. Das Ziel muss es also sein, $\mat{W}^{-1}$ so gut wie möglich zu wählen, damit die Kondition klein wird. Nun ist im Allgemeinen $\mat{W}^{-1}\mat{A}$ nicht s.p.d. Somit könnten wir zwar den CG-Alorithmus trotzdem anwenden, werden aber wegen dieser Tatsache möglicherweise keine Konvergenz erhalten. Um dies zu umgehen findet man einen Lösungsansatz, bei dem mit der Cholesky-Zerlegung eine entsprechende Umformung gefunden werden kann.\\
Die Formeln und Algorithmen zu den folgenden Cholesky-Zerlegungen gehen aus expliziten Formeln der LR-Zerlegung hervor. Da die Cholesky-Zerlegung eine LR-Zerlegung für symmetrisch positiv definite Matrizen darstellt, wollen wir diesen Aspekt nicht näher erläutern.

\subsection{Der Algorithmus des vorkonditionierten konjugierten Gradienten Verfahrens}\label{ss.Algorithmus PCG}

Gegeben seien $\mat{A}, \mat{W} \in \mathbb{R}^{n}$ s.p.d. Bestimme die Lösung Mithilfe eines \textit{beliebigen} Startvektors $u^{0} \in \mathbb{R}^{n}$ zu einer gegebenen rechten Seite $f \in \mathbb{R}^{n}$. Setze $\beta_{-1} = 0$, berechne das Residuum $r^{0} = b - Au^{0}$ und $z^{0} = \mat{W}^{-1}r^{0}$ (löse $\mat{W}z^{0} = r^{0}$).\\
Für $k = 1,2,...$, falls $r^{k-1} \ne 0$ berechne:

\begin{eqnarray}
p^{k-1} &=& z^{k-1} + \beta_{k-2}p^{k-2}, \textnormal{ wobei } \beta_{k-2} = \frac {\langle z^{k-1}, r^{k-1} \rangle} {\langle z^{k-2}, r^{k-2} \rangle} \textnormal{ mit } (k \ge 2), \notag \\
u^{k} &=& u^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ wobei } \alpha_{k-1} = \frac {\langle z^{k-1}, r^{k-1} \rangle} {\langle p^{k-1}, Ap^{k-1} \rangle}, \notag \\
r^{k} &=& r^{k-1} - \alpha_{k-1}Ap^{k-1}, \notag \\
z^{k} &=& \mat{W}^{-1}r^{k} \textnormal{ (löse } \mat{W}z^{k} = r^{k}). \notag
\end{eqnarray}

Wichtig hierbei ist, dass das Lösen von $\mat{W}z^{k} = r^{k}$ mit möglichst wenig Aufwand (ideal: $\mathcal{O}(n)$) berechnet werden soll. Für eine schnelle Lösung von $\mat{W} z^{k} = r^{k}$ verwenden wir eine unvollständige Cholesky-Zerlegung.

\subsection{Die unvollständige Cholesky-Zerlegung}\label{ss.ICCG}

Eine Matrix $\mat{A}$, die symmetrisch positiv definit ist, lässt sich durch eine Cholesky-Zerlegung in eine normierte untere Dreiecksmatrix $\mat{L}$ und eine rechte obere Dreicksmatrix $\mat{U}$ zerlegen, wobei gilt: $\mat{U} = \mat{D}\mat{L}^{T}$. \\
Mit dieser Zerlegung möchten wir nun unser System vorkonditionieren. Allerdings würde eine vollständige Cholesky-Zerlegung viele Nulleinträge in einer dünn besetzten Matrix auslöschen. Darum greift man auf eine unvollständige Cholesky-Zerlegung zurück, bei der die Stellen, an denen $\mat{A}$ Nulleinträge besitzt, in $L$ und $U$ ebenfalls Null bleiben.

\subsubsection{Definition (Muster E)}\label{sss.Muster E}

Ein Muster ist eine Teilmenge $E \subseteq \{1,...,n\} \times \{1,...,n\}$ für die gilt:
\begin{equation}
E = \{(i,j) | 1 \le i,j \le n, a_{i,j} \ne 0 \}.
\end{equation}

Dann lässt sich die Matrix $\mat{A}$ auch folgendermaßen approximieren:
\begin{equation}
\mat{A} \approx \tilde{\mat{L}} \tilde{\mat{L}}^{T}.
\end{equation}
wobei $\tilde{\mat{L}}, \tilde{\mat{L}}^{T}$ nicht die komplette Faktorisierung darstellt, sondern folgende Eigenschaften erfüllt:

\subsubsection{Eigenschaften der Matrix $\tilde{\mat{L}}$}\label{sss.Eigenschaften L CZ}
\begin{itemize}
\item $\tilde{\mat{L}}$ ist normierte untere Dreicksmatrix
\item Es gilt: $\tilde l_{i,j} = 0$, falls $(i,j) \notin E$
\end{itemize}

Natürlich ist diese Faktorisierung ungenauer, als die vollständige Zerlegung. Allerdings genügt sie, um die Kondition des Gleichungssystems in vielen Fällen zu verbessern. Um den Algorithmus effizient und den Rechenaufwand so klein wie möglich zu machen, werden Summen nur über Indizes aus dem Muster berechnet.

\subsubsection{Algorithmus der unvollständigen Cholesky-Zerlegung}\label{sss.Cholesky-Algorithmus}

Seien $\mat{A} \in \mathbb{R}^{n \times n}$ und $E$ das Muster zur Matrix $\mat{A}$. Setze $\tilde{\mat{L}} = \mat{Id}$, $\tilde{\mat{U}} = 0$. Berechne dann für $i=1,2,...,n$:
\begin{eqnarray}
&&\textnormal{for } k = 1,...,i-1: \textnormal{ if } (i,k) \in E,\notag \\
\tilde l_{i,k} &=& (a_{i,k} - \sum_{j=1}^{k-1} \tilde l_{i,j} \tilde u_{j,k})/ \tilde u_{k,k}; \notag \\
&&\textnormal{for } k = i,...,n: \textnormal{ if } (i,k) \in E, \notag \\
\tilde r_{i,k} &=& a_{i,k} - \sum_{j=1}^{i-1} \tilde l_{i,j} \tilde u_{j,k}; \notag
\end{eqnarray}

\textbf{Bemerkungen:}

\begin{itemize}
\item Die für den PCG-Algorithmus wichtige Matrix $\mat{W}$ wird nun definiert als: $\mat{W} = \tilde{\mat{L}} \tilde{\mat{L}}^{T}$. Dadurch wird auch $\mat{W}z^{k} = \tilde{\mat{L}} \tilde{\mat{L}}^{T}z^{k} = r^{k}$ schnell durch Vorwärts- bzw. Rückwärtseinsetzen lösbar.
\item Für viele Probleme zeigt sich, dass $\kappa_{2}(\mat{W}^{-1} \mat{\mat{A}}) \ll \kappa_{2} (A)$ gilt. \\
\end{itemize}

Es gibt einige Varianten zur Vorkonditionierung. Wir wollen uns an dieser Stelle noch mit einer weiteren auseinander setzen.

\subsection{Die modifizierte unvollständige Cholesky-Zerlegung}\label{ss.Modifizierte Cholesky}

Auch bei der modifizierten Methode des Verfahrens gehen wir vor, wie in \autoref{ss.ICCG}. Jedoch werden die Vorschriften für die Matrix $\tilde{\mat{L}}$ abgeändert:

\subsubsection{Eigenschaften der Matrix $\hat{\mat{L}}$}\label{sss.Eigenschaften L MCZ}

Sei $e = (1,1,...,1)^{T}$,

\begin{itemize}
\item $a_{i,j} = (\hat{\mat{L}} \hat{\mat{L}}^{T})_{i,j}$ für alle $(i,j) \in E, i \ne j$,
\item $\mat{A} e = \hat{\mat{L}} \hat{\mat{L}}^{T} e$, d.h. die Zeilensummen stimmen überein und
\item $\hat l_{i,j} = \hat r_{i,j} = 0$ für alle $(i,j) \notin E$.
\end{itemize}

\subsubsection{Algorithmus der modifizierten unvollständigen Cholesky-Zerlegung}\label{sss.Algorithmus MUCZ}

Seien $\mat{A} \in \mathbb{R}^{n \times n}$ s.p.d. und $E$ das Muster zur Matrix $\mat{A}$. Setze $\hat{\mat{L}} = \mat{Id}$ und $\hat{\mat{U}} = 0$. Berechne dann für $i=1,2,...,n$:
\begin{eqnarray}
&&\textnormal{drop } = 0;\notag\\
&&\textnormal{for } k = 1,...,i-1:\notag\\
&&s = \sum_{j=1}^{k-1} \hat l_{i,j} \hat u_{j,k};\notag\\
&&\textnormal{if } (i,k) \in E: \hat l_{i,k} = (a_{i,k} - s)/ \hat u_{k,k};\notag\\
&&\textnormal{else drop = drop + s};\notag \\
&&\textnormal{for } k = i,...,n: \notag \\
&&s = \sum_{j=1}^{i-1} \hat l_{i,j} \hat u_{j,k};\notag\\
&&\textnormal{if } (i,k) \in E: \hat r_{i,k} = a_{i,k} - s; \notag\\
&&\textnormal{else drop = drop + s};\notag \\
\hat u_{i,i} &=& \hat u_{i,i} -drop;
\end{eqnarray}

Wir setzen wieder $\mat{W} = \hat{\mat{L}} \hat{\mat{L}}^{T}$. \\

Weitere Verfahren zur Vorkonditionierung, wie beispielsweise das SSOR-Verfahren, sind u.a. in \cite{SAAD5} zu finden.

\subsection{Effiziente Implementierung der modifizierten unvollständigen Cholesky-Zerlegungen}

Gerade für diese Aufgabenstellung ist es von großem Interesse, wie der Algorithmus in Code umgesetzt wird. Da das Muster $E$ unabhängig von der Dimension $n$ ist haben beide Zerlegungen einen Rechenaufwand von $\mathcal{O}(n^{2})$. Auch wenn wir nur über das Muster $E$ iterieren, werden viele Werte der Matrix $\mat{A}$ überprüft. Dies kostet Rechenzeit.\\
Um dies zu optimieren, wird für $\mat{A}_{2D}$ eine weitere Matrix $\mat{B} \in \mathbb{R}^{n \times 5}$ eingeführt, die das Muster E ersetzt bzw. darstellt. Die 2D Poisson Matrix enthält maximal 5 Werte ungleich Null pro Zeile. Somit können in der $i - ten$ Zeile von $\mat{A}_{2D}$ maximal 5 Indizes $j$ auftauchen, für die gilt $a_{i,j} \ne 0$. In $\mat{B}$ werden die Indizes $j$ nacheinander abgespeichert. Sollten weniger als fünf Werte ungleich Null sein, so wird die zugehörige Zeile von $\mat{B}$ mit $-1$ aufgefüllt.

\textbf{Beispiel:}

Um eine Vorstellung von $\mat{B}$ zu bekommen, wählen wir $\mat{A}_{2D} \in \mathbb{R}^{9 \times 9}$, also $m = 4, N = 3$. Dann folgt:
\begin{equation}
B = 
\begin{pmatrix}
1 & 2 & (1+N) & -1 & -1\\
1 & 2 & 3 & (2+N) & -1\\
  &   & \vdots & &\\
(5-N) & 4 & 5 & 6 & (5+N)\\
  &   & \vdots & &\\
(8-N) & 7 & 8 & 9 & -1\\
(9-N) & 8 & 9 & -1 & -1
\end{pmatrix}
\end{equation}

Mit der Matrix $\mat{B}$ als Ersatz für das Muster $E$, ergibt sich folgender C++-Code für die modifizierten unvollständige Cholesky-Zerlegung:

\subsection{C++-Methode der MIC}\label{s.MIC}\lstinputlisting[language=C++,firstline=381,lastline=416]{code/Algorithms.cpp}

Der Zugriff auf die richtigen Matrixeinträge von $\mat{A}_{2D}$ über $\mat{B}$ erfolgt hier mit die Methode $A.HashMatrix[i]k] = b_{i,k}$. Für $i = 1,...,n$ und $k = 1,...,5$ wird eine Integer Variable $m = b_{i,k}$ gesetzt. In $m$ wird nun für die $i - te$ Zeile der Matrix $\mat{A}_{2D}$ der $j - te$ Zeilenindex gespeichert, für den gilt: $a_{i,m} \ne 0$.

Der Rechenaufwand beträgt unter diesen Bedingungen lediglich $\mathcal{O}(5n) \approx \mathcal{O}(n)$.

Analog würde diese Vorschrift für die unvollständige Cholesky-Zerlegung folgen, was wir hier nicht mehr explizit anführen wollen. Betrachtet man in \autoref{c.Vergleich} die Laufzeiten für den PCG-Algorithmus mit diesen Zerlegungen und der angeführten Implementation, sieht man, wie schnell und effizient die Berechnung vonstatten geht.

\chapter{Mehrgitterverfahren}\label{c.Mehrgitterverfahren}

In diesem Abschnitt wollen wir uns mit den Mehrgittermethoden befassen. Wie \autoref{c.IterativeVerfahren} gezeigt hat, spielt die Kondition der zugehörigen Matrix beim Lösen von Gleichungssystemen eine wichtige Rolle. Zwar konnten wir mit Vorkonditionierung diese Problem in den Griff bekommen, aber wie \autoref{c.Vergleich} zeigen wird, wächst die Anzahl der Iterationsschritte auch bei den vorkonditionieren Verfahren mit der Gitterweite $h$ mit. Mehrgitterverfahren hingegen sind Gitter unabhängig, d.h. die Anzahl der Iterationsschritte (oder Zyklen) hängt nicht davon ab, wie fein wir unser Gitter wählen (\cite{SAAD6}). Der Name für diese Verfahren leitet sich dauraus ab, dass wir gewisse Operationen nicht nur auf dem zugrunde liegenden Gitter ausführen, sondern auf gröbere Gitter wechseln. Dadurch erhalten wir mehrere Gitter beim Lösen des Gleichungssystems.\\
Die Idee dieser Methoden beruht auf der Fehlerglättung. Wir haben bereits festgestellt, dass beispielsweise das Jacobi-Relaxationsverfahren kurzwellige Fehler nach wenigen Iterationsschritten nahezu auslöscht. Zudem werden die niederfrequenten Fehleranteile auf dem feinen Gitter zu hochfrequenten Fehlern auf dem groben Gitter (siehe dazu beispielsweise \cite{STR}).\\
Wir wollen uns in diesem Kapitel an \cite{SAAD7} halten. Spezifische Matrizen für das zweidimensionale Poisson Problem wurden selbst aufgestellt.

\section{Grundlagen}\label{s.Idee MGM}

\begin{description}

\item[1.] Glättungseigenschaft der Jacobi-Relaxation \\
Die kurzwelligen Fehlerterme werden nach wenigen Iterationsschritten geglättet bzw. verschwinden nahezu (siehe \autoref{s.Glättungseigenschaft}).
\item[2.] Residuumsgleichung \\
Die für diesen Algorithmus wichtige Residuumsgleichung lautet:
\begin{equation}
\mat{A} e^{k} = r^{k}
\end{equation}
Die Lösung von $\mat{A} e^{k} = r^{k}$ ist äquivalent zur Lösung von $\mat{A}u=b$ für $e^{k} = 0$.

\end{description}

\textbf{Beweis:}

Das Residuum ist an der $k-ten$ Stelle definiert als 
\begin{equation}
r^{k} = b - \mat{A}u^{k}
\end{equation}
Der Fehler als
\begin{equation}
e^{k} = u^{*} - u^{k},
\end{equation}
wobei $u^{*}$ die exakte Lösung darstellt.
Betrachten wir jetzt nochmals \autoref{eq.Residuumsgleichung}:
\begin{equation}
\mat{A}e^{k} = r^{k} = b - \mat{A}u^{k} = 0 \textnormal{ für } e^{k} = 0.
\end{equation}
Somit ist
\begin{equation}
\mat{A}e^{k} = r^{k} \Longleftrightarrow \mat{A}u^{k} = b \textnormal{ falls } e^{k} = 0.
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

\section{Prolongation}

Bevor wir nun auf die Mehrgittermethoden explizit eingehen, müssen wir uns Gedanken darüber machen, wie wir von einem Gitter auf das andere kommen. Angenommen wir befinden uns auf dem groben Gitter $\Omega_{2h}$, so ist das Ziel auf ein feineres Gitter $\Omega_{h}$ mit wenig Rechenaufwand zu kommen und die Werte aus $\Omega_{2h}$ sollten auf $\Omega_{h}$ gut genähert abgebildet werden. Für die Prolongation wählen wir hierfür eine bilineare Interpolation.


\textbf{Bemerkung:}

Wir wählen der Einfachheit halber das $N$ stets so, dass gilt: $N = 2^{n} - 1$, wobei $n \in \mathbb{N}$. Somit können wir die Schrittweite $h$ auf jedem Gitter komfortabel bestimmen. Es gilt dann z.B. für $N_{h} = 2^{n} - 1$ und für $N_{2h} = 2^{n-1} - 1$.

\subsection{Interpolationsmatrix}

Sei $I^{h}_{2h}: \Omega_{2h} \longrightarrow \Omega_{h}$ eine Abbildung mit $I_{2h}^{h}(u_{2h}) = \mat{I}u_{2h} = u_{h}$, wobei $\mat{I} \in \mathbb{R}^{(2 \tilde N - 1)^{2} \times \tilde N^{2}}$ und $\tilde N$ die Anzahl der inneren Gitterpunkte in x- und y-Richtung auf dem groben Gitter sind. Dabei überführt die Matrix $\mat{I}$ Vektoren von $\Omega_{2h}$ auf $\Omega_{h}$. Sie ist \textit{nicht} quadratisch und kann verschiedene Gestalten haben,z.B.:

\begin{equation}
\mat{I} = \frac {1} {4}
\begin{pmatrix}
I_{1} & & &\\
I_{2} & & &\\
& I_{1} & &\\
& I_{2} & &\\
& & \ddots &\\
& & & I_{1}\\
& & & I_{2}
\end{pmatrix}.
\end{equation}

Für $I_{1},I_{2} \in \mathbb{R}^{(2\tilde N - 1) \times \tilde N}$ gilt:

\begin{equation}
\mat{I}_{1} =
\begin{pmatrix}
4 & & & &\\
2 & 2 & & &\\
& 4 & & &\\
& 2 & 2 & &\\
& & \ddots & &\\
& & & 4 &\\
& & & 2 & 2\\
& & & & 4\\
\end{pmatrix},
\hspace{2cm}
\mat{I}_{2} =
\begin{pmatrix}
2 & \hdots & 2 & & & &\\
4 & \hdots & 4 & & & &\\
& 2 & \hdots & 2 & & &\\
& 4 & \hdots & 4 & & &\\
& & \ddots & & & &\\
& & & 2 & \hdots & 2 &\\
& & & 4 & \hdots & 4 &\\
& & & & 2 & \hdots & 2\\
\end{pmatrix}.
\end{equation}

Wir haben hier die Full-Weighting-Matrix der Interpolation verwendet, da sie zu einem besseren Ergebnis führt. Sie berücksichtigt bei der Interpolation nicht nur Gitterpunkte, die in $\Omega_{2h}$, sowie in $\Omega_{h}$ existieren, sondern auch den jeweiligen Nachbarn. Es gibt andere Möglichkeiten der Interpolation, z.B. den Half-Weighting-Operator, auf die wir in dieser Arbeit nicht explizit eingehen wollen. Zur Veranschaulichung des Full-Weighting-Operators dient Abbildung 4.1. Es geht in jedes $u^{i,j}_{h}$ auch der gewichtete Wert aller Nachbarpunkte von $u^{i,j}_{2h}$ des groben Gitters mit ein.

\textbf{Bemerkung:}

Für den Fall von eindimensionalen Gittern hätte $\mat{I}$ folgende Darstellung:
\begin{equation}
\mat{I} = \frac {1} {2}
\begin{pmatrix}
1\\
2\\
1 & 1\\
  & 2\\
  & 1\\
  &   & \ddots\\
  &   &          & 1 & 1\\
  &   &          &   & 2\\
  &   &          &   & 1
\end{pmatrix}.
\end{equation}
Hier ist $\mat{I} \in \mathbb{R}^{N \times \tilde N}$.

\bild{interpolation}{12cm}{Bei der Interpolation bedienen sich die Punkte vom feinen Gitter (grün und gelb), bei den gewichteten Punkten des groben Gitters (grün). Direkte Nachbarn werden mit $\frac{1}{2}$, diagonale Nachbarn mit $\frac{1}{4}$ gewichtet.}{Illustration der Gewichtung im Falle einer Interpolation mit Full-Weighting-Operator.}\label{img.Prolongation}

Für das Umsetzen in Programmcode ist es natürlich ungünstig eine komplette Matrix-Vektor-Multiplikation zu implementieren, zumal eine Matrix dieser Größe enorm viel Speicherplatz erfordert. Aus diesem Grund geben wir die Interpolation in Komponentenschreibweise an:

\begin{eqnarray}
&&u_{h}^{2i-1,2j-1} = u_{2h}^{i,j} \hspace{0.1cm} i,j = 1,...,\tilde N, \\
&&u_{h}^{2i-1,2j} = \frac{1}{2} (u_{2h}^{i,j} + u_{2h}^{i,j+1}) \hspace{0.1cm} i = 1,...,\tilde N; j = 1,...,\tilde N - 1, \\
&&u_{h}^{2i,2j-1} = \frac{1}{2} (u_{2h}^{i,j} + u_{2h}^{i+1,j}) \hspace{0.1cm} i = 1,...,\tilde N - 1; j = 1,...,\tilde N, \\
&&u_{h}^{2i,2j} = \frac{1}{4} (u_{2h}^{i,j} + u_{2h}^{i,j+1} + u_{2h}^{i+1,j} + u_{2h}^{i+1,j+1}) \hspace{0.1cm} i,j = 1,...,\tilde N - 1.
\end{eqnarray}

Somit ist nun der Übergang vom groben zum feinen Gitter bekannt. Nun wollen wir noch die Gegenrichtung betrachten.

\section{Restriktion}

Sei $R_{h}^{2h}: \Omega_{h} \longrightarrow \Omega_{2h}$ mit $R_{h}^{2h}(u_{h}) = \mat{R}u_{h} = u_{2h}$ und $\mat{R} \in \mathbb{R}^{\tilde N^{2} \times (2 \tilde N - 1)^{2}}$. Diese Abbildungsvorschrift nennt man Restriktion. Auch hier gibt es unterschiedliche Methoden, wobei in diesem Abschnitt das Gegenstück zur obigen Interpolation - der Full-Weighting-Operator für die Restriktion - verwendet wird. Auch dieser stellt den exaktesten Übergang zwischen beiden Gittern dar und hat einen speziellen Bezug zur Matrix $\mat{I}$
\begin{equation}
\mat{R} = \frac {1} {4} \mat{I}^{T}
\end{equation}

\subsection{Restriktionsmatrix}

Dadurch ist die Matrixdarstellung gegeben durch:

\begin{equation}
R = \frac{1}{16}
\begin{pmatrix}
I_{1}^{T} & I_{2}^{T}\\
		  & I_{1}^{T} & I_{2}^{T}\\
		  &			  &			  & \ddots\\
		  &			  &			  &		   & I_{1}^{T} & I_{2}^{T}
\end{pmatrix},
\end{equation}

wobei $I_{1}^{T}, I_{2}^{T}$ die transponierten Matrizen von $I_{1}, I_{2}$ darstellen.

Für die Umsetzung in Code benutzen wir die Komponentenschreibweise:

Für ein $u_{2h}$ auf $\Omega_{2h}$ gilt:
\begin{eqnarray}
u_{2h}^{i,j} &=& \frac {1} {16} (4u_{h}^{i,j}+2(u_{h}^{i+1,j}+u_{h}^{i-1,j}+u_{h}^{i,j+1}+u_{h}^{i,j-1})\notag \\
&+& u_{h}^{i-1,j-1}+u_{h}^{i-1,j+1}+u_{h}^{i+1,j+1}+u_{h}^{i+1,j-1})
\end{eqnarray}

Das Vorgehen wird in Abbildung 4.2 illustriert.

\bild{restrict}{8cm}{Ausgehend von einem Punkt innerhalb des Gitters, bedient dieser sich der Werte seiner direkten ($\frac{2}{16}$ in gelb) und diagonalen Nachbarn ($\frac{1}{16}$ in grün). Der Punkt, in dem restringiert wird fließt mit einer Gewichtung von $\frac{4}{16}$ (rot) in den neuen Wert ein.}{Ausgehend von einem Punkt innerhalb des Gitters, ist hier die Gewichtung der Werte veranschaulicht. Jeder Wert wird zusätzlich mit einem Faktor $\frac{1}{16}$ multipliziert} \label{img.restriction}

\textbf{Bemerkung:}

Auch hier wollen wir noch die Restriktionsmatrix für den eindimensionalen Fall angeben:

\begin{equation}
\mat{R} = \frac {1} {4}
\begin{pmatrix}
1 & 2 & 1\\
  & 1 & 2 & 1\\
  &   &   &   & \ddots\\
  &   &   &   &        & 1 & 2 & 1
\end{pmatrix}
\end{equation}

\section{Transformation der Matrix}

Zum Abschluss sollte noch die Matrix $\mat{A}$ vom feinen Gitter auf das grobe Gitter transformiert werden. Befänden wir uns nicht auf dem Einheitsquadrat oder hätte $\mat{A}$ eine nicht so regelmäßige Struktur wie beispielsweise $\mat{A}_{2D}$, dann gilt folgende Transformationsvorschrift:

\begin{equation}
\mat{A}_{2h} = \mat{R}\mat{A}_{h}\mat{I},
\end{equation}

mit $\mat{A}_{2h},\mat{A}_{h},\mat{R},\mat{I}$ wie oben.\\
Da wir uns jedoch auf dem Einheitsquadrat befinden und $\mat{A}_{2D}$ eine günstige Struktur hat, wollen wir diesen Abschnitt nicht weiter vertiefen. Lediglich soll angegeben werden, wie $\mat{A}_{2h}$ in unserem Fall nach der Transformation aussieht.

\textbf{Beispiel:}

Sei $N = 7$ die Anzahl der inneren Gitterpunkte in x- und y-Richtung des feinen Gitters und $\tilde N = 3$ die Anzahl der Gitterpunkte in x- bzw. y-Richtung des groben Gitters. Außerdem seien $\mat{A}_{2h} \in \mathbb{R}^{9 \times 9}$, $\mat{A}_{h} \in \mathbb{R}^{49 \times 49}$, $\mat{I} \in \mathbb{R}^{49 \times 9}$ und $\mat{R} \in \mathbb{R}^{9 \times 49}$. So folgt:

\begin{equation}
\mat{R}\mat{A}_{2D_{h}}\mat{I} = \mat{R} \frac{1}{h^{2}}
\begin{pmatrix}
A_{1} & -Id \\
-Id & A_{2} & \ddots \\
 & \ddots & \ddots \\
 & & A_{6} & -Id\\
 & & -Id & A_{7}
\end{pmatrix}
\mat{I} = \frac{1}{(2h)^{2}}
\begin{pmatrix}
A_{1} & -Id \\
-Id & A_{2} & -Id \\
 & -Id & A_{3}
\end{pmatrix}
= \mat{A}_{2D_{2h}}
\end{equation}

Die Matrizen $\mat{A}_{2D_{ih}}$ sind also für alle $i \in \{2^{n} \in \mathbb{N} | n \in \mathbb{N} \}$ stets bekannt.

\section{Das Zweigitterverfahren}\label{s.Der Zweigitter-Algorithmus}

Wie in \autoref{s.Finite Differenzen} gesehen befinden wir uns bei der Diskretisierung der Poisson-Gleichung auf einem Gebiet $\Omega_{h} = (0,1)^{2}$ mit Schrittweite $h = \frac {1} {m}$. Nach der Ausführung von $\tilde k$ Iterationsschritten des Jacobi-Relaxationsverfahren sind auf diesem Gitter die kurzwelligen Fehler $e^{k} = u^{*} - u^{k}$ geglättet bzw. nahezu verschwunden. Zudem haben wir die Äquivalenz der Gleichung $\mat{A} u = f$ und $\mat{A} e = r$ für $e = 0$ gesehen.\\
Wir berechnen nach $\tilde k$ Glättungsschritten das Residuum $r^{k}$ und wollen sodann $\mat{A}_{h} e^{k}_{h} = r^{k}_{h}$ lösen. Die Lösung dieser Gleichung auf dem einem gröberen Gitter ist natürlich wesentlich günstiger, als auf dem feinen Gitter. Darum bringen wir $r^{k}_{h}$ durch Restriktion auf das gröbere Gitter und lösen die Residuumsgleichung dort direkt. Anschließend bringen wir den approximierten Fehlerterem $e_{2h}^{k}$ durch Prolongation zurück auf das feine Gitter und addieren $e_{h}^{k}$ zu $u_{h}^{k}$, da $u^{*} = u^{k} + e^{k}$. Abschließend wird $\tilde k$ mal nachgeglättet. Wir wiederholen dieses Vorgehen bis zur Konvergenz. Nachfolgend der Pseudocode für den Algorithmus des Zweigitterverfahrens:

\begin{eqnarray}
\textbf{while }                                 &||u^{k} - u^{*}|| < TOL \notag \\
&\textit{pre-smooth }                         &\textnormal{JacobiRelaxationMethod} \notag \\
&\textit{calculate residual }        &r^{k} = b - Au^{k} \notag \\
&\textit{restrict }                         &r^{k}_{2h}=Rr^{k}_{h} \notag \\
&                                                                &\mat{A}^{2h} = R\mat{A}^{h}I \notag \\
&\textit{set error }                        &e^{0}_{2h} = 0 \notag \\
&\textit{solve direct }                        &\mat{A}^{2h} e_{2h} = r^{k}_{2h} \notag \\
&\textit{prolongate }                        &e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }                        &u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth (optional) }        &\textnormal{JacobiRelaxationMethod} \notag \\
\textbf{end } \notag
\end{eqnarray}

Es bleiben zwei Fragen nun unbeantwortet:
\begin{description}
\item[1.] Wie soll die Residuumsgleichung auf dem gröberen Gitter gelöst werden?
\item[2.] Wie steht es mit der Konvergenz dieses Verfahrens? Besitzt es die nötige Rechengeschwindigkeit?
\end{description}

Die Antwort auf die Frage nach der Konvergenz werden wir in dieser Arbeit schuldig bleiben. Für ein weiteres Studium wird \cite{SAAD8} empfohlen.
Als Löser verwenden wir den PCG-Algorithmus aus \autoref{c.IterativeVerfahren} mit einer modifizierten unvollständigen Cholesky-Zerlegung.\\

Der klare Nachteil dieser Methode liegt natürlich am Lösen der Residuumsgleichung. Wählen wir ein sehr feines Gebiet $\Omega_{h}$ mit $m = 256$, also $h = \frac {1} {256}$, so liefert das Gebiet $\Omega_{2h}$ immer noch ein Gleichungssystem der Dimension $n = 127^{2}$. Ein System dieser Ordnung zu lösen erfordert großen Rechenaufwand, der in dieser Form nicht immer erwünscht ist. \\

\section{Mehrgitter-Algorithmen}\label{s.Mehrgitteralgorithmus}

Da also das Lösen der Residuumsgleichung auf dem groben Gitter einen Löser erfordert, der zusätzlichen Rechenaufwand bedeutet, ist der Zweigitter-Algorithmus nicht immer die erste Wahl zum Lösen eines Gleichungssystems.

Eine andere Methode ist, das Gitter immer gröber zu machen, bis das System direkt lösbar ist oder zumindest entsprechend klein ist, um dann wieder auf das feinste Gitter zurück zu kehren. Wir erweitern also das Zweigitterverfahren um Rekursion. Denn ruft sich die Funktion in jedem Iterationsschritt selbst auf und löst, sobald sie sich auf dem gröbsten Gitter befindet, erhalten wir folgende rekursive Funktion:

\begin{eqnarray}
\textbf{V-cycle }(u,b)                         & \notag \\
&\textbf{if }\textit{(coarsest grid) }&\textnormal{return } u_{finest grid}=\mat{A}^{-1}b \notag \\
&\textbf{else }                                                & \notag \\
&\textit{pre-smooth }                                 &\textnormal{JacobiRelaxationMethod} \notag \\
&\textit{calculate residual }                &r^{k} = b - Au^{k} \notag \\
&\textit{restrict }                                 &r^{k}_{2h}=Rr^{k}_{h} \notag \\
&                                                                        &\mat{A}^{2h} = R\mat{A}^{h}P \notag \\
&\textit{recursion }                                &e^{k}_{2h} = \textbf{V-cycle}(0,r^{k}_{2h}) \notag \\
&\textit{prolongate }                                &e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }                                &u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth }                                        &\textnormal{JacobiRelaxationMethod} \notag \\
&                                                                        &\textnormal{return } u_{h} \notag \\
\textbf{end}                                                & \notag
\end{eqnarray}

Die Schritte sind im wesentlichen die gleichen, als die beim Zweigitterverfahren. Es wird auf jedem Gitter eine a priori Glättung durchgeführt und das Residuum restringiert. Auf dem gröbsten Gitter wird die Residuumsgleichung mit $e = 0$ gelöst. Der Fehlerterm wird anschließend prolongiert und auf dem nächst feineren Gitter zum jeweiligen $u^{k}$ dazu addiert. Nach einer a posteriori Glättung wird das neu berechnete $u^{k}$ ebenfalls prolongiert. Sobald wir auf das feinste Gitter zurück gekehrt sind, erhalten wir eine neue Approximation für $u_{h}^{k}$.

\bild{V-cycle}{6cm}{\textbf{rot:} a priori Glättung, \textbf{gelb:} Restriktion, \textbf{grün:} direktes Lösen, \textbf{braun:} Prolongation, \textbf{blau:} a posteriori Glättun und \textbf{schwarz:} a posteriori Glättung und Korrektur des Fehlers.}{Veranschaulichung eines V-Zyklus in den Mehrgittermethoden.} \label{V-Cycle}

Dieser Algorithmus ist auch als V-Zyklus bekannt. Wie dieser Name zustande kommt, illustriert Abbildung 4.3. Nun gibt es eine weitere Variante, den W-Zyklus (illustriert in Abbildung 4.4). Durch zweifachen rekursiven Aufruf der Funktion entstehen wesentlich mehr Wechsel zwischen den Gittern. Natürlich ist die Rechenzeit durch häufigeres glätten, lösen, restringieren und prolongieren höher:

\begin{eqnarray}
\textbf{W-cycle }(u,b)                         & \notag \\
&\textbf{if }\textit{(coarsest grid) }&\textnormal{return } u_{finest grid}=\mat{A}^{-1}b \notag \\
&\textbf{else }                                                & \notag \\
&\textit{pre-smooth }                                 &\textnormal{JacobiRelaxationMethod} \notag \\
&\textit{calculate residual }                &r^{k} = b - Au^{k} \notag \\
&\textit{restrict }                                 &r^{k}_{2h}=Rr^{k}_{h} \notag \\
&                                                                        &\mat{A}^{2h} = R\mat{A}^{h}P \notag \\
&\textit{recursion }                                &\tilde e^{k}_{2h} = \textbf{W-cycle}(0,r^{k}_{2h}) \notag \\
&\textit{recursion }                                &e^{k}_{2h} = \textbf{W-cycle}(0,\tilde e^{k}_{2h}) \notag \\
&\textit{prolongate }                                &e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }                                &u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth }                                        &\textnormal{JacobiRelaxationMethod} \notag \\
&                                                                        &\textnormal{return } u_{h} \notag \\
\textbf{end}                                                & \notag
\end{eqnarray}

\bild{W-cycle}{12cm}{\textbf{rot:} a priori Glättung, \textbf{gelb:} Restriktion, \textbf{grün:} direktes Lösen, \textbf{braun:} Prolongation, \textbf{blau:} a posteriori Glättun und \textbf{schwarz:} a posteriori Glättung und Korrektur des Fehlers.}{Ein W-Zyklus der die einzelnen Schritte illustriert.} \label{W-Cycle}

Für manche Systeme erhält man durch schnellere Konvergenz eine kürzere Rechenzeit, als bei einem V-Zyklus. Im Falle der Poisson Gleichung ist dies nicht der Fall, wie \autoref{c.Vergleich} zeigen wird. Außerdem ist dort die Gitterunabhängigkeit schön zu sehen, da die Anzahl der benötigten Zyklen bis zur Konvergenz nahezu konstant ist.

\chapter{Implementierung und Beispiel}\label{c.Vergleich}

In diesem letzten Kapitel werden praktische Code-Beispiele gezeigt. Außerdem werden anhand eines Beispiels die Iterationsschritte und Rechenzeit der verschiedenen Verfahren betrachtet.

\section{Beispiel einer Poisson Gleichung}\label{s.Beispiel einer Poisson Gleichung}

Seien $f: \Omega \rightarrow \mathbb{R}$ und $g: \partial\Omega \rightarrow \mathbb{R}$ stetige Funktionen mit $f(x,y) = -4.$ und $g(x,y) = x^{2} + y^{2}$. Sei außerdem $\Omega = (0,1)\times(0,1) \in \mathbb{R}^{2}$. Gegeben ist das Randwertproblem
\begin{eqnarray}
        -\Delta u(x,y) &=& f(x,y) = -4 \textnormal{ in } \Omega \\
    u(x,y) &=& g(x,y) = x^{2} + y^{2} \textnormal{ auf } \partial \Omega
\end{eqnarray}
Gesucht ist eine Funktion $u(x,y)$, die diese Gleichung löst. \\
Offensichtlich löst der elliptische Paraboloid $u(x,y) = x^{2} + y^{2}$ die partielle Differentialgleichung, da $\partial_{xx}u(x,y) = \partial_{yy}u(x,y) = 2$. Allerdings wollen wir nun diese Lösung auch numerisch erhalten.\\
Die gewünschte Lösung sollte also folgendem Graphen ergeben:

\bild{MyFunction}{12cm}{Die analytische Lösung für diese Poisson-Gleichung war gegeben durch $u(x,y) = x^{2} + y^{2}$.}{Plot der Funktion $u(x,y) = x^{2} + y^{2}$, als analytische Lösung einer partiellen Differentialgleichung.}

Zu beachten ist, dass wir $\Omega_{h} \in (0,1)^{2}$ gewählt haben. Die Lösung unseres Systems sollte folgenden Graphen als Ergebnis haben:

\bild{MyFunctionOnSquare}{12cm}{Dieser Graph entspricht der Lösung der Poisson-Gleichung auf dem Einheitsquadrat mit $h = \frac{1}{1024}$. Als Ausgangsdaten wurde die Lösung durch einen Mehrgitter-V-Zyklus mit 3 Gittern verwendet mit $m = 1024$.}{Plot des Lösungsvektors nach einem V-Zyklus im Einheitsquadrat.}

\section{Bemerkung zur Implementierung in C++}

Das gesamte Programm wurde objektorientiert geschrieben, darum ist von Methoden und Klassen, nicht von Funktionen die Rede. In den folgenden Beispielen wollen wir die Lösung der Poisson Gleichung für verschiedene Verfahren betrachten. Dafür werden einige der Methoden dargestellt. Wir wollen nicht nur die Iterationsschritte genauer betrachten, sondern auch die Rechenzeit.\\
An manchen Stellen im Code kommt die Vermutung auf, dass es sich um Pseudocode handeln könnte. Dies ist natürlich nicht der Fall. Es wurden lediglich bestimmte Operatoren wie *,+,-,etc. überladen.

\section{Speicherung von $\mat{A}_{2D}$}

Für eine effiziente Implementierung ist die Speicherung der Poisson-Matrix ein wichtiges Kriterium. Da die Werte der Diagonale und die der Nebendiagonalen zu jeder Zeit bekannt sind und die Matrix symmetrisch ist, werden in der C++-Methode PoissonMatrix lediglich drei double-Werte gespeichert. Somit benötigen wir lediglich drei mal 8 Byte, also 24 Byte, Speicherplatz für die gesamte Matrix.\\
Für die Matrizen $\mat{L}$ und $\mat{R}$, deren Werte ebenfalls nur auf der Diagonale und den 4 Nebendiagonalen ungleich Null sind und die beide ebenfalls symmetrisch sind, gehen wir ähnlich vor. Da dort die Werte jedoch unterschiedlich sind, werden pro Matrix drei Vektoren (vector<double>) angelegt, die pro Matrix einen Speicherplatz von  $< 24n$ Bytes ausmachen. Hier hängt zwar der Speicherplatz von der Größe der Matrix ab, ist aber immer noch überschaubar.\\
Durch diese Art der Speicherung können wir die Matrix-Vektor-Multiplikation, die im Allgemeinen einen Rechenaufwand von $\mathcal{O}(n^{2})$ hat, anpassen, so dass der Aufwand auf $\mathcal{O}(n)$ sinkt.

\lstinputlisting[language=C++]{code/MatrixVector.cpp}

\section{Abbruchkriterien}

Um zu verstehen, warum wir Abbruchkriterien benötigen, hier ein kurzes Beispiel:\\
Das CG-Verfahren konvergiert nach maximal $n$ Schritten bei exakter Arithmetik. Wir bräuchten somit kein Abbruchkriterium, damit wir die optimale Lösung finden. Angenommen die Dimension der Matrix ist $n = 10^{6}$. Dann werden trotz der schnellen Konvergenz eine große Anzahl an Iterationen benötigt, nämlich gerade n. Um dies zu vermeiden, lässt man den Algorithmus abbrechen, sobald eine gewisse Toleranzgrenze erreicht ist. In der Praxis schätzt man beispielsweise das $k-te$ Residuum in der A-Norm oder der 2-Norm gegen $r^{0}$ ab. In diesem Beispiel wählen wir folgenden Ansatz:

\begin{equation}
\| u^{k} - u^{*} \|_{2} \le 10^{-3} \cdot \| u^{0} - u^{*} \|_{2},
\end{equation}

wobei $u^{*} \in \mathbb{R}^{n}$ die Lösung des Gleichungssystems darstellt.\\
Im Allgemeinen ist natürlich die Lösung der partiellen Differentialgleichung nicht bekannt. Hier existiert die analytische Lösung und wir können das Abbruchkriterium so wählen.

\textbf{Bemerkung:}

In \cite{DR4} findet man für einige dieser Verfahren und dem gewählten Abbruchkriterium Tabellen mit Vergleichswerten für $m = 40,80,160,320$. Da wir die Werte für diese $m$ bereits kennen, beschränken wir uns auf Werte für $m = 2^{n} - 1, n \in \mathbb{N}_{>0}$, um einen besseren Vergleich zu den Mehrgitterverfahren zu erhalten.

\section{Jacobi-Verfahren}\label{s.Jacobi mit Beispiel}

Wie wir bereits in \autoref{ss.Spektralradius Jacobi} gesehen haben, konvergiert das Jacobi-Verfahren nur langsam. Es überrascht darum nicht, dass einige Iterationsschritte nötig sind, um das Gleichungssystem zu lösen. Trotz einer effizienten Programmierung benötigt die Methode viel Rechenzeit. Dies illustriert Tabelle 5.1.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32 & 64 & 128 & 256 \\\hline\hline
\multirow{5}* & Iterationsschritte & 1340 & 5344 & 21341 & 85282 \\\cline{2-6}
& Rechenzeit in s &  0.17  & 2.89 & 47.22 & 771.03 \\\hline
\end{tabular}
\caption[Tabelle für das Jacobi-Verfahren mit Iterationsschritten und Rechenzeit.]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}\label{tab.Jacobi}

Es ist gut zu sehen, dass sich die Anzahl der Iterationsschritte vervierfacht, sobald $m$ verdoppelt wird.

\section{Jacobi-Relaxations-Verfahren}\label{s.JacobiRelax mit Beispiel}

Zunächst soll der C++-Code angegeben werden. Setzt man $\omega = 1$ erhält man obiges Jacobi-Verfahrens.

\lstinputlisting[language=C++,firstline=232,lastline=255]{code/Algorithms.cpp}

Der Code lässt gut erkennen, wie die Struktur von $\mat{A}_{2D}$ gut ausgenutzt werden kann.

\subsection{Parameter $\omega = \frac {4} {5}$}

In \autoref{ss.Spektralradius Jacobi Relax} haben wir gesehen, das der Spektralradius des Jacobi-Relaxationsverfahrens näher an eins liegt als der des Jacobi-Verfahrens, denn:
\begin{equation}
\cos (\pi h) < 1 - \omega(1 - \cos (\pi h)) < 1,
\end{equation}
für $\omega \in (0,1)$.\\
Aus diesem Grund benötigt das relaxierte Verfahren auch wesentlich mehr Iterationsschritte. Wir betrachten das Verfahren, für den Parameter $\omega = \frac{4}{5}$.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32  & 64 & 128 & 256 \\\hline\hline
\multirow{5}* & Iterationsschritte & 1676  & 6681 & 26676 & 106603 \\\cline{2-6}
& Rechenzeit in s &  0.23  & 3.86 & 62.95 & 1031.14 \\\hline
\end{tabular}
\caption[Tabelle für das Jacobi-Relaxationsverfahren mit Iterationsschritten und Rechenzeit.]{Es ist deutlich zu erkennen, dass das Jacobi-Relaxationsverfahrens als iterativer Löser ungeeignet ist.}
\vspace{2ex}\end{table}\label{tab.Parameter 2}

\section{CG-Verfahren}\label{s.CG mit Beispiel}

Da das CG-Verfahren eines der effizienteren Iterationsverfahren ist, sollten die Messwerte entsprechend gut sein. Man erkennt aber in Tabelle 5.3 die schlechte Kondition von $\mat{A}_{2D}$. Pro Verdopplung der Gitterweite, verdoppeln sich auch die Iterationsschritte. Die Rechenzeit für ein sehr feines Gitter ist ebenfalls nicht akzeptabel.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32 & 64 & 128 & 256 & 512 \\\hline\hline
\multirow{6}* & Iterationsschritte & 52 & 104 & 210 & 420 & 841  \\\cline{2-7}
& Rechenzeit in s & 0.02 & 0.15 & 1.33 & 10.96 & 87.76 \\\hline
\end{tabular}
\caption[Tabelle für das CG-Verfahren mit Iterationsschritten und Rechenzeit.]{Ein gutes iteratives Verfahren, dass jedoch für feine Gitter nicht die gewünschte Effizienz aufweist.}
\vspace{2ex}\end{table}\label{tab.CG}

Der C++-Code für das CG-Verfahren, wie auch im nächsten Abschnitt für das PCG-Verfahren ist verhältnismäßig lang. Wir wollen daher auf das Codebeispiel verzichten.

\section{PCG-Verfahren}\label{s.PCG mit Beispiel}

\subsection{Mit unvollständiger Cholesky-Zerlegung}

Auch wenn man eine klare Verbesserung zum Standardverfahren der konjugierten Gradienten sieht, sind die Ergebnisse immer noch nicht in einem akzeptablen Rahmen.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32  & 64 & 128 & 256 & 512 & 1024 \\\hline\hline
\multirow{7}* & Iterationsschritte & 16  & 32 & 63  & 126 & 251 & 502 \\\cline{2-8}
& Rechenzeit in s &  0.01  & 0.10 & 0.74 & 5.97 & 47.09 & 373.97 \\\hline
\end{tabular}
\caption[Tabelle für das PCG-Verfahren (mit unvollständiger Cholesky-Zerlegung) mit Iterationsschritten und Rechenzeit.]{Es ist anhand der Tabelle schön zu erkennen, dass die Anzahl der Schritte proportional mit der Gitterweite zunimmt.}
\vspace{2ex}\end{table}

Offensichtlich gilt für das vorkonditionierte Verfahren der konjugierten Gradienten, dass die Anzahl der Schritte ungefähr halb so groß ist, als die Gitterweite $m$. Für die feinsten Gitter ist die Rechenzeit allerdings ungenügend.

\subsection{Mit modifizierter unvollständiger Cholesky-Zerlegung}

Im Fall der modifizierten unvollständigen Cholesky-Zerlegung erwartet man nun einen deutlichen Effekt auf Anzahl der Iterationsschritte und Rechenzeit.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32  & 64 & 128 & 256 & 512 & 1024 \\\hline\hline
\multirow{7}* & Iterationsschritte & 7  & 9 & 12  & 17 & 24 & 33 \\\cline{2-8}
& Rechenzeit in s &  0.01  & 0.03 & 0.17 & 0.96 & 5.12 & 27.07 \\\hline
\end{tabular}
\caption[Tabelle für das PCG-Verfahren (mit modifizierter unvollständiger Cholesky-Zerlegung) mit Iterationsschritten und Rechenzeit.]{Es ist deutlich zu erkennen, dass diese Variante des PCG-Verfahrens die effizientere von beiden darstellt.}
\vspace{2ex}\end{table}\label{tab.MIC}

Wie Tabelle 5.5 nun eindrucksvoll zeigt, brauchen wir für das feinster Gitter maximal $33$ Iterationsschritte. Auch die Rechenzeit ist mit $27.07$ Sekunden gut. Wie wir im nächsten Abschnitt sehen werden, sind lediglich die Mehrgittermethoden schneller und effizienter.

\section{Das Mehrgitterverfahren}\label{s.Multigrid mit Beispiel}

Auch wenn der Zweigitteralgorithmus nicht immer die beste Wahl zur Lösung einer partiellen Differentialgleichung ist, stellt er speziell für die Poisson Gleichung eine äußerst effiziente Methode dar. Besonders bemerkenswert ist bei den Mehrgitterverfahren die Feinheit des Gitters, dass mit einer maximalen Schrittweite von $h = \frac{1}{4096}$ diskretisiert werden kann.

\subsection{Zweigitterverfahren}\label{ss.Zweigitterverfahren mit Beispiel}

Als Löser auf dem gröbsten Gitter verwenden wir das PCG-Verfahren mit einer modifizierten unvollständigen Cholesky-Zerlegung verwendet.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32  & 64 & 128 & 256 & 512 & 1024 & 2048 & 4096 \\\hline\hline
\multirow{9}* & Anzahl der Zyklen & 3  & 3 & 3  & 3 & 3 & 3 & 3 & 3 \\\cline{2-10}
& Rechenzeit in s &  0.01  & 0.02 & 0.13 & 0.55 & 2.54 & 12.08 & 57.66 & 282.13 \\\hline
\end{tabular}
\caption[Tabelle für einen Zweigitteralgorithmus mit Anzahl der Zyklen und Rechenzeit.]{Das Lösen auf dem groben Gitter erfolgt mit der modifizierten unvollständigen Cholesky-Zerlegung.}
\vspace{2ex}\end{table}

Ein Iterationsschritt ist ein Zyklus vom feinen auf das grobe Gitter und zurück.

\subsection{V-Zyklus}\label{ss.V-Zyklus mit Beispiel}

Betrachten wir nun mehr als zwei Gitter, stellt sich durch "trial and error" heraus, dass für die Poisson-Gleichung ein 3-Gitter-V-Zyklus die günstigste Variante darstellt. Nachfolgend sind die durchaus imposanten Werte dargestellt:

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32  & 64 & 128 & 256 & 512 & 1024 & 2048 & 4096 \\\hline\hline
\multirow{9}* & Anzahl der Zyklen & 5  & 4 & 4  & 4 & 4 & 4 & 4 & 4 \\\cline{2-10}
& Rechenzeit in s &  0.00  & 0.02 & 0.01 & 0.47 & 1.97 & 8.30 & 35.44 & 154.53 \\\hline
\end{tabular}
\caption[Tabelle für einen V-Zyklus mit Anzahl der Zyklen und Rechenzeit.]{Die Messwerte für einen V-Zyklus mit einem feinen und zwei groben Gittern.}
\vspace{2ex}\end{table}

Wie nun deutlich wird, sind beim Zweigitterverfahren, wie auch bei der Verwendung eines V-Zyklus, die Anzahl der Zyklen nahezu konstant. Es ist also nicht vom Gitter abhängig, wie oft ein Zyklus durchlaufen werden muss, damit der Algorithmus konvergiert.\\
Zwar benötigen wir beim V-Zyklus ca. einen Durchlauf mehr, dafür hat dieser eine geringer Rechenzeit, da er auf einem noch gröberen Gitter lösen kann.\\
Vergleichen wir nun die Mehrgittermethoden, insbesondere V-Zyklen, mit dem PCG-Verfahren, so sind deutliche Unterschiede zu erkennen. Das schnellere der vorkonditionieren CG-Verfahren war mit einer modifizierten unvollständigen Cholesky-Zerlegung. Aber auch bei diesem Verfahren, ist die Anzahl der Iterationsschritte mit kleinerer Gitterweite $h$ gewachsen. Auch die Rechenzeit liegt für ein Gitter mit $m = 1024$ weit über denen der Mehrgittermethoden. Bei diesen war es durch ihre Gitterunabhängigkeit sogar möglich, ein noch feineres $m$ als dieses zu wählen.

\subsection{W-Zyklus}\label{ss.W-Zyklus mit Beispiel}

Abschließend werfen wir noch einen Blick auf einen W-Zyklus. Auch hier stellt sich heraus, dass drei Gitter am günstigsten sind.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|l|l|l|l|}\hline
\multicolumn{2}{|c||}{$m$} & 32  & 64 & 128 & 256 & 512 & 1024 & 2048 & 4096 \\\hline\hline
\multirow{9}* & Anzahl der Zyklen & 5  & 5 & 5  & 4 & 4 & 4 & 4 & 3 \\\cline{2-10}
& Rechenzeit in s &  0.01  & 0.05 & 0.22 & 0.82 & 3.74 & 17.31 & 81.88 & 297.59 \\\hline
\end{tabular}
\caption[Tabelle für einen W-Zyklus mit Anzahl der Zyklen und Rechenzeit.]{Auch bei einem W-Zyklus sieht man deutlich, dass die Anzahl der benötigten Zyklen unabhängig von der Gitterweite ist.}
\vspace{2ex}\end{table}

Zwar braucht der W-Zyklus nicht mehr Iterationsschritte, jedoch ist der Rechenaufwand höher. Es werden pro Zyklus mehr Restriktionen und Interpolationen benötigt. Zudem werden auf jedem Gitter Glättungen durchgeführt. Und es wird öfter direkt gelöst. Dies benötigt mehr Rechenzeit, was durch die Tabelle belegt wird. \\

Die Implementierung der drei Mehrgittermethoden ist die gleiche. Es wird lediglich beim Aufruf der C++-Methode die Anzahl der Gitter mitgegeben und ob es ein V- oder W-Zyklus sein soll. Wählt man zwei Gitter, so fungiert der Algorithmus als Zweigitterverfahren. Dieser sieht dann wie folgt aus:

\subsection{Mehrgitteralgorithmus C++-Methode}\label{s.mgm}\lstinputlisting[language=C++,firstline=151,lastline=170]{code/Algorithms2.cpp}

\chapter{Fazit}

Diese Arbeit hat gezeigt, wie die Poisson Gleichung auf ihrem Gebiet diskretisiert werden kann. Das daraus resultierende Gleichungssystem $\mat{A} u = f$ ermöglicht uns auf eine Bandbreite von Lösungsverfahren zurück greifen zu können. Wir haben gesehen, dass iterative Verfahren ihre Daseinsberechtigung haben, da einige von ihnen schnell sehr gute Lösungen des Gleichungssystems liefern. Allen voran ist hier das Verfahren der konjugierten Gradienten. Konditioniert man die Ausgangsmatrix geschickt vor, so können wir große Systeme effizient lösen. Das Umsetzen in Programmcode ist für die vorkonditionieren Verfahren nicht immer einfach, vor allem weil man versuchen möchte, die Rechenzeit so klein wie möglich zu halten.\\
Die Implementierung der Mehrgittermethoden dagegen ist weitaus einfacher. Jedoch ist hier darauf zu achten, wie man die Poisson Gleichung aufstellt und wie Operationen wie Restriktion und Prolongation geschickt programmiert werden. Hat man ein fertigs Programm, so wird man mit einer Gitter unabhängigen Anzahl an Zyklen und enorm kurzer Rechenzeit belohnt.\\
Die Mehrgittermethoden stellen zur Lösung der Poisson Gleichung ein hervorragendes Werkzeug dar. Es ist schwer vorstellbar, dass in naher Zukunft ein ähnlich effizienter Algorithmus entwickelt wird, um die Lösung dieser Gleichung zu ermöglichen.

\chapter{C++-Code}
\section{Hauptprogramm}\lstinputlisting[language=C++]{code/PoissonSolver.cpp}
\section{Klassen}\lstinputlisting[language=C++]{code/classes.h}
\section{Poisson Matrix Klassen}\lstinputlisting[language=C++]{code/PoissonMatrix.cpp}
\section{Cholesky-Zerlegung Klassen}\lstinputlisting[language=C++]{code/LUMatrices.cpp}
\section{Vektor Klassen}\lstinputlisting[language=C++]{code/Vector.cpp}
\section{Algorithmen}\lstinputlisting[language=C++]{code/Algorithms.cpp}
\section{Makefile}\lstinputlisting[language=make]{code/Makefile}
\section{Gnuplot Datei}\lstinputlisting[language=Gnuplot]{code/plot.gpl}

\chapter*{Danksagung}
\addcontentsline{toc}{chapter}{Danksagung}

\begin{large}
An dieser Stelle möchte ich mich noch bei einigen Menschen bedanken, die mich bei der Erstellung dieser Bachelorarbeit unterstützt und ausgehalten haben:
\begin{itemize}
\item \textit{Meiner Mama}: Für die Unterstützung, das Vertrauen in mich und die große Geduld mit mir.
\item \textit{Meiner Freundin}: Ohne dich, Sylvia, wäre ich nie durch dieses Studium gekommen. Deine Hilfe, dein Verständnis und dein Zuspruch haben mir sehr geholfen.
\item \textit{Meinen besten Freunden}: Obwohl ihr Mathematik alle nicht sonderlich mögt, habt ihr euch meine scheinbar endlosen Vorträge immer wieder angehört.
\item \textit{Christopher Rupprecht}: Die gute Hilfestellung hat mir gerade bei meinen Algorithmen sehr weiter geholfen.
\item \textit{Prof. Dr. Harald Garcke}:  Vielen Dank für die kompetente Unterstützung und die investierte Zeit. Es ist nicht selbstverständlich, dass ein Professor immer zur Verfügung steht und ein offenes Ohr für seine Studenten hat. Durch ihre Einbringung und Hilfe war es mir möglich diese Arbeit auf ein gutes Niveau zu bringen. Vielen Dank für die hervorragende Betreuung!
\end{itemize}
\end{large}

\begin{thebibliography}{56}

\bibitem[ALO1]{ALO1}
Götz Alefeld, Ingrid Lenhardt und Holger Obermaier,
Parallele numerische Verfahren,
Seiten 18-19,
Springer-Verlag Berlin Heidelberg,
2002.
\bibitem[ALO2]{ALO2}
Götz Alefeld, Ingrid Lenhardt und Holger Obermaier,
Parallele numerische Verfahren,
Seite 21,
Springer-Verlag Berlin Heidelberg,
2002.
\bibitem[DR1]{DR1}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seiten 470-472,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR2]{DR2}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seite 479,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR3]{DR3}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seite 480,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR4]{DR4}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seiten 549-583,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR5]{DR5}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seiten 550,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR6]{DR6}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seiten 566,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR7]{DR7}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seiten 567,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR8]{DR8}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seiten 569,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[DR9]{DR9}
Wolfgang Dahmen und Arnold Reusken,
Numerik für Ingeneure und Naturwissenschaftler,
2.,korrigierte Auflage,
Seiten 570,
Springer-Verlag Berlin Heidelberg,
2008, 2006.
\bibitem[GL]{GL}
G. H. Golub, C. F. van Loan,
Matrix Computations,
3. Aufl.,
Oxford,
University Press,
1996.
\bibitem[SAAD1]{SAAD1}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seite 411,
SIAM Verlag,
2003.
\bibitem[SAAD2]{SAAD2}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seiten 414-416,
SIAM Verlag,
2003.
\bibitem[SAA3]{SAA3}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seiten 415,
SIAM Verlag,
2003.
\bibitem[SAAD4]{SAAD4}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seiten 416,
SIAM Verlag,
2003.
\bibitem[SAAD5]{SAAD5}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seiten 105-106,
SIAM Verlag,
2003.
\bibitem[SAAD6]{SAAD6}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seiten 407,
SIAM Verlag,
2003.
\bibitem[SAAD7]{SAAD7}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seiten 419-429,
SIAM Verlag,
2003.
\bibitem[SAAD8]{SAAD8}
Yousef Saad,
Iterative Methods for sparse linear systems,
2nd ed.,
Seiten 435-437,
SIAM Verlag,
2003.
\bibitem[STR]{STR}
Gilbert Strang,
Wissenschaftliches Rechnen,
ab Seiten 661-676,
Springer-Verlag Berlin Heidelberg,
2010.

\end{thebibliography}

\end{document}