\input{arbeit-vorlage-praeambel.tex} % Importiere die Einstellungen aus der Präambel
% hier beginnt der eigentliche Inhalt
\begin{document}
\pagenumbering{Roman} % große Römische Seitenummerierung
\pagestyle{empty}

% Titelseite
\clearscrheadings\clearscrplain

\begin{center}
\begin{Huge}
Fakultät für Mathematik\\
\vspace{3mm}
\end{Huge}{\Large Universität Regensburg}\\

\vspace{20mm}
\begin{Large}
Ein Vergleich des Verfahrens der konjugierten Gradienten und Mehrgittermethoden, angewandt auf die diskretisierte Poisson-Gleichung\\
\end{Large}
\vspace{8mm}
Bachelor-Arbeit\\
\vspace{0.4cm}
\vspace{2 cm}
Michael Bauer \\
Matrikel-Nummer 1528558\\
\vspace{8cm}
\begin{tabular}{ll}
{\bf Erstprüfer}&Prof. Garcke\\
{\bf Zweitprüfer}&Prof. Blank\\
\end{tabular}

\end{center}
\clearpage


\pagestyle{useheadings} % normale Kopf- und Fußzeilen für den Rest

\tableofcontents
% \listoffigures
% \listoftables

%\chapter*{Symbolverzeichnis}\label{s.sym}
%\addcontentsline{toc}{chapter}{Symbolverzeichnis}
%\markboth{Symbolverzeichnis}{Symbolverzeichnis}
%\section*{Allgemeine Symbole}\label{s.sym.alg}
% \begin{flushleft}\begin{tabularx}{\textwidth}{l|X}
% Symbol & Bedeutung\\\hline
% $a$ & der Skalar $a$ \\
% $\vec{x}$ & der Vektor $\vec{x}$\\
% $\mat{A}$ & die Matrix $\mat{A}$\\
% \end{tabularx}\end{flushleft}




% richtiger Inhalt
\chapter{Einleitung}\label{c.Einleitung}
\pagenumbering{arabic} % ab jetzt die normale arabische Nummerierung

Viele Prozesse in den Naturwissenschaften, wie Biologie, Chemie und Physik, aber auch der Medizin, Technik und Wirtschaft lassen sich auf partielle Differentialgleichungen (PDG) zurückführen. Das Lösen solcher Gleichungen ist allerdings nicht immer möglich, oder aufwendig. \\
Eine PDG, die vor Allem in der Physik häufige Verwendung findet, ist die Poisson-Gleichung – eine elliptische partielle Differentialgleichung zweiter Ordnung. So genügt diese Gleichung beispielsweise dem elektrostatischen Potential $u$ zu gegebener Ladungsdichte $f$, aber auch dem Gravitationspotential $u$ zu gegebener Massendichte $f$. \\
Methoden aus der numerischen Mathematik ermöglichen uns nun das Lösen von partiellen Differentialgleichungen mittels computerbasierten Algorithmen. Hierbei wird jedoch nicht die Lösung direkt bestimmt, sondern versucht eine exakte Approximation der Lösung zu erhalten. Dabei ist es wichtig, dass der zugrunde liegende Algorithmus effizient ist. \\
Um nun die Lösung einer partiellen Differentialgleichung mittels effizienten Algorithmus bestimmen zu können, müssen wir uns im Vorfeld Gedanken darüber machen, wie wir diese am besten erhalten. Eine der zentralen Methoden der Numerik sind Finite Differenzen. Hierbei diskretisiert man das Gebiet, auf dem die PDG definiert ist und führt die Gleichung auf ein lineares Gleichungssystem zurück. \\
Eine Möglichkeit ein solches lineares Gleichungssystem zu lösen sind iterative Verfahren. Somit hat man ein großartiges Werkzeug, dass eine Lösung innerhalb weniger Iterationsschritte berechnet und auch mit sehr großen Systemen gut zurecht kommt. Natürlich gilt das nicht für jedes Verfahren, weshalb wir über die Verfahren sprechen möchten, die diese Kriterien erfüllen. \\
Abschließend wollen wir uns dann noch mit Mehrgittermethoden beschäftigen. Sie stellen die wohl effizienteste und modernste Methode dar, ein lineares Gleichungssystem zu lösen.


\chapter{Diskretisierung der Poisson-Gleichung im $\mathbb{R}^{2}$}\label{c.Diskretisierte Poisson-Gleichung}

\section{Definition (Poisson-Gleichung)}\label{s.Poisson-Gleichung}

Sei $\Omega = (0,1)\times(0,1) \in \mathbb{R}^{2}$ ein beschränktes, offenes Gebiet. Gesucht wird eine Funktion $u(x,y)$, die das Randwertproblem
\begin{eqnarray}
-\Delta u(x,y) &=& f(x,y) \textnormal{ in } \Omega, \\
u(x,y) &=& g(x,y) \textnormal{ in } \partial \Omega\label{eq.Dirichlet1}
\end{eqnarray}
löst.
Dabei seien $f: \Omega \rightarrow \mathbb{R}$ und $g: \partial\Omega \rightarrow \mathbb{R}$ stetige Funktionen und es bezeichnet $\Delta := \sum\limits_{k=1}^{n} \frac {\partial^{2}} {\partial x_{k}^{2}}$ den Laplace-Operator. Für die Poisson-Gleichung im $\mathbb{R}^{2}$ gilt dann:
\begin{eqnarray}
-\Delta u(x,y) &=& \frac {\partial^{2} u(x,y)} {\partial x^{2}} + \frac {\partial^{2} u(x,y)} {\partial y^{2}} = f(x,y) \textnormal{ in } \Omega, \\
u(x,y) &=& g(x,y) \textnormal{ in } \partial \Omega.\label{eq.Dirichlet2}
\end{eqnarray}
\autoref{eq.Dirichlet1} bzw. \autoref{eq.Dirichlet2} nennt man Dirichlet-Randbedingung.\\

\textbf{Beachte:} $\partial_{xx}u(x,y) = \frac {\partial^{2}u(x,y)} {\partial x^{2}}$.

Um diese (elliptische) partielle Differentialgleichung nun in $\Omega$ zu diskretisieren, bedarf es der Hilfe der Finiten Differenzen Methode.

\section{Finite Differenzen-Methode für die Poisson-Gleichung}\label{s.Finite Differenzen}

\subsection{Zentraler Differenzenquotient zweiter Ordnung}\label{ss.Differenzenquotient zweiter Ordnung}

Wir betrachten ein $(x,y) \in \Omega$ beliebig. Dann gilt für $h > 0$ mit der Taylorformel

\begin{equation}
u(x+h,y) = \sum_{k = 0}^{n} \frac {h^{k}} {k!} \frac {\partial^{k}u(x,y)} {\partial x^{k}} \approx u(x,y) + h \partial_{x} u(x,y) + \frac {h^{2}} {2!} \partial_{xx} u(x,y) + \mathcal{O}(h^{3}),\label{eq.Partiall x+h}
\end{equation}
\begin{equation}
u(x-h,y) = \sum_{k = 0}^{n} (-1)^{k} \frac {h^{k}} {k!} \frac {\partial^{k}u(x,y)} {\partial y^{k}} \approx u(x,y) - h \partial_{x} u(x,y) + \frac {h^{2}} {2!} \partial_{xx} u(x,y) - \mathcal{O}(h^{3}).\label{eq.Partiall x-h}
\end{equation}

Analog können wir diese Betrachtung für $u(x,y+h)$ und $u(x,y-h)$ machen:

\begin{equation}
u(x,y+h) = \sum_{k = 0}^{n} \frac {h^{k}} {k!} \frac {\partial^{k}u(x,y)} {\partial y^{k}} \approx u(x,y) + h \partial_{y} u(x,y) + \frac {h^{2}} {2!} \partial_{yy} u(x,y) + \mathcal{O}(h^{3}),
\end{equation}
\begin{equation}
u(x,y-h) = \sum_{k = 0}^{n} (-1)^{k} \frac {h^{k}} {k!} \frac {\partial^{k}u(x,y)} {\partial y^{k}} \approx u(x,y) - h \partial_{y} u(x,y) + \frac {h^{2}} {2!} \partial_{yy} u(x,y) - \mathcal{O}(h^{3}).
\end{equation}

Löst man nun \autoref{eq.Partiall x+h} und \autoref{eq.Partiall x-h} jeweils nach $\partial_{xx} u(x,y)$ auf und addiert die zwei Gleichungen, so erhält man:

\begin{equation}
\partial_{xx} u(x,y) + O(h^{2}) = \frac {u(x-h,y) - 2u(x,y) + u(x+h,y)} {h^{2}}.
\end{equation}

Ebenso lösen wir nach $\partial_{yy} u(x,y)$ auf und erhalten:
\begin{equation}
\partial_{yy} u(x,y) + O(h^{2}) = \frac {u(x,y-h) - 2u(x,y) + u(x,y+h)} {h^{2}}.
\end{equation}

Diese Näherungen nennt man den \textbf{zentralen Differenzenquotienten der zweiten Ableitung}. $O(h^{2})$ ist ein Term zweiter Ordnung und wird vernachlässigt.

Somit erhalten wir für $\Delta u(x,y)$ die Näherung
\begin{eqnarray}
\Delta u(x,y) &=& \partial_{xx} u(x,y) + \partial_{yy} u(x,y) \notag \\
&\approx& \frac {u(x-h,y) + u(x+h,y) - 4u(x,y) + u(x,y-h) + u(x,y+h)} {h^{2}}.\label{eq.Differenzenquotienten}
\end{eqnarray}

\subsection{Diskretisierung von $\Omega$}\label{ss.Diskretisierung}

Mit einem zweidimensionalen Gitter, der Gitterweite $h$, wobei $h \in \mathbb{Q}$ mit $h = \frac {1} {m}$ und $m \in \mathbb{N}_{>1}$, wird nun das Gebiet $\Omega$ diskretisiert. Die Zahl $N := (m-1)$ gibt uns an, wie viele Gitterpunkte es jeweils in x- bzw. y-Richtung gibt.\\

Für $i,j = 1,...,N$ kann man dann $u(x,y)$ auch schreiben als:
\begin{equation}
u(x_{i},y_{j}) = u(ih,jh).
\end{equation}

$\Omega$ fassen wir als $\Omega_{h}$ auf, so dass:
\begin{equation}
\Omega_{h} := \{u(ih, jh) | 1 \le i,j \le N\}.
\end{equation}

\bild{diffStar}{8cm}{5-Punkt-Differenzenstern im Gitter}{5-Punkt-Differenzenstern im Gitter}\label{img.5-Point-Star}

Mit \autoref{eq.Differenzenquotienten} ergibt sich nun für $\Delta u(x,y) \approx \Delta_{h} u(x,y)$ die diskretisierte Form:
\begin{equation}
\Delta_{h} u(x,y) = \frac {u(x-h,y) + u(x+h,y) - 4u(x,y) + u(x,y-h) + u(x,y+h)} {h^{2}}.\label{eq.5-Point-Star}
\end{equation}
für alle $(x,y) \in \Omega_{h}$. \\
Man stellt $\Delta_{h} u(x,y)$ häufig auch als 5-Punkt-Differenzenstern (\autoref{img.5-Point-Star}) in der Form
\begin{equation}
\left[-\Delta_{h}\right]_{\xi} = \frac {1} {h^{2}}
\begin{pmatrix}
  & -1 & \\
-1 & 4 & -1 \\
  & -1 & 
\end{pmatrix}
\textnormal{, } \xi \in \Omega_{h}
\end{equation}
dar. (Dahmen Reusken)
% \begin{eqnarray}
% \Delta_{h} u(x,y) &:=& \frac {u(x-h,y) + u(x+h,y) - 4u(x,y) + u(x,y-h) + u(x,y+h)} {h^{2}} \notag \\
% &=& \frac {1} {h^{2}}
% \begin{pmatrix}
% \frac {u(x,y+h)} {u(x,y)}, & 1, & \frac {u(x,y-h)} {u(x,y)}
% \end{pmatrix}
% \begin{pmatrix}
%   & 1 & \\
% 1 & -4 & 1 \\
%   & 1 & 
% \end{pmatrix}
% \begin{pmatrix}
% u(x+h,y) \\
% u(x,y) \\
% u(x-h,y)
% \end{pmatrix}
% \end{eqnarray}
% Diese Approximation wird auch 5-Punkt-Differenzenstern genannt, siehe dazu Abbildung (2.1).

\bild{grid}{8cm}{Nummerierung der Punkte von $\Omega = (0,1)^{2}$ mit $m=5$. In x- bzw. y-Richtung gibt es jeweils $N=4$ Punkte}{Gitter}\label{img.gridWithNumbers}

Nummeriert man nun alle Gitterpunkte des Gitters fortlaufend von links unten nach rechts oben (\autoref{img.gridWithNumbers}) und stellt für jeden dieser Punkte \autoref{eq.5-Point-Star} auf, so führt dies mit dem 5-Punkt-Differenzenstern auf eine $N^{2} \times N^{2}$-Matrix der Form:

\begin{equation}
\mat{A} =
\begin{pmatrix}
A_{1} & -Id \\
-Id & A_{2} & \ddots \\
 & \ddots & \ddots & -Id \\
 & & -Id & A_{n}
\end{pmatrix},\label{eq.2D Matrix}
\end{equation}
wobei $\mat{Id} \in \mathbb{R}^{N \times N}$ die Identität meint und für alle $i = 0,..,N$ gilt:

\begin{equation}
A_{i} = 
\begin{pmatrix}
4 & -1 & & \\
-1 & 4 & \ddots & \\
 & \ddots & \ddots & -1 \\
 & & -1 & 4
\end{pmatrix}.
\end{equation}
$A_{i} \in \mathbb{R}^{N \times N}$.

Unser Ziel ist ein lineares Gleichungssystem der Form $\mat{A}u = f$. Dafür muss zusätzlich die rechte Seite $f$ aufgestellt werden. Da die Funktion $f(x,y)$ bekannt ist, spielen lediglich noch die Randpunkte eine wesentliche Rolle. Zu jeder Komponente von $f$, die einen Randpunkt als Nachbarn hat, wird dieser dazu addiert. Hat eine Komponente von $f$ zwei Nachbarn am Rand von $\Omega_{h}$, werden beide addiert. Dies führt uns auf folgende rechte Seite [Dahmen/Reusken]:

\begin{equation}
f = h^{2}
\begin{pmatrix}
f_{1} \\ f_{2} \\ \vdots \\ f_{N}
\end{pmatrix},
\end{equation}

wobei gilt

\begin{equation}
f_{1} = 
\begin{pmatrix}
f(h,h) + h^{-2}(g(h,0)+g(0,h)) \\
f(2h,h) + h^{-2}(g(2h,0)) \\
\vdots \\
f(1-2h,h) + h^{-2}(g(1-2h,0)) \\
f(1-h,h) + h^{-2}(g(1-h,0)+g(0,1-h))
\end{pmatrix},
\end{equation}

\begin{equation}
f_{j} = 
\begin{pmatrix}
f(h,jh) + h^{-2}(g(0,jh)) \\
f(2h,jh) \\
\vdots \\
f(1-2h,jh) \\
f(1-h,jh) + h^{-2}(g(1,jh))
\end{pmatrix}
2 \le j \le N-1,
\end{equation}

\begin{equation}
f_{N} = 
\begin{pmatrix}
f(h,1-h) + h^{-2}(g(h,1)+g(0,1-h)) \\
f(2h,1-h) + h^{-2}(g(2h,1)) \\
\vdots \\
f(1-2h,1-h) + h^{-2}(g(1-2h,1)) \\
f(1-h,1-h) + h^{-2}(g(1-h,1)+g(1,1-h))
\end{pmatrix}.
\end{equation}

Schließlich erhalten wir das lineare Gleichungssystem der Form $\mat{A}u = f$, wobei $\mat{A}$ und $f$ bekannt sind und $u$ die approximierte Lösung, der partiellen Differentialgleichung darstellt.\\
Wir bezeichnet ab jetzt die diskrete 2D Poisson Matrix aus \autoref{eq.2D Matrix} mit $\mat{A}_{2D}$.

% \section{Das Residuum und die Residuumsgleichung}\label{s.Residuum und Resdiuumsgleichung}

% Ein weiterer Begriff, der im Zusammenhang mit linearen Gleichungssystemen oft fällt, ist der des Residuums. Dieses wird uns in \autoref{s.Das Verfahren der konjugierten Gradienten} und \autoref{c.Mehrgitterverfahren} wieder begegnen.

% \subsection{Definition (Residuum)}\label{ss.Das Residuum}

% Sei $\mat{A}u = f$ ein lineares Gleichungssystem mit $A \in \mathbb{R}^{n \times n}$, $u,f \in \mathbb{R}^{n}$. Dann gilt für $r \in \mathbb{R}^{n}$:
% \begin{equation}
% r := f - \mat{A}u
% \end{equation}
% wobei wir $r$ das Residuum nennen.

% \subsection{Die Residuumsgleichung}\label{ss.Resdiuumsgleichung}

% Seien $\mat{A},u,f$ und $r$ wie in \autoref{ss.Das Residuum}. Dann gilt für $e \in \mathbb{R}^{n}$:

\section{Eigenschaften der Matrix $\mat{A}_{2D}$}\label{s.Eigenwerte und Eigenvektoren}

Offensichtlich gilt $\mat{A}_{2D} = \mat{A}_{2D}^{T}$. Also ist $\mat{A}_{2D} \in \mathbb{R}^{n \times n}$ eine symmetrische Matrix. Somit existiert eine Orthogonalbasis aus Eigenvektoren für die gilt:
\begin{equation}
\mat{A}_{2D}v_{i,j} = \lambda_{i,j} v_{i,j},
\end{equation}
wobei $\lambda_{1,1},...,\lambda_{N,N} \in \mathbb{R}$ und $v_{i,j} \in \mathbb{R}^{n}$.

\subsection{Eigenwerte und Eigenvektoren von $\mat{A}_{2D}$}\label{ss.Eigenwerte und Eigenvektoren}

Für $k \in \mathbb{N}$ und $h = \frac {1} {m}$ wie oben, seien $(x_{k},y_{l}) \in \Omega_{h}$ mit $x_{k} := k \cdot h, y_{l} := l \cdot h$ und $\theta_{k} \in \mathbb{R}$ mit $\theta_{k} := k \pi h$. Dann gilt für die Eigenwerte:
\begin{equation}
\lambda_{k,l} = 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2}) \right) \textnormal{ für } 1 \le k,l \le N,\label{eq.Eigenwerte}
\end{equation}
Für einen Eigenvektor am Punkt $(x_{k},y_{l})$ gilt:
\begin{equation}
v_{k,l} = \sin(i \pi x_{k}) \sin(j \pi y_{l}) = \sin(i\theta_{k}) \sin(j\theta_{l}) \textnormal{ für } 1 \le i,j \le N.
% v_{i,j}(x_{k},y_{l}) = \sin(i \pi x_{k}) \sin(j \pi y_{l}) \textnormal{ für } 1 \le i,j,k,l \le N.
\end{equation}
wobei $N$ jeweils die Anzahl der Gitterpunkte in x- und in y-Richtung definiert.

\textbf{Bemerkung:}

Ein Eigenvektor im Punkt $(x_{k},y_{l})$ lässt sich auch in der folgenden Form darstellen:
\begin{equation}
v_{k.l} = 
\begin{pmatrix}
\sin(\theta_{k})\sin(\theta_{l})\\
\sin(\theta_{k})\sin(2 \theta_{l})\\
\vdots \\
\sin(\theta_{k})\sin(N \theta_{l})\\
\sin(2 \theta_{k})\sin(\theta_{l})\\
\sin(2 \theta_{k})\sin(2 \theta_{l})\\
\vdots\\
\sin(N \theta_{k})\sin((N-1) \theta_{l})\\
\sin(N \theta_{k})\sin(N \theta_{l})
\end{pmatrix}.
\end{equation}

\textbf{Beispiel:}

Um sich die Eigenvektoren besser vorstellen zu können, wollen wir zunächst drei Eigenvektoren des eindimensionalen Poisson Problems, auf das wir hier nicht näher eingehen möchten, betrachten. Das Gebiet $\Omega_{h} \in (0,1)$ mit einer Schrittweite $h = \frac{1}{15}$ und drei zugehörigen Eigenvektoren:

\bild{sin1}{8cm}{Dieser Graph stellt den Eigenwert am ersten Punkt, also $h = \frac{1}{15}$ dar. Zu sehen sind harmonische, kurzwellige Sinuskurven.}{Für Faktor 1}
\bild{sin7}{8cm}{Bei diesen stark oszillierenden Sinuswellen handelt es sich um den Eigenvektor im Punkt $\frac{7}{15}$.}{Für Faktor 7}
\bild{sin13}{8cm}{Diese gleichmäßigen langen Sinuswellen gehören zum Punkt $\frac{13}{15}$.}{Für Faktor 13}\label{img.1D Langwelle}

Man sieht deutlich, das die Oszillation unterschiedlich ist. Manche der Eigenvektoren sind langwellig, manche kurzwellig. Wenn wir nun zurück zu unserer Ausgangssituation des zweidimensionalen Poisson Problems gehen und das Gitter aus \autoref{img.gridWithNumbers} als Basis nehmen, erhalten wir für den Punkt 11 des Gitters folgende Darstellung des Eigenvektors:

\bild{EV}{14cm}{Man kann am Eigenvektor des Gitterpunktes $(\frac {3} {5},\frac {3} {5})$ gut die gleichmäßigen Sinuswellen erkennen.}{Man kann am Eigenvektor des Gitterpunktes $(\frac {3} {5},\frac {3} {5})$ gut die überlagerten Sinuswellen erkennen.}\label{img.Jacobi1}

Auch hier sind die Sinuswellen gut zu erkennen. Man kann dieses Bild in etwa mit \autoref{img.1D Langwelle} vergleichen. Dieser Eigenvektor besitzt eine langwellige Oszillation.

\textbf{Beweis zu \autoref{s.Eigenwerte und Eigenvektoren}:}

Wir wollen zunächst die $\mat{A}_{i} \in \mathbb{R}^{N \times N}$ von $\mat{A} \in \mathbb{R}^{n \times n}$ genauer Betrachten.\\
Behauptung: Für eine Matrix $\mat{B} \in \mathbb{R}^{N \times N}$ mit
\begin{equation}
\mat{B} = 
\begin{pmatrix}
a & b\\
c & a & b\\
  & \ddots & \ddots & \ddots\\
  &		   & c & a & b\\
  &		   &  & c & a
\end{pmatrix}
\end{equation}

gilt für die Eigenwerte $\lambda_{k} := a + 2b\left(\frac{c}{b}\right)^{\frac{1}{2}} \cos(\theta_{k})$ und die Eigenvektoren $v_{k_{i}} := \left(\frac{c}{b}\right)^{\frac{i}{2}} \sin(i \theta_{k})$ für alle $1 \le i \le N$, $\lambda_{k} \in \mathbb{R}, v_{k} \in \mathbb{R}^{N \times N}$.

\textit{Beweis:}

Da $\lambda_{k}, v_{k}$ Eigenwerte bzw. Eigenvektoren von $\mat{B}$ sind gilt folgende Gleichung:
\begin{equation}
(\mat{B} - \lambda_{k} \mat{Id}) v_{k} = 0
\end{equation}

\begin{equation}
\Longleftrightarrow
\begin{pmatrix}
a - \lambda_{k} & b\\
c & a - \lambda_{k} & b\\
  & \ddots & \ddots & \ddots\\
  &		   & c & a - \lambda_{k} & b\\
  &		   &  & c & a - \lambda_{k}
\end{pmatrix}
v_{k} = 0
\end{equation}

\begin{equation}
\Longleftrightarrow
\begin{pmatrix}
(a - \lambda_{k}) v_{k_{1}} + b v_{k_{2}}\\
c v_{k_{1}} + (a - \lambda_{k}) v_{k_{2}} + b v_{k_{3}}\\
\vdots\\
c v_{k_{N-2}} + (a - \lambda_{k}) v_{k_{N-1}} + b v_{k_{N}}\\
c v_{k_{N-1}} + (a - \lambda_{k}) v_{k_{N}}
\end{pmatrix}
= 0
\end{equation}

Wir wollen zunächst die einzelnen Summanden ausrechnen bzw. vereinfachen:

\begin{description}
\item[1.] Löse $(a - \lambda_{k}) v_{k_{i}}$
\begin{eqnarray}
(a - \lambda_{k}) v_{k_{i}} &=& (a - (a + 2b\left(\frac{c}{b}\right)^{\frac{1}{2}} \cos(\theta_{k}))) \left(\frac{c}{b}\right)^{\frac{i}{2}} \sin(i \theta_{k})\notag \\
&=& -2b\left(\frac{c}{b}\right)^{\frac{i+1}{2}} \cos(\theta_{k})\sin(i \theta_{k}) \notag
\end{eqnarray}
\item[2.] Löse $b v_{k_{i+1}}$
\begin{eqnarray}
b v_{k_{i+1}} &=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} \sin((i+1) \theta_{k})\notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) + \cos(i \theta_{k}) \sin(\theta_{k})) \notag
\end{eqnarray}
\item[3.] Löse $c v_{k_{i-1}}$
\begin{eqnarray}
c v_{k_{i-1}} &=& c \left(\frac{c}{b}\right)^{\frac{i-1}{2}} \sin((i-1) \theta_{k}) = \left(c^{2}\frac{c}{b}\right)^{\frac{i-1}{2}} \sin((i-1) \theta_{k})\notag \\
&=& \left(\frac{1}{b^{-2}} \frac{c}{b}\right)^{\frac{i+1}{2}} \sin((i-1) \theta_{k}) = b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} \sin((i-1) \theta_{k}) \notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) - \cos(i \theta_{k}) \sin(\theta_{k})) \notag
\end{eqnarray}
\end{description}

Nun rechnen wir jede Zeile unseres Vektors aus:

\begin{description}
\item[1.] Zeile $1$, also $i = 1$
\begin{eqnarray}
&& -2b\left(\frac{c}{b}\right)^{\frac{1+1}{2}} \cos(\theta_{k})\sin(\theta_{k}) + b \left(\frac{c}{b}\right)^{\frac{1+1}{2}} (\cos(\theta_{k}) \sin(\theta_{k}) + \cos(\theta_{k}) \sin(\theta_{k})) \notag \\
&=& c (-2\cos(\theta_{k})\sin(\theta_{k}) + 2\cos(\theta_{k})\sin(\theta_{k})) = 0. \notag
\end{eqnarray}
\item[2.] Zeile $2,..,N-1$, betrachte für alle $2 \le i \le N-1$
\begin{eqnarray}
&& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) - \cos(i \theta_{k}) \sin(\theta_{k})) - 2b\left(\frac{c}{b}\right)^{\frac{i+1}{2}} \cos(\theta_{k})\sin(i \theta_{k}) \notag \\
&+& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (\cos(\theta_{k}) \sin(i \theta_{k}) - \cos(i \theta_{k}) \sin(\theta_{k})) \notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{i+1}{2}} (-2\cos(\theta_{k})\sin(i\theta_{k}) + 2\cos(\theta_{k})\sin(i\theta_{k}) \notag \\
&+& \cos(i\theta_{k})\sin(\theta_{k}) - \cos(i\theta_{k})\sin(\theta_{k})) = 0. \notag
\end{eqnarray}
\item[3.] Zeile $N$, somit für $i = N$
\begin{eqnarray}
&& b \left(\frac{c}{b}\right)^{\frac{N+1}{2}} (\cos(\theta_{k}) \sin(N \theta_{k}) - \cos(N \theta_{k}) \sin(\theta_{k})) - 2b\left(\frac{c}{b}\right)^{\frac{N+1}{2}} \cos(\theta_{k})\sin(N \theta_{k}) \notag \\
&=& b \left(\frac{c}{b}\right)^{\frac{N+1}{2}}(-2\cos(\theta_{k})\sin(N\theta_{k}) + \cos(\theta_{k})\sin(N\theta_{k}) - \cos(N\theta_{k})\sin(\theta_{k})) \notag \\
&=& -b \left(\frac{c}{b}\right)^{\frac{N+1}{2}}(\cos(\theta_{k})\sin(N\theta_{k}) + \cos(N\theta_{k})\sin(\theta_{k})). \notag \\
&=& -b \left(\frac{c}{b}\right)^{\frac{N+1}{2}} \sin((N+1)\theta_{k}) \overset{h = \frac{1}{N+1}}{=} -b \left(\frac{c}{b}\right)^{\frac{N+1}{2}} \sin(k\pi) = 0. \notag
\end{eqnarray}
\end{description}
\begin{flushright}
$\blacksquare$
\end{flushright}
Für die Matrizen $A_{i}$ erhalten wir mit $a = 4, b = c = -1$ für die Eigenwerte $\lambda_{k} = 4 (1 - \frac{1}{2} \cos(\theta_{k}))$ und die Eigenvektoren $v_{k} = \sin(i\theta_{k})$ für alle $1 \le i \le N$.\\
Offensichtlich hat auch $-\mat{Id}$ die selben Eigenvektoren wie $A_{i}$, mit den Eigenwerte $\mu_{k} = -1$, denn
\begin{equation}
(-\mat{Id} - \lambda_{k}\mat{Id}) v_{k} = \mat{0} v_{k} = 0.
\end{equation}
Nun wollen wir diese Erkenntnisse für unsere Matrix $\mat{A}_{2D} \in \mathbb{R}^{n \times n}$ verwenden. Da diese Matrix ebenfalls eine Tridiagonalmatrix ist, folgt für $a = A_{j}$ und $b = c = -Id$.\\
Unser gesuchter Eigenvektor in einem Punkt $(x_{k},y_{l})$ war gegeben durch $v_{k,l} = \sin(i\theta_{k}) \sin(j\theta_{l}) = \sin(j\theta_{l})v_{k}$.  Wegen $\mat{A}_{2D}$ symmetrisch folgt:
\begin{equation}
\mat{A}_{2D}v_{k,l} = 
\begin{pmatrix}
A_{1} & -Id\\
-Id & A_{2} & -Id\\
    & \ddots & \ddots & \ddots\\
   	&		 & -Id    & A_{n-1} & -Id\\
   	&		 &		  & -Id    & A_{n}
\end{pmatrix}
\begin{pmatrix}
\sin(\theta_{l}) v_{k}\\
\sin(2\theta_{l}) v_{k}\\
\vdots\\
\sin((N-1)\theta_{l}) v_{k}\\
\sin(N\theta_{l}) v_{k}\\
\end{pmatrix}
= \lambda_{k,l}
\begin{pmatrix}
\sin(\theta_{l}) v_{k}\\
\sin(2\theta_{l}) v_{k}\\
\vdots\\
\sin((N-1)\theta_{l}) v_{k}\\
\sin(N\theta_{l}) v_{k}\\
\end{pmatrix}\notag
\end{equation}

Nun wollen wir wie oben $\mat{A}_{2D} v_{k,l}$ explizit ausrechnen und müssen dafür wieder drei Fälle unterscheiden:
\begin{description}
\item[1.] Zeile $1$, also $j = 1$
\begin{eqnarray}
&&\Longrightarrow A_{1} \sin(\theta_{l}) v_{k} - Id \sin(2\theta_{l}) v_{k} = \underbrace{A_{1} v_{k}}_{=\lambda_{k} v_{k}} \sin(\theta_{l}) + (\underbrace{-Id v_{k}}_{\mu_{k} v_{k}}) \underbrace{\sin(2\theta_{l})}_{2\cos({\theta_{l})\sin({\theta_{l}})}} \notag \\
&=& \lambda_{k}v_{k}\sin(\theta_{l}) + 2\mu_{k}v_{k}\cos(\theta_{l})\sin(\theta_{l}) = (\lambda_{k} + 2\mu_{k}\cos(\theta_{l})) \underbrace{v_{k}\sin(\theta_{l})}_{v_{k,l}} \notag \\
&=& (\lambda_{k} + 2\mu_{k}\cos(\theta_{l}))v_{k,l} \overset{\textnormal{ einsetzen von } \lambda_{k},\mu_{k}}{=} (4(1 - \frac{1}{2}\cos{\theta_{k}}) - 2\cos(\theta_{l})) v_{k,l} \notag \\
&=& (4 - 2\cos(\theta_{k}) - 2\cos(\theta_{l})) v_{k,l} = (2 - 2\cos(\theta_{k}) + 2 - 2\cos(\theta_{l})) \notag \\
&=& (2(1 - \frac{1}{2}\cos(\theta_{k})) + 2(1 - \frac{1}{2}\cos(\theta_{l})) = 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) = \lambda_{k,l} v_{k,l} \notag
\end{eqnarray}
\item[2.] Zeile $2,..,N-1$, betrachte für alle $2 \le j \le N-1$
\begin{eqnarray}
&&\Longrightarrow -Id v_{k} \sin((j-1) \theta_{l}) + A_{j} v_{k} \sin(j\theta_{l}) - Id v_{k} \sin((j+1) \theta_{l}) \notag \\
&=& \mu_{k} v_{k} (\sin(j\theta_{l})\cos(\theta_{l}) - \cos(j\theta_{l})\sin(\theta_{l})) + \lambda_{k} v_{k} \sin(j\theta_{l}) \notag \\
&-& \mu_{k} v_{k} (\sin(j\theta_{l})\cos(\theta_{l}) + \cos(j\theta_{l})\sin(\theta_{l})) \notag \\
&=& 2 \mu_{k} v_{k} \sin(j\theta_{l})\cos(\theta_{l}) + \lambda_{k} v_{k} \sin(j\theta_{l}) = (\lambda_{k} + \mu_{k} \cos(\theta_{l})) \sin(j\theta_{l}) v_{k} \notag \\
&=& (\lambda_{k} + \mu_{k} \cos(\theta_{l})) \sin(j\theta_{l}) v_{k,l} \overset{\textnormal{ wie oben }}{=} 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) = \lambda_{k,l} v_{k,l} \notag
\end{eqnarray}
\item[3.] Zeile $N$, somit für $j = N$
\begin{eqnarray}
&&\Longrightarrow -Id v_{k} \sin((N-1)\theta_{l}) + A_{N} v_{k} \sin(N \theta_{l}) \notag \\
&=& \mu_{k} v_{k} (\sin((N-1)\theta_{l}) + \underbrace{\sin((N+1)\theta_{l})}_{= 0}) + \lambda_{k} v_{k} \sin(N\theta_{l}) \notag \\
&=& \mu_{k} v_{k} (\sin(N\theta_{l})\cos(\theta_{l}) - \cos(N\theta_{l})\sin(\theta_{l}) + \sin(N\theta_{l})\cos(\theta_{l}) \notag \\
&+& \cos(N\theta_{l})\sin(\theta_{l})) + \lambda_{k} v_{k} \sin(N \theta_{l}) = 2\mu_{k} v_{k} \sin(N\theta_{l})cos(\theta_{l}) + \lambda_{k} v_{k} \sin(N\theta_{l}) \notag \\
&=& (\lambda_{k} + 2\mu_{k}\cos(\theta_{l})) \sin(N\theta_{l}) v_{k} \overset{\textnormal{ wie oben }}{=} 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) = \lambda_{k,l} v_{k,l} \notag
\end{eqnarray}
\end{description}
Also sind die $\lambda_{k,l}$ Eigenwerte von $\mat{A}_{2D}$ zu den Eigenvektoren $v_{k,l}$.
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Folgerung ($\mat{A}_{2D}$ ist s.p.d.)}

Für für die Eigenwerte gilt $\lambda_{k,l} \in \mathbb{R}_{> 0}$, d.h. $\mat{A}_{2D}$ ist symmetrisch positiv definit.

\textbf{Beweis:}

Mit $\sin^{2}(x) \in (0,1)$ für $x \in (0,\frac{\pi}{2})$ und $0 < \frac{\pi h}{2} < \frac{\pi}{2}$ gilt:
\begin{equation}
\lambda_{k,l} = 4 \left(\sin^{2}(\frac {\theta_{k}} {2}) + \sin^{2}(\frac {\theta_{l}} {2})\right) > 0.
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Definition (Kondition einer symmetrischen Matrix)}\label{ss.Definition Kondition einer Matrix}

Sei $\mat{A}$ eine symmetrische Matrix des $\mathbb{R}^{n \times n}$. Dann gilt für die euklidische Kondition der Matrix:
\begin{equation}
\kappa_{2} (A) := \frac {\lambda_{max}} {\lambda_{min}} \ge 1.\label{eq.Kondition}
\end{equation}
Je kleiner die Konditionszahl $\kappa$, desto besser ist eine Matrix konditioniert.

\subsection{Lemma (Kondition von $\mat{A}_{2D}$)}\label{ss.Matrixkondition}

Für die Kondition der Matrix $\mat{A}_{2D}$ gilt:
\begin{equation}
\kappa_{2} (A_{2D}) = \frac {\cos^{2}(\frac{\pi h}{2})} {\sin^{2}(\frac{\pi h}{2})}.
\end{equation}

\textbf{Beweis:}

Zudem gilt mit $h = \frac{1}{m}$ und da $sin^{2}(x) \in (0,1)$ streng monoton steigend ist für $x \in (0,\frac{\pi}{2})$:
\begin{eqnarray}
\lambda_{min} &=& \lambda_{1,1} = 8\sin^{2}(\frac{\pi h}{2}),\label{eq.Lambda min}\\
\lambda_{max} &=& \lambda_{N,N} = 8\sin^{2}(\frac{N\pi h}{2}) \overset{\substack{h=\frac {1} {m}, \\N=m-1}}{=} 8\sin^{2}(\frac{(m-1)\pi}{2m}) \notag \\
&=& 8\sin^{2}(\frac{\pi}{2} - \frac{\pi}{2m}) = 8\sin^{2}(\frac{\pi}{2} - \frac{\pi h}{2}) \notag \\
&=& 8(\sin(\frac{\pi}{2}\cos(\frac{\pi h}{2})) - \cos(\frac{\pi h}{2})\sin(\frac{\pi}{2}))^{2} = 8\cos^{2}(\frac{\pi h}{2}) \label{eq.Lambda max}
\end{eqnarray}

Somit folgt aus \autoref{eq.Lambda min} und \autoref{eq.Lambda max}:
\begin{equation}
\kappa_{2} (A_{2D}) = \frac {\lambda_{N,N}} {\lambda_{1,1}} = \frac {\cos^{2} (\frac {\pi h} {2})} {\sin^{2} (\frac {\pi h} {2})}. \notag
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}
In [DahmenReusken] wird außerdem gezeigt, dass sich $\kappa_{2}(\mat{A}_{2D})$ wie folgt nähern lässt:
% Substituiert man nun $\cos^{2}(\frac {\pi h} {2}) = 1 - \sin^{2}(\frac {\pi h} {2})$ und nähert $\sin (\frac {\pi h} {2}) = \frac {\pi h} {2} + \mathcal{O}(h^{3})$ mit der Taylorformel, so erhält man:
\begin{equation}
\frac {\cos^{2} (\frac {\pi h} {2})} {\sin^{2} (\frac {\pi h} {2})} = \left( \frac {2} {\pi h} \right)^{2} (1 + \mathcal{O}(h^{2})).
\end{equation}

Natürlich wollen wir die Funktion $u(x,y)$ so gut wie möglich in $\Omega_{h}$ approximieren und sind daher bestrebt das Gitter so fein als möglich zu wählen. Daraus ergibt sich jedoch die negative Eigenschaft von $\mat{A}_{2D}$. \\
Je größer $m$ gewählt wird, also je feiner das Gitter wird, desto schlechter wird die Kondition der Matrix.
\begin{equation}
\left( \frac {2} {\pi h} \right)^{2} (1 + \mathcal{O}(h^{2})) = \left( \frac {2} {\pi \frac {1} {m}} \right)^{2} (1 + \mathcal{O}(\frac {1} {m^{2}})) \approx \frac {4m^{2}} {\pi}.
\end{equation}

Diesem Problem werden wir später nochmals gegenüber stehen.

\chapter{Iterative Lösungsvefahren für lineare Gleichungssysteme}\label{c.IterativeVerfahren}

Gleichungssysteme, die partielle Differentialgleichungen lösen, können sehr groß werden. Aus diesem Grund sind direkte Verfahren, wie z.B. der Gauß-Algorithmus oder die LR-Zerlegung nicht geeignet. Ihr Rechenaufwand beläuft sich im Allgemeinen auf $\mathcal{O}(n^{3})$ und ist zu langsam.\\
Wesentlich besser geeignet für diese Problemstellung sind iterative Verfahren. Sie zeichnen sich durch eine schnelle Konvergenz und einen geringeren Rechenaufwand aus - falls sie konvergieren.\\
Ein Großteil der Definitionen, Sätze und Lemmata in diesem Kapitel sind sinngemäß aus Dahmen/Reusken.

\section{Grundbegriffe}\label{s.Grundbegriffe}

\subsection{Definition (Iterationsmatrix)}\label{ss.Iterationsmatrix}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$. Sei außerdem $\mat{C} \in \mathbb{R}^{n \times n}$ eine nichtsinguläre Matrix. Für die iterative Lösung eines linearen Gleichungssystems der Form $Au = f$ ist die Iterationsmatrix $\mat{T} \in \mathbb{R}^{n \times n}$ definiert als:
\begin{equation}
\mat{T} := (\mat{Id} - \mat{C} \mat{A}),
\end{equation}
wobei die Iterationsvorschrift für $k=1,...,n$ gegeben ist durch:
\begin{equation}
u^{k+1} := (\mat{Id} - \mat{C} \mat{A})u^{k} + \mat{C} f.\label{eq.Iteratives Verfahren}
\end{equation}

\subsection{Definition (Spektralradius)}\label{s.Spektralradius}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$. Und seien für alle $i=1,...,n$, $\lambda_{i} \in \mathbb{R}$. Dann gilt:
\begin{equation}
\rho(A) := \max_{1 \le i \le n} | \lambda_{i} |.
\end{equation}
Ist $\mat{A}$ symmetrisch so gilt auch $\rho(A) = \| A \|_{2}$ und $\lambda_{i} \in \mathbb{R}_{>0}$ für alle $i=1,...,n$.

\subsection{Satz (Konvergenz iterativer Verfahren)}\label{ss.Konvergenz iterativer Verfahren}

Ein iteratives Verfahren mit beliebigen Startvektor $x^{0} \in \mathbb{R}^{n}$ konvergiert genau dann gegen die exakte Lösung $x^{*} \in \mathbb{R}^{n}$, wenn gilt:
\begin{equation}
\rho(\mat{T}) = \rho(Id - CA) < 1.
\end{equation}

Einen Beweis hierzu findet man z.B. in (Dahmen/Reusken und Verweis).

\subsection{Definition (Residuum und Fehler)}\label{ss.Residuum und Fehler}

Sei $u^{*} \in \mathbb{R}^{n}$ die exakte Lösung des linearen Gleichungssystems $Au = f$. Sei außerdem $u^{k} \in \mathbb{R}^{n}$ die Approximation der Lösung im $k-ten$ Iterationsschritt. Dann gilt für das Residuum:
\begin{equation}
r^{k} := f - \mat{A} u^{k}.\label{eq.Residuum}
\end{equation}
Der Fehler, also die Diskrepanz zwischen exakter und approximierter Lösung, ist definiert als:
\begin{equation}
e^{k} := u^{*} - u^{k}.\label{eq.Fehler}
\end{equation}
Durch Multiplikation mit der Matrix $\mat{A}$ ergibt sich:
\begin{equation}
e = u^{*} - u \Leftrightarrow \mat{A}e = \mat{A}(u^{*} - u) \Leftrightarrow \mat{A}e = \mat{A} u^{*} - \mat{A} u \Leftrightarrow \mat{A}e = b - \mat{A} u \Leftrightarrow \mat{A}e = r.\label{eq.Residuumsgleichung}
\end{equation}
$\mat{A}e = r$ nennen wir Residuumsgleichung.

\section{Das Jacobi-Verfahren (Gesamtschrittverfahren)}\label{s.Das Jacobi-Iterationsverfahren}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$ und $f,u \in \mathbb{R}^{n}$, wobei $u$ die Lösung des linearen Gleichungssystems $\mat{A}u = f$ ist. Dann lässt sich $\mat{A}$ wie folgt zerlegen:

\begin{equation}
A = D - L - U.
\end{equation}

Dabei sind $\mat{D},\mat{L},\mat{U} \in \mathbb{R}^{n \times n}$, wobei $\mat{D} = diag(a_{1,1},...,a_{n,n})$ und $\mat{L}$ eine strikte untere und $\mat{U}$ eine strikte obere Dreiecksmatrix ist. \\
Somit ergibt sich für $\mat{A}u = f$:

\begin{equation}
Au = f \Leftrightarrow (D-L-U)u = f \Leftrightarrow Du = (L+U)u + f.
\end{equation}

Ist nun $\mat{D}$ nicht singulär, so gilt für das Jacobi-Verfahren folgende Iterationsvorschrift:
\begin{equation}
Du^{k+1} = (L+U)u^{k} + f \Leftrightarrow u^{k+1} = D^{-1}(L+U)u^{k} + D^{-1}f.
\end{equation}

\subsection{Algorithmus (Jacobi-Verfahren)}\label{ss.Allgemeines Jacobi-Verfahren}
In Komponentenschreibweise mit einem Startvektor $u^{0} \in \mathbb{R}^{n}$ \textit{beliebig} und $k=1,2,...$. Berechne für $i=1,...,n$:

\begin{equation}
u^{k+1}_{i} = \frac {1} {a_{ii}} (f_{i} - \sum_{\substack{j = 1 \\ j \ne i}}^{n} a_{ij}u^{k}_{i}).
\end{equation}

In jedem Schritt zur Berechnung von $u^{k+1}$ muss hier die Information seines Vorgängers $u^{k}$ bekannt sein. Der Rechenaufwand pro Iterationsschritt beträgt $\mathcal{O}(n^{2})$ und entspricht somit einer Matrix-Vektor-Multiplikation.

\subsection{Satz (Iterationsmatrix des Jacobi-Verfahrens)}\label{ss.Iterationsmatrix Jacobi}

Für die Iterationsmatrix des Jacobi-Verfahrens gilt:
\begin{equation}
\mat{T}_{J} := (\mat{Id} - \mat{D}^{-1} \mat{A})
\end{equation}
Hier ist also $\mat{C} = \mat{D}^{-1}$

\textbf{Beweis}:

Mit der Iterationsvorschrift folgt:
\begin{eqnarray}
u^{k+1} &=& D^{-1}(L+U)u^{k} + D^{-1}f \overset{(L+U)=(D-A)}{=} D^{-1}(D-A)u^{k}+D^{-1}f \notag \\
&=& (Id-D^{-1}A)u^{k} + D^{-1}f. \notag
\end{eqnarray}
Also $\mat{T}_{J} = (\mat{Id} - \mat{D}^{-1} \mat{A})$.
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Satz (Eigenwerte der Jacobi-Iterationsmatrix bzgl. $\mat{A}_{2D}$)}\label{ss.EW Jacobi}

Man sieht leicht ein, dass die Eigenvektoren von $\mat{T}_{J}$ gleich denen von $\mat{A}_{2D}$ sind. Dann gilt für die Eigenwerte der Iterationsmatrix:
\begin{equation}
\lambda_{i,j}(\mat{T}_{J}) = 1 - \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right). \label{eq.Jacobi-Eigenwerte}
\end{equation}
für $1 \le i,j \le N$ und $\theta_{i}$, $\theta_{j}$ wie in \autoref{ss.Eigenwerte und Eigenvektoren}.

\textbf{Beweis:}\label{b.EW Jacobi}

Dieser folgt direkt mit dem Beweis aus \autoref{ss.EW Relax} für $\omega = 1$.
\begin{flushright}
$\blacksquare$
\end{flushright}

% Für $\mat{D} = 4 \mat{Id}$ folgt $\mat{D}^{-1} = \frac {1} {4} \mat{Id}$. Für die Iterationsmatrix $\mat{T}_{J}$ angewandt auf einen Vektor $u$ gilt:
% \begin{eqnarray}
% Tu = (Id - D^{-1}A)u = Id u - D^{-1} \underbrace{Au}_{=\lambda(A) u} = Id u - \frac {1} {4} Id u A = u (1 - \frac {1} {4} \lambda(A)). \notag
% \end{eqnarray}
% Die Eigenwerte von $\mat{T}$ lassen sich also einfach durch $(1 - \frac {h^{2}} {4} \lambda(A))$ berechen:
% \begin{equation}
% \lambda_{i,j}(\mat{T}_{J}) = 1 - \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right) \textnormal{ für } 1 \le i,j \le N. \notag
% \end{equation}
% \begin{flushright}
% $\blacksquare$
% \end{flushright}

\subsection{Lemma (Spektralradius der Jacobi-Iterationsmatrix bzgl. $\mat{A}_{2D}$)}\label{ss.Spektralradius Jacobi}

Das Jacobi-Verfahren konvergiert für die diskrete Poisson Gleichung und es gilt für den Spektralradius:
\begin{equation}
\rho(\mat{T}_{J}) = \cos(\pi h) < 1.
\end{equation}

\textbf{Beweis:}\label{b.Spektral Jacobi}

Folgt mit Beweis aus \autoref{ss.Spektralradius Jacobi Relax} und $\omega = 1$.
\begin{flushright}
$\blacksquare$
\end{flushright}

% Mit \autoref{eq.Jacobi-Eigenwerte} folgt:
% \begin{eqnarray}
% \rho(Id-D^{-1}A_{2D}) &=& \max_{1 \le i \le n} | \lambda_{i} | = \max_{1 \le i \le n} | 1 - \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{}} {2} \right) | \notag \\
% &=& 1 - 2\sin^{2} (\frac {\pi h} {2}) = 1 - 2(\frac{1} {2} (1 - \cos (\pi h)) \notag \\
% &=& \cos (\pi h) \overset{\textnormal{Taylorformel}}{\approx} 1 - \frac {1} {2} \pi^{2} h^{2}. \notag
% \end{eqnarray}
% \begin{flushright}
% $\blacksquare$
% \end{flushright}

Das Jacobi-Verfahren konvergiert also für $\mat{A}_{2D}$. Allerdings nimmt mit feinerem Gitter, also kleinere Schrittweite $h$, die Konvergenzgeschwindigkeit stark ab, da der Spektralradius nahe bei $1$ liegt. Sie dazu auch \autoref{ss.Spektralradius Jacobi Relax}\\
Abschließend wollen wir den Rechenaufwand verbessern. Dafür nutzen wir die Dünnbesetztheit von $\mat{A}_{2D}$ aus. Es sind pro Zeile maximal $5$ Einträge ungleich Null. Oder anders formuliert, hat jeder Gitterpunkt in $\Omega_{h}$ höchstens 4 Nachbarn. Nutzt man diese Struktur aus, so erhält man den folgenden Algorithmus. 
% Da die Matrix, die durch das diskretisierte Poisson-Problem aufgestellt wird, dünn besetzt ist, können wir den Rechenaufwand für dieses spezielle Problem auf $\mathcal{O}(n)$ pro Iterationsschritt verbessern. Dafür benötigen wir nochmals den 5-Punkt-Differenzenstern und die partielle Differentialgleichung:

% \begin{equation}
% \frac {u(x_{i-1},y_{j}) + u(x_{i+1},y_{j}) - 4u(x_{i},y_{j}) + u(x_{i},y_{j-1}) + u(x_{i},y_{j+1})} {h^{2}} = f(x,y)
% \end{equation}

% Löst man diese Gleichung nun für alle $i,j$ nach $u(x_{i},y_{j})$ auf, so erhält man folgende Iterationsvorschrift für das Jacobi-Verfahren: \\

\subsection{Algorithmus (Jacobi-Verfahren für $\mat{A}_{2D}$)}\label{ss.Jacobi für Poisson}

Berechne für $k = 1,2,...$ mit Startvektor $u^{0} \in \mathbb{R}^{n}$ \textit{beliebig}\\
Für $i = 1,...,N$ und für $j = 1,...,N$:
\begin{equation}
u^{k+1}_{i,j} =  \frac {1} {a_{i,i}} (f_{i,j} - u^{k}_{i-1,j} + u^{k}_{i+1,j} + u^{k}_{i,j-1} + u^{k}_{i,j+1}).
\end{equation}
(Werte, bei denen eine Null im Index steht, werden ignoriert!)\\
Der Rechenaufwand pro Iterationsschritt beträgt lediglich $\mathcal{O}(N \cdot N)=\mathcal{O}(n)$ Schritte.

% \subsection{Das Jacobi-Relaxations-Verfahren für $\mat{A}_{2D}$}\label{ss.Jacobi-Verfahren der Poisson Gleichung}
\section{Das Jacobi-Relaxationsverfahren}\label{s.Jacobi Relaxation}

Wir wollen nochmal das Jacobi-Verfahren betrachten:

\begin{equation}
u^{k+1} = (Id - D^{-1}A)u^{k} + D^{-1}f.
\end{equation}

Zunächst soll eine Umformung erfolgen:
\begin{eqnarray}
u^{k+1} &=& (Id - D^{-1}A)u^{k} + D^{-1}f \notag \\
&=& u^{k} - D^{-1}Au^{k} + D^{-1}f \notag \\
&=& u^{k} + D^{-1} \underbrace{(f - Au^{k})}_{= r^{k}}.
\end{eqnarray}

Wir addieren also zu $u^{k}$ das Residuum. Die Idee ist nun dieses mit einem Parameter $\omega \in \mathbb{R}$ zu multiplizieren:
\begin{eqnarray}
u^{k+1} &=& u^{k} + D^{-1} \omega r^{k} = u^{k} + \omega D^{-1} (f - Au^{k}) \notag \\
&=& u^{k} - \omega D^{-1} Au^{k} + \omega D^{-1}f = (Id - \omega D^{-1} A)u^{k} + \omega D^{-1}f \notag \\
&=& (Id - \omega Id + \omega Id - \omega D^{-1} A)u^{k} + \omega D^{-1}f \notag \\
&=& (1 - \omega)Id + \omega(Id - D^{-1}A) u^{k} + \omega D^{-1}f.\label{eq.Jacobi-Relaxation}
\end{eqnarray}
\autoref{eq.Jacobi-Relaxation} stellt die Iterationsvorschrift für das Jacobi-Relaxationsverfahren. Die Iterationsmatrix ist offensichtlich gegeben durch:
\begin{equation}
\mat{T}_{J_{\omega}} := (1 - \omega) \mat{Id} + \omega(\mat{Id} - \mat{D}^{-1} \mat{A}) = (\mat{Id} - \omega \mat{D}^{-1} \mat{A}).
\end{equation}

Letztlich erweitert man den Jacobi-Algorithmus also mit dem Parameter $\omega$ und addiert $(1 - \omega)u^{k}$ zu $u^{k+1}$.

\subsection{Algorithmus (Jacobi-Relaxations-Verfahren)}\label{ss.Algorithmus Jacobi Relax}

Sei $u^{0} \in \mathbb{R}^{n}$ ein \textit{beliebiger} Startvektor und $k=1,2,...$ berechne für $i=1,...,n$:

\begin{equation}
u^{k+1}_{i} = (1 - \omega)u^{k}_{i} + \frac {\omega} {a_{ii}} (f_{i} - \sum_{\substack{j = 1 \\ j \ne i}}^{n} a_{ij}u^{k}_{i}).
\end{equation}

Beachte: Für $\omega = 1$ erhalten wir das Jacobi-Verfahren. Auch hier beträgt der Rechenaufwand $\mathcal{O}(n^{2})$.

\subsection{Satz (Eigenwerte des Jacobi-Relaxationsverfahren bzgl. $\mat{A}_{2D}$)}\label{ss.EW Relax}

Die Eigenvektoren entsprechen denen von $\mat{A}_{2D}$ und für die Eigenwerte von $\mat{T}_{J_{\omega}}$ gilt:
\begin{equation}
\lambda_{i,j}(\mat{T}_{J}) = 1 - \omega \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right) \textnormal{ für } 1 \le i,j \le N.\label{eq.Jacobi-Eigenwerte}
\end{equation}
$\theta_{i}$, $\theta_{j}$ wie in \autoref{ss.Eigenwerte und Eigenvektoren}.

\textbf{Beweis:}\label{sss.EW JacobiRelax}

Für $\mat{D} = 4 \mat{Id}$ folgt $\mat{D}^{-1} = \frac {1} {4} \mat{Id}$. Für die Iterationsmatrix $\mat{T}_{J_{\omega}}$ angewandt auf einen Vektor $u$ gilt:
\begin{eqnarray}
\mat{T}_{J_{\omega}}u = (\mat{I}d - \omega \mat{D}^{-1}\mat{A})u = \mat{Id} u - \omega \mat{D}^{-1} \underbrace{\mat{A}u}_{=\lambda_{i,j}(A) u} = \mat{Id} u - \frac {\omega} {4} \mat{Id} \mat{A} u = u (1 - \frac {\omega} {4} \lambda_{i,j}(\mat{A})). \notag
\end{eqnarray}
Die Eigenwerte von $\mat{T}_{J_{\omega}}$ lassen sich also einfach durch $(1 - \frac {\omega} {4} \lambda_{i,j}(A))$ berechnen:
\begin{equation}
\lambda_{i,j}(\mat{T}_{J}) = 1 - \omega \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2}) \right) \textnormal{ für } 1 \le i,j \le N. \notag
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

\subsection{Lemma (Spektralradius der Jacobi-Relaxations-Matrix bzgl. $\mat{A}_{2D}$)}\label{ss.Spektralradius Jacobi Relax}

Das Jacobi-Relaxations-Verfahren konvergiert für die diskrete Poisson Gleichung und es gilt für den Spektralradius:
\begin{equation}
\rho(\mat{T}_{J_{\omega}}) = 1 - \omega(1 - \cos (\pi h)) < 1.
\end{equation}

\textbf{Beweis:}\label{b.Spektral JacobiRelax}

Mit \autoref{eq.Jacobi-Eigenwerte} folgt:
\begin{eqnarray}
\rho(\mat{Id}-\omega\mat{D}^{-1}\mat{A}_{2D}) &=& \max_{1 \le i,j \le N} | \lambda_{i,j} | = \max_{1 \le i,j \le N} | 1 - \omega \left(\sin^{2}(\frac {\theta_{i}} {2}) + \sin^{2}(\frac {\theta_{j}} {2} \right) | \notag \\
&=& 1 - 2 \omega \sin^{2} (\frac {\theta_{1}} {2}) = 1 - 2 \omega \sin^{2} (\frac {\pi h} {2}) = 1 - 2 \omega (\frac{1} {2} (1 - \cos (\pi h)) \notag \\
&=& 1 - \omega(1 - \cos (\pi h)) \overset{\textnormal{Taylorformel}}{\approx} 1 - \omega(1-(1-\frac{\pi^{2} h^{2}}{2})) \notag \\
&=& 1 - \frac {\omega} {2} \pi^{2} h^{2} < 1. \notag
\end{eqnarray}
\begin{flushright}
$\blacksquare$
\end{flushright}

Die Iterationsmatrix des Jacobi-Relaxationsverfahrens bezüglich für $\mat{A}_{2D}$ und für ein $\omega \in (0,1)$ eine bessere Kondition, als die des Standard-Jacobi-Verfahrens. Häufig verwendete Werte im zweidimensionalen Fall sind $\omega = \frac {1} {2}$ und $\omega = \frac {4} {5}$, wobei letzterer Wert für das Mehrgitterverfahren den optimalen Parameter darstellt [Saad]. Leider ist für kleines $h$ die Konvergenz immer noch sehr langsam. Den Rechenaufwand kann man allerdings auch hier verbesseren. Mit der selben Vorgehensweise wie in \autoref{ss.Jacobi für Poisson} erhalten wir:

\subsection{Algorithmus (Jacobi-Relaxations-Verfahren für $\mat{A}_{2D}$)}\label{ss.Algorithmus Jacobi Relax Poisson}

Berechne für $k = 1,2,...$ mit Startvektor $u^{0} \in \mathbb{R}^{n}$ \textit{beliebig}\\
Für $i = 1,...,N$ und für $j = 1,...,N$:
\begin{equation}
u^{k+1}_{i,j} = (1 - \omega)u^{k} + \frac {\omega} {a_{i,i}} (f_{i,j} - u^{k}_{i-1,j} + u^{k}_{i+1,j} + u^{k}_{i,j-1} + u^{k}_{i,j+1}),
\end{equation}

mit Rechenaufwand $\mathcal{O}(n)$.

% \section{Das Gauß-Seidel-Verfahren (Einzelschrittverfahren)}\label{s.Das Gauss-Seidel-Verfahren}

% Mit einer ähnlichen Vorgehensweise wie in \autoref{s.Das Jacobi-Iterationsverfahren}, wollen wir uns nun ein weiteres Verfahren herleiten.

% \subsection{Das allgemeine Gauss-Seidel-Iterationsverfahren}\label{ss.Allgemeines Gauss-Seidel-Verfahren}

% Wir zerlegen die Matrix $\mat{A}$ wie folgt:
% \begin{equation}
% A = D - L - U
% \end{equation}

% Somit ergibt sich für $\mat{A}u = f$:
% \begin{equation}
% Au = f \Leftrightarrow (D-L-U)u = f \Leftrightarrow (D-L)u = Uu + f
% \end{equation}

% Daraus können wir nun folgende Iterationsvorschrift ableiten:
% \begin{equation}
% (D-L)u^{k+1} = Uu^{k} + f \Leftrightarrow u^{k+1} = (D-L)^{-1}Uu^{k} + (D-L)^{-1}f
% \end{equation}

% In Komponentenschreibweise für $i=1,...,n$:
% \begin{equation}
% \sum\limits_{j=1}^{i} a_{ij}u_{j}^{k+1} = -\sum\limits_{j=i+1}^{n} a_{ij}u_{j}^{k} + f_{i}\label{eq.GaussSeidel}
% \end{equation}

% Formt man \autoref{eq.GaussSeidel} um, so erhält man den Algorithmus des Gauss-Seidel-Verfahrens mit Startvektor $u^{0} \in \mathbb{R}$ \textit{beliebig}:

% Für $k = 1,2,...$ berechne für $i = 1,...,n$
% \begin{eqnarray}
% u_{i}^{k+1} &=& \frac {1} {a_{ii}} (f_{i} - \sum\limits_{j=1}^{i-1} a_{ij}u_{j}^{k+1} - \sum\limits_{j=i+1}^{n} a_{ij}u_{j}^{k})
% \end{eqnarray}

% Dieses Verfahren nutzt sofort die Werte, die im laufenden Iterationsschritt berechnet wurden. Somit spart man sich das Alloziieren eines temporären Vektors, also Speicherplatz. \\
% Das Gauss-Seidel-Verfahren wartet ebenfalls mit einem Rechaufwand von $\mathcal{O}(n^{2})$ auf. Wie beim Jacobi-Verfahren kann man dies auf $\mathcal{O}(n)$ optimieren.

% \subsection{Das Gauss-Seidel-Iterationsverfahren für die Poisson-Gleichung}\label{ss.Gauss-Seidel-Verfahren der Poisson Gleichung}

% Ebenso wie in \autoref{ss.Jacobi-Verfahren der Poisson Gleichung} entwickeln wir den Algorithmus, beachten aber dabei, dass bereits berechnete Werte im Iterationsschritt wieder genutzt werden:

% \begin{eqnarray}
% u^{k}_{i,j} &=&  \frac {1} {4} (u^{k}_{i-1,j} + u^{k}_{i,j-1} + u^{k-1}_{i+1,j} + u^{k-1}_{i,j+1} - h^{2}f(x_{i},y_{j})) \\
% &=& \frac {1} {4} (f_{i,j} - u^{k}_{i-1,j} + u^{k}_{i,j-1} + u^{k-1}_{i+1,j} + u^{k-1}_{i,j+1})
% \end{eqnarray}
% Auch hier reduziert sich der Rechenaufwand auf $\mathcal{O}(n)$.


% \section{Warum Einzelschritt- und Gesamtschrittverfahren?}\label{s.Warum Einzelschritt- und Gesamtschrittverfahren?}
% Zum lösen von großen linearen Gleichungssystemen werden das Jacobi-Verfahren und das Gauss-Seidel-Verfahren heute kaum mehr verwendet. Es gibt mittlerweile schnellere, stabilere und effizientere Verfahren wie wir im fortlaufenden sehen werden. Einen großen Vorteil haben allerdings beide Verfahren: Sie löschen große Fehler bereits nach den ersten Iterationsschritten aus. Darum finden sie besonders große Verwendung in den Mehrgittermethoden, die wir später kennen lernen werden.

% \section{Die SOR-Methode (SOR = successive overrelaxation)}\label{s.SOR-Methode}

% Eine Optimierung von \autoref{eq.GaussSeidel} bietet das SOR-Verfahren oder auch Gauss-Seidel-Relaxationsverfahren. Die Einführung eines Relaxationsparameters $\omega$ kann in einigen Fällen das Iterationsverfahren effizienter machen. Speziell für die Poisson-Gleichung wird nicht nur die Effizienz signifikant verbessert, es ist auch ein optimaler Parameter $\omega$ bekannt (siehe hierzu Dahmen/Reusken).

% \subsection{Algorithmus des SOR-Verfahrens}\label{ss.Algorithmus des SOR-Verfahrens}

% Für $k = 1,2,...$ berechne für $i = 1,...,n$
% \begin{eqnarray}
% u_{i}^{k+1} &=& u_{i}^{k} - \frac {\omega} {a_{ii}} (f_{i} - \sum\limits_{j=1}^{i-1} a_{ij}u_{j}^{k+1} - \sum\limits_{j=i}^{n} a_{ij}u_{j}^{k}) \\
% &=& (1-\omega)u_{i}^{k} - \frac {\omega} {a_{ii}} (f_{i} - \sum\limits_{j=1}^{i-1} a_{ij}u_{j}^{k+1} - \sum\limits_{j=i+1}^{n} a_{ij}u_{j}^{k})
% \end{eqnarray}

% Offensichtlich erhält man für $\omega = 1$ wieder das Gauss-Seidel-Verfahren. Außerdem existiert, wie bereits erwähnt, nicht immer ein $\omega$, welches das Verfahren schneller konvergieren lässt. Da für die Poisson Gleichung sogar ein optimaler Parameter existiert, wollen wir uns diesen näher ansehen.

% \subsection{Satz (Optimaler Parameter $\omega$)}\label{ss.optimales omega}

% Sei $\mu := cos(\pi h)$. Für die diskretisierte Poisson Gleichung $Au = f$ gilt dann für den Relaxationsparameter:

% \begin{equation}
% \omega_{opt} := 1 + \left( \frac {\mu} {1 + \sqrt{1 - \mu^{2}}} \right)
% \end{equation}

% Verlgeichen Sie hierzu auch Dahmen/Reusken (Seite 564).

% \subsection{SOR in Matrix-Darstellung}\label{ss.Matrixdarstellung}

% Der Vollständigkeit halber, wollen wir uns jetzt noch der Matrix-Darstellung der SOR-Methode widmen. Im wesentlichen entspricht sie dem Gauss-Seidel-Verfahren. Lediglich der Relaxationsparameter muss eingebaut werden. Man erhält:

% \begin{eqnarray}
% (\mat{Id}+\omega\mat{D}^{-1}\mat{L})x^{k+1} &=& [(1-\omega)\mat{I}-\omega\mat{D}^{-1}\mat{R}]x^{k} + \omega\mat{D}^{-1}b \notag \\
% \Longleftrightarrow \mat{D}^{-1}(\mat{D}+\omega\mat{L})x^{k+1} &=& \mat{D}^{-1}[(1-\omega)\mat{D}-\omega\mat{R}]x^{k} + \omega\mat{D}^{-1}b \notag \\
% \end{eqnarray}

% dies ergibt

% \begin{equation}
% x^{k+1} = (\mat{D}+\omega\mat{L})^{-1}[(1-\omega)\mat{D}-\omega\mat{R}]x^{k}+\omega(\mat{D}+\omega\mat{L})^{-1}b
% \end{equation}

% Mit dieser Gleichung und einigen weiteren Überlegungen kann gezeigt werden, dass folgender Satz gilt.

% \subsection{Satz}\label{ss.konvergenz für omega}

% Sei $\mat{A}$ hermitesch und positiv definit, dann konvergiert das Gauß-Seidel-Relaxationsverfahren genau dann, wenn $\omega \in (0,2)$ ist. (aus oranges buch) \\ \\

% Einen Beweis hierzu findet man beispielsweise in (oranges buch) \\ \\

% Aus \autoref{ss.optimales omega} ist der optimale Paramater für die Poisson Gleichung bereits bekannt. Also konvergiert - wie erwartet - das Verfahren für $\omega = $.

% \subsection{Satz (Iterationsmatrix des Gauss-Seidel-Verfahrens)}

% Für die Iterationsmatrix des Jacobi-Verfahrens gilt:
% \begin{equation}
% \mat{T}_{GS} := (\mat{Id} - (\mat{D} - \mat{L})^{-1} \mat{A})
% \end{equation}
% Hier ist also $\mat{C} = (\mat{D} - \mat{L})^{-1}$

% \textbf{Beweis}:

% Mit der Iterationsvorschrift folgt:
% \begin{eqnarray}
% u^{k+1} &=& (D-L)^{-1}Uu^{k} + (D-L)^{-1}f \\
% &\overset{\mat{U}=(\mat{D}-\mat{L}-\mat{A})}{=}& (D-L)^{-1}(D-L-A)u^{k}+(D-L)^{-1}f \\
% &=& (D-L)^{-1}(D-L)-(D-L)^{-1}Au^{k}+(D-L)^{-1}f \\
% &=& (Id - (D-L)^{-1}A)u^{k} + (D-L)^{-1}f
% \end{eqnarray}
% $\Longrightarrow$ Beh.

% \subsection{Lemma (Spektralradius der Gauss-Seidel-Iterationsmatrix bzgl. $\mat{A}_{2D}$)}

% Das Gauss-Seidel-Verfahren konvergiert für die diskrete Poisson Gleichung und man kann zeigen, dass gilt:
% \begin{equation}
% \rho(\mat{T}_{GS}) = (\rho(\mat{T}_{J}))^{2} = \cos^{2}(\pi h) < 1
% \end{equation}

% \textbf{Beweis:}

% Einen ausführlichen Beweis findet man z.B. in (Dahmen/Reusken). Das $\cos^{2}(\pi h) < 1$ ist, folgt aus der Taylorentwicklung von $\cos^{2}$, für die gilt:
% \begin{equation}
% \cos^{2}(\pi h) \approx 1 - \pi^{2} h^{2} < 1
% \end{equation}

\section{Glättungseigenschaft}\label{s.Glättungseigenschaft}

Für die Iterationsmatrix des Jacobi-Relaxationsverfahrens gilt:
\begin{equation}
u^{k+1} = (Id - \omega D^{-1}A)u^{k} + \omega D^{-1}f.
\end{equation}
Die Eigenvektoren sind gegeben durch:
\begin{equation}
v_{k,l} = sin(i \theta_{k})sin(j \theta_{l}) \textnormal{ für } 1 \le i,j,k,l \le N
\end{equation}
für $\theta_{k}, \theta_{l}$ wie oben.\\
Als Eigenvektoren der Matrizen $\mat{T}_{J}$ bzw. $\mat{T}_{J_{\omega}}$ bilden diese Vektoren eine Basis des $\mathbb{R}^{n}$, wobei $n = N^{2}$.
Für den Fehlerterm im $k-ten$ Iterationsschritt gilt:
\begin{equation}
e^{k} = u^{*} - u^{k}.
\end{equation}

Betrachten wir nun den $(k+1)-ten$ Fehler und formen geschickt um, so gilt:
\begin{eqnarray}
e^{k+1} &=& u^{*} - u^{k+1} = u^{*} - \left( (Id - \omega D^{-1} A)u^{k} + \omega D^{-1}f \right) \notag \\
&=& \underbrace{u^{*} - u^{k}}_{= e^{k}} + \omega D^{-1} A u^{k} - D^{-1}f = e^{k} + \omega D^{-1} (Au^{k} - f) \notag \\
&=& e^{k} + \omega D^{-1} (Au^{k} - Au^{*}) = e^{k} + \omega D^{-1} A(u^{k} - u^{*}) \notag \\
&=& e^{k} + \omega D^{-1} A e^{k} = (Id - \omega D^{-1} A) e^{k}.
\end{eqnarray}

\bild{omegaeins}{14cm}{Dies ist das Eigenwertspektrum für $\omega = 1$. Das Gesamtschrittverfahren hat keine Glättungseigenschaft. Es liegen nur wenige der Eigenwerte um die Null.}{Dies ist das Eigenwertspektrum für $\omega = 1$. Das Gesamtschrittverfahren hat keine Glättungseigenschaft. Es liegen nur wenige der Eigenwerte um die Null.}\label{img.Jacobi1}

Da die $n$ Eigenvektoren $v_{i,j}$ eine Basis bilden, lässt sich der Fehler $e$ als Linearkombination der $v_{i,j}$ darstellen:
\begin{eqnarray}
e^{k+1} &=& \sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k+1}_{i,j} v_{i,j} = (Id - \omega D^{-1}A) \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} v_{i,j}\right) \notag \\
&=& \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} (Id - \omega D^{-1}A) v_{i,j}\right) = \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} (v_{i,j} - \frac {\omega} {4} \underbrace{A v_{i,j}}_{= \lambda_{i,j}(A)v_{i,j}})\right) \notag \\
&=& \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} \underbrace{(1 - \frac {\omega} {4} \lambda_{i,j}(A))}_{= \lambda_{i,j}(\mat{T}_{J_{\omega}})} v_{i,j}\right) = \left(\sum_{i = 1}^{N} \sum_{j = 1}^{N} \xi^{k}_{i,j} \lambda_{i,j}(\mat{T}_{J_{\omega}}) v_{i,j}\right).
\end{eqnarray}

\bild{omegaeinhalb}{14cm}{Für $\omega = \frac{1}{2}$ ist gut zu erkennen, dass für $i,j$ nahe $N$ viele Eigenwerte nahe Null liegen.}{Für $\omega = \frac{1}{2}$ ist gut zu erkennen, dass für $i,j$ nahe $N$ viele Eigenwerte nahe Null liegen.}\label{img.Jacobi2}

Betrachten wir nun die Eigenwerte der Iterationsmatrix für $\omega = 1$ (Jacobi-Verfahren), sieht man, dass die Eigenwerte für $i,j$ nahe Null oder $i,j$ nahe N den Fehler schlecht bis gar nicht dämpfen (\subfigureautorefname{ 3.1}). Interessanter weiße ist der optimale Parameter $\omega = 1$, wenn man das Ganze als iteratives Verfahren verwendet [Saad]. Das Jacobi-Verfahren besitzt aber offensichtlich keine Glättungseigenschaft.\\
Da wir eine Fehlerglättung erreichen wollen, wählen wir nun $\omega = \frac {1} {2}$ (\subfigureautorefname{ 3.2}). Wir stellen fest, dass die Eigenwerte der Iterationsmatrix zwischen $0$ und $1$ liegen. Sind $i,j > \frac {N} {2}$ so werden die Fehleranteile wesentlich besser gedämpft, da die Eigenwerte hier nahe Null liegen. Diese Eigenschaft nennt man die \textit{Glättungseigenschaft}.\\
Es stellt sich z.B. heraus [Saad], dass der optimale Relaxationsparameter, der unabhängig von der Schrittweite $h$ gewählt werden kann, $\omega = \frac {4} {5}$ ist. In \subfigureautorefname{ 3.3} ist gut zu sehen, dass ein Großteil des Spektrums nahe Null liegt.

\bild{omeganullacht}{14cm}{Für den optimalen Parameter $\omega = \frac{4}{5}$ [Saad] ist gut zu erkennen, dass viele Eigenwerte um die Null liegen und somit eine gute Glättungseigenschaft besteht.}{Für den optimalen Parameter $\omega = \frac{4}{5}$ [Saad] ist gut zu erkennen, dass viele Eigenwerte um die Null liegen und somit eine gute Glättungseigenschaft besteht.}\label{img.Jacobi3}

\section{Das Verfahren der konjugierten Gradienten}\label{s.Das Verfahren der konjugierten Gradienten}

Das Verfahren der konjugierten Gradienten wurde 1952 von Hestenes und Stiefel erstmals vorgestellt. Es zeichnet sich durch Stabilität und schnelle Konvergenz aus. \\
Das CG-Verfahren (conjugate gradient) - wie es auch genannt wird - ist eine Projektions- und Krylow-Raum-Methode.

\subsection{Definition (A-orthogonal)}\label{ss.A-orthogonal}
Sei $\mat{A}$ eine symmetrische, nicht singuläre Matrix. Zwei Vektoren $x,y \in \mathbb{R}^{n}$ heißen \underline{\textbf{konjugiert}} oder \underline{\textbf{A-orthogonal}}, wenn $x^{T}Ay = 0$ gilt.

\subsection{Satz}
Sei $A\in\mathbb{R}^{n \times n}$ s.p.d. und
\begin{equation}
f(u) := \frac {1} {2} u^{T}Au - f^{T}u,
\end{equation}
wobei $f,u \in \mathbb{R}^{n}$. Dann gilt:
\begin{center}
f hat ein eindeutig bestimmtes Minimum und
\end{center}
\begin{equation}
Au^{*} = f \Longleftrightarrow f(u^{*}) = \underset{u\in\mathbb{R}^{n}}{\min} f(u)
\end{equation}

% \textbf{Beweis:}

% \begin{enumerate}
% \item Eindeutigkeit: per Widerspruch \\
% Sei $\hat x$ ein weiteres Minimum von f. Dann ist
% \begin{equation}
% \nabla f(\hat x) = A\hat x - b = 0 \Rightarrow A\hat x = b. \notag
% \end{equation}
% $\Rightarrow Ax = b$ hat zwei Lösungen $x^{*}$ und $\hat x$. Widerspruch, da $A$ eine quadratische Matrix und $det(A) \ne 0 \Rightarrow $ das GLS hat eine eindeutige Lösung.
% \item $\Rightarrow:$ Sei $x^{*}$ die eind. Lsg. von $Ax = b$. Dann kann man $f(x)$ auch folgenderma{\ss}en schreiben:
% \begin{equation}
% f(x) = \frac 1 2 (x - x^{*})^{T}A(x - x^{*}) - c \hspace{2mm} mit \hspace{2mm} c = \frac 1 2 (x^{*})^{T}Ax^{*}. \notag
% \end{equation}
% Da $y^{T}Ay > 0 \hspace{2mm} \forall _{y \ne 0}$ und $c$ konstant ist, da es nicht von $x$ abhängt, folgt
% $$f(x) = \underbrace {\frac 1 2 (x - x^{*})^{T}A(x - x^{*})}_{\ge 0} - c$$
% ist genau dann minimal, wenn $x = x^{*}$.
% \item $\Leftarrow:$ Sei $f(x^{*})$ das Minimum von $f(x)$, dann gilt
% \begin{equation}
% \nabla f(x^{*}) = Ax^{*} - b = 0 \Rightarrow Ax^{*} = b. \notag
% \end{equation}
% $\Rightarrow x^{*}$ löst $Ax = b$
% \end{enumerate}
% \begin{flushright}
% $\blacksquare$
% \end{flushright}

Einen Beweis hierzu findet man z.B. in [Dahmen/Reusken].\\
Es ist also äquivalent die Funktion $f(u)$ zu minimieren und das Gleichungssystem $Au = f$ zu lösen. Betrachtet man nun den Gradienten von $f(x)$, so stellt man fest, dass gilt:
\begin{equation}
\nabla f(x) = Ax - f = -r.
\end{equation}
% Der Gradient von $f(x)$ ist das negative Residuum. Beim normalen Gradientenverfahren wählt man als Richtung des steilsten Abstiegs genau dieses Residuum. Beim CG-Verfahren wählt man konjugierte Richtungen $p$ für die Richtung des steilsten Abstiegs. Die dabei berechneten (konjugierten) Vektoren $p$ sind alle A-orthogonal und spannen einen sogenannten Krylow-Raum $U_{k} = \{p^{0},...,p^{k}\}$ auf. Für den Zusammenhang zwischen dem CG-Verfahren und Krylow-Räumen möchte ich an dieser Stelle auf [Braess] verweisen.\\
Da wir also stets das Minimum im Teilraum $U_{k}$ suchen, wird uns folgendes Lemma hilfreich sein:

% \bild{CG_Illu}{8cm}{Abstieg}{Abstieg}\label{img.CG}

\subsection{Lemma - (A-orthogonaler) Projektionssatz}\label{s.Projektionssatz}

Sei $U_{k}$ ein k-dimensionaler Teilraum des $\mathbb{R}^{n} \hspace{1mm} (k \le n)$, und $p^{0}, p^{1},...,p^{k-1}$ eine $\textit{A-orthogonale Basis}$ dieses Teilraums, also $\langle p^{i}, p^{j} \rangle _{A} = 0 \hspace{2mm} f\ddot{u}r \hspace{2mm} i \ne j$. Sei $v \in \mathbb{R}^{n}$, dann gilt für $u^{k} \in U_{k}$:

\begin{equation}
\|u^{k} - v\|_{A} = \underset{u \in U_{k}}{\min} \|u - v\|_{A}
\end{equation}

genau dann, wenn $u^{k}$ die $\textit{A-orthogonale Projektion}$ von $v$ auf $U_{k} = span\{p^{0},...,p^{k-1}\}$ ist. Außerdem hat $\textbf{u}^{k}$ die Darstellung

\begin{equation}
P_{U_{k,\langle \cdot,\cdot \rangle}}(v) = \textbf{u}^{k} = \sum_{j=0}^{k-1} \frac {\langle v, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j}
\end{equation}

Der Beweis zu diesem Lemma folgt direkt aus dem Projektionssatz [Dahmen/Reusken]

\subsection{Allgemeiner Algorithmus der konjugierten Gradienten}\label{ss.Allgemeiner CG-Algorithmus}

Zur Erzeugung der Lösung von $u^{*}$ durch Näherungen $u^{1}, u^{2},...$ definieren wir folgende Teilschritte [Dahmen/Reusken]:

\begin{description}

\item[0.] Definiere Teilraum $U_{1}$ und bestimme $r^{0}$ mit beliebigen Startvektor $u^{0}$
\begin{equation}
U_{1} := span\{r^{0}\} \textnormal{, wobei } r^{0} = f - Au^{0}
\end{equation}

\item[1.] Bestimme eine A-orthogonale Basis
\begin{equation}
p^{0},...,p^{k-1} \textnormal{ von } U_{k}.
\end{equation}

\item[2.] Bestimme eine Näherungslösung $u^{k}$, so dass gilt:
\begin{equation}
\|u^{k} - u^{*}\|_{A} = \underset{u \in U_{k}}{\min} \|x - u^{*}\|_{A}.
\end{equation}
Wir berechnen also:
\begin{equation}
u^{k} = \sum_{j=0}^{k-1} \frac {\langle u^{*}, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j}.\label{eq.Projektion}
\end{equation}

\item[3.] Erweitere den Teilraum $U_{k}$ und berechne erneut das Residuum
\begin{equation}
U_{k+1} := span\{p^{0},...,p^{k-1},r^{k}\} \textnormal{ wobei } r^{k} := f - Au^{k}.
\end{equation}

\end{description}

Nachdem man ein Residuum berechnet hat, startet der erste Iterationsschritt: Man erweitert seinen Teilraum um das Residuum und bestimmt darauf hin eine A-orthogonale Basis dieses Teilraumes. Ein gängiges Verfahren ist das Gram-Schmidt-Orthonormalisierungsverfahren. Die neue Näherungslösung bzgl. $U_{k}$ kann dann über den (A-orthogonalen) Projektionssatz bestimmt werden. Nachdem erneut ein Residuum berechnet wurde, startet der nächste Iterationsschritt.\\
Wegen \autoref{eq.Projektion} könnte man vermuten, dass $u^{*}$ zur Durchführung des Algorithmus bekannt sein muss. Die folgenden Lemmata werden zeigen, dass dem nicht so ist.

\subsection{Lemma}
Sei $u^{*} \in \mathbb{R}^{n}$ die Lösung von $Au = f$. Dann gilt für ein $y \in U_{k}$:
\begin{equation}
\langle u^{*}, y \rangle _{A} = \langle f, y \rangle
\end{equation}

\textbf{Beweis:}

Wir nutzen die Eigenschaften des A-Skalarproduktes aus:
\begin{equation}
\langle u^{*}, y \rangle _{A} = u^{{*}^{T}}Ay = y^{T}Au^{*} = y^{T}f = f^{T}y = \langle f, y \rangle. \notag
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

Nun wollen wir \autoref{eq.Projektion} neu formulieren.

\subsection{Lemma}
Sei $u^{*} \in \mathbb{R}^{n}$ die Lösung von $Au = f$ und $u^{k} \in \mathbb{R}^{n}$ die optimale Approximation von $u^{*}$ in $U_{k}$. Dann kann $u^{k}$ wie folgt berechnet werden:
\begin{equation}
u^{k} = u^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ mit } \alpha_{k-1} := \frac {\langle r^{0}, p^{k-1} \rangle} {\langle {p^{k-1}, Ap^{k-1}} \rangle}. \notag\label{eq.Alpha}
\end{equation}

Einen Beweis findet man in [Dahmen/Reusken].

\textbf{Bemerkung:}
$u^{k}$ kann dadurch mit wenig Aufwand aus $u^{k-1}$ und $p^{k-1}$ berechnet werden.

\subsection{Lemma}
Das Residuum $r^{k} \in \mathbb{R}^{n}$ kann einfach berechnet werden durch:
\begin{equation}
r^{k} = r^{k-1} - \alpha_{k-1}Ap^{k-1},
\end{equation}
wobei $\alpha_{k-1}$ wie in \autoref{eq.Alpha}.

\textbf{Beweis:}
\begin{eqnarray}
&&u^{k} = u^{k-1} + \alpha_{k-1}p^{k-1} \notag \\
&&\Longleftrightarrow Au^{k} = Au^{k-1} + \alpha_{k-1}Ap^{k-1} \notag \\
&&\Longleftrightarrow b - Au^{k} = b - Au^{k-1} - \alpha_{k-1}Ap^{k-1} \notag \\
&&\Longrightarrow r^{k} = r^{k-1} - \alpha_{k-1}Ap^{k-1}. \notag
\end{eqnarray}
\begin{flushright}
$\blacksquare$
\end{flushright}

Da wir nun $u^{k}$ und $r^{k}$ recht komfortabel bestimmen können, wollen wir im Folgenden noch eine Möglichkeit sehen, um $p^{k}$ schnell zu berechnen.

\subsection{Satz (Bestimmung einer A-orthogonalen Basis)}
Durch
\begin{equation}
p^{k-1} = r^{k-1} - \sum_{j=0}^{k-2} \frac {\langle r^{k-1}, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j},
\end{equation}
wird die A-orthogonale Basis zum Vektor $r^{k-1}$ bestimmt, wobei $p^{k-1},r^{k-1} \in \mathbb{R}^{n}$.

\textbf{Beweis:}

Der Beweis folgt direkt aus dem Gram-Schmidt-Orthonormalisierungsverfahren, welches allerdings einen hohen Rechenaufwand vorweist.\\
Wir wollen ohne Beweis (siehe z.B. [Dahmen/Reusken]) angeben, wie man die $p^{k}$ effizienter bestimmen kann.

\subsection{Satz}
Für die Berechnung von $p^{k}$ gilt:
\begin{equation}
p^{k-1} = r^{k-1} - \frac {\langle r^{k-1}, Ap^{k-2} \rangle} {\langle p^{k-2}, Ap^{k-2} \rangle} p^{k-2}.
\end{equation}

Substituiert man nun geschickt einige Werte in den Skalarprodukten, führt das auf den Algorithmus der konjugierten Gradienten.

% \textbf{Beweis:}
% \begin{eqnarray}
% x^{k} &=& \sum_{j=0}^{k-1} \frac {\langle x^{*}, p^{j} \rangle _{A}} {\langle {p^{j}, p^{j}} \rangle _{A}} p^{j} =
% \underbrace{\sum_{j=0}^{k-2} \frac {\langle x^{*}, p^{j} \rangle _{A}} {\langle {p^{j}, p^{j}} \rangle _{A}} p^{j}}_{=x^{k-1}} +
% \frac {\langle \overbrace{Ax^{*}}^{=b=b-Ax^{0}=r^{0}}, p^{k-1} \rangle} {\langle {Ap^{k-1}, p^{k-1}} \rangle} p^{k-1}\\
% &=& x^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ mit } \alpha_{k-1} \overset{(8)}{=} \frac {\langle r^{0}, p^{k-1} \rangle} {\langle {p^{k-1}, Ap^{k-1}} \rangle}
% \end{eqnarray}
% \begin{flushright}
% $\blacksquare$
% \end{flushright}

% \subsubsection{Erklärung zum CG-Algorithmus}\label{Erklärung zum CG-Algorithmus}

% \begin{description}

% \item[zu 0.] Initialisiere den Algorithmus und bestimme das Residuum. Hierfür ist ein Startvektor $x^{0}$ beliebig zu wählen.

% \item[zu 1.] Um eine A-orthogonale Basis von den $U_{k}$ zu bestimmen ist im wesentlichen ein (A-) Orthogonalisierungverfahren notwendig.

% \item[zu 2.] Hier wird das Verfahren aus \autoref{s.Projektionssatz} angewandt, um eine neue Näherungslösung in $U_{k}$ zu bestimmen.

% \item[zu 3.] Erweiterung des Teilraums $U_{k}$ zu $U_{k+1}$ durch das Hinzufügen des $k-ten$ Resdiduums (berechne: $r^{k} := b - Ax^{k}$).

% \end{description}

\subsection{Numerischer Algorithmus der konjugierten Gradienten}\label{ss.Numerisches CG}

Gegeben ist eine symmetrisch positiv definite Matrix $\mat{A} \in \mathbb{R}^{n}$. Bestimme die (Näherungs-) Lösung $u^{*}$ mit Hilfe eines \textit{beliebigen} Startvektors $u^{0} \in \mathbb{R}^{n}$ zu einer gegebenen rechten Seite $b \in \mathbb{R}^{n}$. Setze $\beta_{-1} := 0$ und berechne das Residuum $r^{0} = b - Au^{0}$. \\
Für $k = 1,2,...$, falls $r^{k-1} \ne 0$ berechne:

\begin{eqnarray}
p^{k-1} &=& r^{k-1} + \beta_{k-2}p^{k-2}, \textnormal{ wobei } \beta_{k-2} = \frac {\langle r^{k-1}, r^{k-1} \rangle} {\langle r^{k-2}, r^{k-2} \rangle} \textnormal{ mit } (k \ge 2) \notag \\
u^{k} &=& u^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ wobei } \alpha_{k-1} = \frac {\langle r^{k-1}, r^{k-1} \rangle} {\langle p^{k-1}, Ap^{k-1} \rangle} \notag \\
r^{k} &=& r^{k-1} - \alpha_{k-1}Ap^{k-1} \notag
\end{eqnarray}

Man muss in diesem Algorithmus pro Iterationsschritt lediglich zwei Skalarprodukte ausrechnen (die $r^{k-1}$-Skalarprodukte können für die $r^{k-2}$ nach der Berechnung des neuen Residuums wieder verwendet werden!) und eine Matrix-Vektor-Multiplikation durchführen. Somit erhält man einen Rechenaufwand von $\mathcal{O}(n^{2})$. Angewandt auf $\mat{A}_{2D}$ kann man - durch das Ausnutzen der Dünnbesetztheit - einen Aufwand pro Schritt von $\mathcal{O}(n)$ erreichen. Dies erreicht man durch eine geschickt gewählte Matrix-Vektor-Multiplikation, die die Struktur von $\mat{A}_{2D}$ ausnutzt.

An dieser Stelle soll noch ein kurzer Satz über die Konvergenz des Verfahrens folgen. Einen Beweis hierzu findet man z.B. in (Dahmen/Reusken 573):

\subsection{Satz (Konvergenz des CG-Algorithmus) [DahmenReusken]}\label{ss.Konvergenz CG}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$ symmetrisch, positiv definit und seien $u,f,u^{*},u^{k} \in \mathbb{R}^{n}$, wobei $u^{*}$ die exakte Lösung des Gleichungssystems $Au = b$ und $u^{k}$ die approximierte Lösung durch das CG-Verfahren ist. Dann gilt für $k = 1,2,...$:
\begin{equation}
\| u^{k} - u^{*} \|_{A} \le 2 \left( \frac {\kappa_{2} (A) - 1} {\kappa_{2} (A) + 1} \right)^{k} \| u^{0} - u^{*} \|_{A}
\end{equation}
Da stets $\frac {\kappa_{2} (A) - 1} {\kappa_{2} (A) + 1} < 1$ gilt, sichert dieser Satz die Konvergenz des Algorithmus. Man sieht also, dass die Konvergenz des Verfahrens von der Kondition der Matrix $\mat{A}_{2D}$ abhängt.

\textbf{Bemerkung:}

Das Verfahren der konjugierten Gradienten konvergiert im Allgemeinen nicht, wenn $\mat{A}$ nicht symmetrisch, positiv definit ist.

\section{Vorkonditioniertes Verfahren der konjugierten Gradienten (PCG)}\label{s.PCG}

Das PCG-Verfahren (preconditioned conjugate gradient) ist eine optimierte Version des CG-Verfahrens. Wie wir in \autoref{ss.Matrixkondition} gesehen haben, ist $\mat{A}_{2D}$ für feinere Gitter schlecht konditioniert. Und in \autoref{ss.Konvergenz CG} haben wir gesehen, dass die Konvergenz des CG-Verfahrens von eben dieser Matrix abhängt. Dies mindert natürlich die Effizienz des Verfahrens. Die Idee ist nun, die bei der Iteration zu Grunde liegende Matrix $\mat{A}$ durch eine ähnliche Matrix mit besserer Kondition zu ersetzen, damit sich das Konvergenzverhalten verbessert.

\subsection{Satz}

Sei $\mat{W} \in \mathbb{R}^{n \times n}$ s.p.d. dann gilt:
\begin{equation}
\mat{A}u = f \Longleftrightarrow \mat{W}^{-1} \mat{A} u = \mat{W}^{-1} f.
\end{equation}
Es macht also keinen Unterschied, ob wir $Au = f$ oder das äquivalente System lösen.

\subsubsection{Beweis:}

\begin{eqnarray}
\mat{A}u = f &\Longleftrightarrow& u = \mat{A}^{-1} \mat{E} f \Longleftrightarrow u = \mat{A}^{-1} \mat{W} \mat{W}^{-1} f \notag \\
&\Longleftrightarrow& u = (\mat{W}^{-1} \mat{A})^{-1} f \Longleftrightarrow \mat{W}^{-1} \mat{A} u = \mat{W}^{-1} f. \notag
\end{eqnarray}
\begin{flushright}
$\blacksquare$
\end{flushright}

Die Konditionszahl dieses Problem ist nun durch $\kappa_{2}(\mat{W}^{-1}\mat{A})$ bedingt. Das Ziel muss es also sein, $\mat{W}^{-1}$ so gut wie möglich zu wählen, damit die Kondition klein wird. Nun ist im Allgemeinen $\mat{W}^{-1}\mat{A}$ nicht s.p.d. Somit könnten wir zwar den CG-Alorithmus trotzdem anwenden, werden aber wegen dieser Tatsache möglicherweise keine Konvergenz erhalten. Um dies zu umgehen findet man einen Lösungsansatz (z.B. in Dahmen/Reusken 576), bei dem mit der Cholesky-Zerlegung eine entsprechende Umformung gefunden werden kann.

\subsection{Der Algorithmus des vorkonditionierten konjugierten Gradienten Verfahrens}\label{ss.Algorithmus PCG}

Gegeben seien $\mat{A}, \mat{W} \in \mathbb{R}^{n}$ s.p.d. Bestimme die (Näherungs-) Lösung $u^{*}$ mit Hilfe eines \textit{beliebigen} Startvektors $u^{0} \in \mathbb{R}^{n}$ zu einer gegebenen rechten Seite $b \in \mathbb{R}^{n}$. Setze $\beta_{-1} := 0$, berechne das Residuum $r^{0} = b - Au^{0}$ und $z^{0} = \mat{W}^{-1}r^{0}$ (löse $\mat{W}z^{0} = r^{0}$).\\
Für $k = 1,2,...$, falls $r^{k-1} \ne 0$ berechne:

\begin{eqnarray}
p^{k-1} &=& z^{k-1} + \beta_{k-2}p^{k-2}, \textnormal{ wobei } \beta_{k-2} = \frac {\langle z^{k-1}, r^{k-1} \rangle} {\langle z^{k-2}, r^{k-2} \rangle} \textnormal{ mit } (k \ge 2), \notag \\
u^{k} &=& u^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ wobei } \alpha_{k-1} = \frac {\langle z^{k-1}, r^{k-1} \rangle} {\langle p^{k-1}, Ap^{k-1} \rangle}, \notag \\
r^{k} &=& r^{k-1} - \alpha_{k-1}Ap^{k-1}, \notag \\
z^{k} &=& \mat{W}^{-1}r^{k} \textnormal{ (löse } \mat{W}z^{k} = r^{k}). \notag
\end{eqnarray}

Wichtig hierbei ist, dass das Lösen von $\mat{W}z^{k} = r^{k}$ mit möglichst wenig Aufwand (ideal: $\mathcal{O}(n)$) berechnet werden soll.

\subsection{Die unvollständige Cholesky-Zerlegung}\label{ss.ICCG}

Eine Matrix $\mat{A}$, die symmetrisch positiv definit ist, lässt sich durch eine Cholesky-Zerlegung in eine normierte untere Dreiecksmatrix $\mat{L}$ und eine rechte obere Dreicksmatrix $\mat{U}$ zerlegen, wobei gilt: $\mat{U} := \mat{D}\mat{L}^{T}$ \\
Mit dieser Zerlegung möchten wir nun unser System vorkonditionieren. Allerdings würde eine vollständige Cholesky-Zerlegung viele Nulleinträge in einer dünn besetzten Matrix auslöschen. Darum greift man auf eine unvollständige Cholesky-Zerlegung zurück, bei der die Stellen, an denen $\mat{A}$ Nulleinträge besitzt, in $L$ und $U$ ebenfalls Null bleiben.

\subsubsection{Definition (Muster E)}\label{sss.Muster E}

Sei $E \subseteq \{1,...,n\} \times \{1,...,n\}$ ein Muster, für das gilt:
\begin{equation}
E := \{(i,j) | 1 \le i,j \le n, a_{i,j} \ne 0 \}.
\end{equation}

Dann lässt sich die Matrix $\mat{A}$ auch folgendermaßen schreiben:
\begin{equation}
\mat{A} = \approx \tilde{\mat{L}} \tilde{\mat{L}}^{T}.
\end{equation}
wobei $\tilde{\mat{L}}, \tilde{\mat{L}}^{T}$ nicht die komplette Faktorisierung darstellt, sondern folgende Eigenschaften erfüllt:

\subsubsection{Eigenschaften der Matrix $\tilde{\mat{L}}$}\label{sss.Eigenschaften L CZ}
\begin{itemize}
\item $\tilde{\mat{L}}$ ist normierte untere Dreicksmatrix
\item Es gilt: $l_{i,j} = 0$, falls $(i,j) \notin E$
\end{itemize}

Natürlich ist diese Faktorisierung ungenauer, als die vollständige Zerlegung, allerdings genügt sie, um die Kondition des Gleichungssystems in vielen Fällen zu verbessern. Um den Algorithmus effizient und den Rechenaufwand so klein wie möglich zu machen, werden Summen nur über Indizes aus dem Muster berechnet.

\subsubsection{Der numerische Algorithmus der unvollständigen Cholesky Zerlegung}\label{sss.Cholesky-Algorithmus}

Seien $\mat{A} \in \mathbb{R}^{n \times n}$ und $E$ das Muster zur Matrix $\mat{A}$. Setze $\mat{L} = \mat{Id}$, $\mat{R} = 0$. Berechne dann für $i=1,2,...,n$:
\begin{eqnarray}
l_{i,i} &=& \left( a_{i,i} - \sum^{i-1}_{j=1,(i,j) \in E} l_{i,j}^{2} \right)^{\frac {1} {2}} \\
\textnormal{for k } &=& i+1,...,n: \textnormal{ if } (k,i) \in E: \\
l_{k,i} &=& \left( a_{k,i} - \sum^{k-1}_{j=1,(k,j) \in E, (i,j) \in E} l_{k,j} l_{i,j} \right) / l_{i,i}
\end{eqnarray}

\textbf{Bemerkungen:}

\begin{itemize}
\item Die für den PCG-Algorithmus wichtige Matrix $\mat{W}$ wird nun definierte als: $\mat{W} := \tilde{\mat{L}} \tilde{\mat{L}}^{T}$. Dadurch wird auch $\mat{W}z^{k} = \tilde{\mat{L}} \tilde{\mat{L}}^{T}z^{k} = r^{k}$ schnell durch Vorwärts- bzw. Rückwärtseinsetzen lösbar.
\item Für viele Probleme zeigt sich, dass $\kappa_{2}(\mat{W}^{-1} \mat{\mat{A}}) \ll \kappa_{2} (A)$ gilt. \\
\end{itemize}

Es gibt einige Varianten dieses Verfahrens. Wir wollen uns an dieser Stelle noch mit einer dieser auseinander setzen.

\subsection{Die modifizierte unvollständige Cholesky-Zerlegung}\label{ss.Modifizierte Cholesky}

Auch bei der modifizierten Methode des Verfahrens gehen wir vor, wie in \autoref{ss.ICCG}. Jedoch werden die Vorschriften für die Matrix $\tilde{\mat{L}}$ abgeändert:

\subsubsection{Eigenschaften der Matrix $\tilde{\mat{L}}$}\label{sss.Eigenschaften L MCZ}

Sei $e := (1,,1,...,1)^{T}$

\begin{itemize}
\item $a_{i,j} = (\tilde{\mat{L}} \tilde{\mat{L}}^{T})_{i,j}$ für alle $(i,j) \in E, i \ne j$
\item $\mat{A} e = \tilde{\mat{L}} \tilde{\mat{L}}^{T} e$, d.h. die Zeilensummen stimmen überein.
\item $l_{i,j} = r_{i,j} = 0$ für alle $(i,j) \notin E$
\end{itemize}

\subsubsection{Der numerische Algorithmus der modifizierten unvollständigen Cholesky-Zerlegung}\label{sss.Algorithmus MUCZ}

Seien $\mat{A} \in \mathbb{R}^{n \times n}$ s.p.d. und $E$ das Muster zur Matrix $\mat{A}$. Berechne dann für $i=1,2,...,n$:
\begin{eqnarray}
l_{i,i} &=& \left( a_{i,i} - \sum^{i-1}_{j=1,(i,j) \in E} l_{i,j}^{2} \right)^{\frac {1} {2}} \\
\textnormal{for k } &=& i+1,...,n: \\
&\textnormal{ if }& (k,i) \in E: \\
l_{k,i} &=& \left( a_{k,i} - \sum^{k-1}_{j=1,(k,j) \in E, (i,j) \in E} l_{k,j} l_{i,j} \right) / l_{i,i} \\
&\textnormal{ else }&:\\
a_{k,k} &=& a_{i,i} - \sum^{k-1}_{j=1,(k,j) \in E, (i,j) \in E} l_{k,j} l_{i,j}
\end{eqnarray}

Wir setzen wieder $\mat{W} := \tilde{\mat{L}} \tilde{\mat{L}}^{T}$. \\
Es gibt noch weitere Verfahren (siehe z.B. Saad), wie beispielsweise das SSOR-Verfahren, die wir hier nicht weiter diskutieren wollen.

\subsection{Effiziente Implementation der modifizierten Cholesky-Zerlegungen}

Gerade für diese Aufgabenstellung ist es von großem Interesse, wie der Algorithmus in Code umgesetzt wird. Offensichtlich haben beide Zerlegungen einen Rechenaufwand von $\mathcal{O}(n^{2})$. Auch wenn wir nur über das Muster $E$ iterieren, werden viele Werte der Matrix $\mat{A}$ überprüft. Dies kostet ebenfalls Rechenzeit.\\
Um dies zu optimieren wird im Fall der Matrix $\mat{A}_{2D}$ eine weitere Matrix $\mat{B} \in \mathbb{R}^{n \times 5}$ eingeführt. Wie bereits mehrfach erwähnt, enthält die 2D Poisson Matrix maximal 5 Werte ungleich Null pro Zeile. Somit können in der $i - ten$ Zeile maximal 5 Indizes $j$ auftauchen, für die gilt $a_{i,j} \ne 0$. In den Zeilen, in denen die Matrix weniger als fünf Werte ungleich Null hat, wird die $i - te$ Zeile von $\mat{B}$ mit $-1$ aufgefüllt.

\textbf{Beispiel:}

Um eine Vorstellung von $\mat{B}$ zu bekommen, wählen wir $\mat{A}_{2D} \in \mathbb{R}^{9 \times 9}$, also $m = 4, N = 3$. Dann folgt:
\begin{equation}
B = 
\begin{pmatrix}
1 & 2 & (1+N) & -1 & -1\\
1 & 2 & 3 & (2+N) & -1\\
  &   & \vdots & &\\
(5-N) & 4 & 5 & 6 & (5+N)\\
  &   & \vdots & &\\
(8-N) & 7 & 8 & 9 & -1\\
(9-N) & 8 & 9 & -1 & -1
\end{pmatrix}
\end{equation}

Mit der Matrix $\mat{B}$ als Ersatz für das Muster $E$ ergibt sich folgender C++-Code für die modifizierten unvollständige Cholesky-Zerlegung:

\subsection{C++-Methode der MIC}\label{s.MIC}\lstinputlisting[language=C]{code/MIC.c}

Analog würde diese Vorschrift für die unvollständige Cholesky-Zerlegung folgen, was wir hier nicht mehr explizit anführen wollen. Betrachtet man in \autoref{c.Vergleich} die Laufzeiten für den PCG-Algorithmus mit diesen Zerlegungen und der angeführten Implementation, sieht man wie schnell und effizient die Berechnung von statten geht.

\chapter{Mehrgitterverfahren}\label{c.Mehrgitterverfahren}

In diesem Abschnitt sollen nun die Mehrgittermethoden genauer betrachtet werden. Zunächst aber nochmals die Grundlagen:

\section{Grundlagen}\label{s.Idee MGM}

\begin{description}

\item[1.] Glättungseigenschaft der Jacobi-Relaxation \\
Das Jacobi-Relaxationsverfahren löscht kurzwellig Fehler in den ersten Iterationsschritten aus. Langwellige Fehler werden nur sehr langsam beseitigt.
\item[2.] Residuumsgleichung \\
Die für diesen Algorithmus wichtige Residuumsgleichung lautet:
\begin{equation}
\mat{A} e^{k} = r^{k}\label{eq.Residuumsgleichung}
\end{equation}
Die Lösung von $\mat{A} e^{k} = r^{k}$ ist äquivalent zur Lösung von $\mat{A}u=b$ für $e^{k} = 0$.

\end{description}

\textbf{Beweis:}

Das Residuum ist an der $k-ten$ Stelle definiert als 
\begin{equation}
r^{k} = b - \mat{A}u^{k}
\end{equation}
Der Fehler
\begin{equation}
e = u^{*} - u^{k}\label{eq.Fehler}
\end{equation}
wobei $u^{*}$ die exakte Lösung darstellt.
Betrachten wir jetzt nochmals \autoref{eq.Residuumsgleichung}, dann stellen wir fest, dass gilt:
\begin{equation}
\mat{A}e^{k} = r^{k} = b - \mat{A}u^{k} = 0 \textnormal{ für } e^{k} = 0.
\end{equation}
Somit ist
\begin{equation}
\mat{A}e^{k} = r^{k} \Longleftrightarrow \mat{A}u^{k} = b \textnormal{ falls } e^{k} = 0.
\end{equation}
\begin{flushright}
$\blacksquare$
\end{flushright}

% \begin{description}

% \item[3.] Grobe Fehler nach einer Gittertransformation \\
% Kurzwellige Fehler auf dem feinen Gitter bleiben kurzwellig, wenn man diese auf ein grobes Gitter transformiert. Langwellige Fehleranteile des feinen Gitters, werden jedoch auf dem groben Gitter ebenfalls zu kurzwelligen.

% \end{description}

% \textbf{Beispiel:}

% Wir wollen diese Aussage anhand eines Beispiels illustrieren. Wir betrachten...

\section{Prolongation}

Bevor wir nun auf die Mehrgittermethoden explizit eingehen, müssen wir uns Gedanken darüber machen, wie wir von einem Gitter auf das andere kommen. Angenommen wir befinden uns auf dem groben Gitter $\Omega_{2h}$, so ist das Ziel auf ein feineres Gitter $\Omega_{h}$ mit wenig Rechenaufwand zu kommen und die Werte aus $\Omega_{2h}$ sollten auf $\Omega_{h}$ gut genähert abgebildet werden. Für die Prolongation wählen wir hierfür eine lineare Interpolation.

\subsection{Interpolationsmatrix}

Sei $I^{h}_{2h}: \Omega_{2h} \longrightarrow \Omega_{h}$ eine Abbildung mit $I_{2h}^{h}(u_{2h}) = \mat{I}u_{2h} = u_{h}$, wobei $\mat{I} \in \mathbb{R}^{(2 \tilde N - 1)^{2} \times \tilde N^{2}}$ und $\tilde N$ die Anzahl der Gitterpunkte auf dem groben Gitter. Dabei überführt die Matrix $\mat{I}$ Vektoren von $\Omega_{2h}$ auf $\Omega_{h}$. Sie ist \textit{nicht} symmetrisch und kann verschiedene Gestalten haben.

\begin{equation}
\mat{I} = \frac {1} {4}
\begin{pmatrix}
I_{1} & & &\\
I_{2} & & &\\
& I_{1} & &\\
& I_{2} & &\\
& & \ddots &\\
& & & I_{1}\\
& & & I_{2}
\end{pmatrix},
\end{equation}

für $I_{1},I_{2} \in \mathbb{R}^{(2\tilde N - 1) \times \tilde N}$ gilt folgende Darstellung:

\begin{equation}
\mat{I}_{1} =
\begin{pmatrix}
4 & & & &\\
2 & 2 & & &\\
& 4 & & &\\
& 2 & 2 & &\\
& & \ddots & &\\
& & & 4 &\\
& & & 2 & 2\\
& & & & 4\\
\end{pmatrix},
\hspace{2cm}
\mat{I}_{2} =
\begin{pmatrix}
2 & \hdots & 2 & & & &\\
4 & \hdots & 4 & & & &\\
& 2 & \hdots & 2 & & &\\
& 4 & \hdots & 4 & & &\\
& & \ddots & & & &\\
& & & 2 & \hdots & 2 &\\
& & & 4 & \hdots & 4 &\\
& & & & 2 & \hdots & 2\\
\end{pmatrix}.
\end{equation}

Wir haben hier die Full-Weighted-Matrix der Interpolation verwendet, da sie die höchste Genauigkeit beim Übergang von $\Omega_{2h}$ auf $\Omega_{h}$ besitzt. Sie berücksichtigt bei der Interpolation nicht nur Gitterpunkte, die in $\Omega_{2h}$, sowie in $\Omega_{h}$, existieren, sondern auch den jeweiligen Nachbarn. Es gibt andere Möglichkeiten der Interpolation, z.B. den Half-Weighting-Operator, auf die wir in dieser Arbeit nicht explizit eingehen wollen. Zur Veranschaulichung des Full-Weighted-Operators dient \autoref{img.Prolongation}. Es geht also in jedes $u^{i,j}_{h}$ auch der gewichtete Wert aller Nachbarpunkte von $u^{i,j}_{2h}$ des feinen Gitters mit ein.

\textbf{Beispiel:}

\begin{equation}
\mat{I}u^{i,j}_{2h} = \frac {1} {4}
\begin{pmatrix}
I_{1} & & &\\
I_{2} & & &\\
& I_{1} & &\\
& I_{2} & &\\
& & \ddots &\\
& & & I_{1}\\
& & & I_{2}
\end{pmatrix}
\begin{pmatrix}
u_{2h}^{(1)} \\
\vdots \\
u_{2h}^{(m)}
\end{pmatrix} =
\begin{pmatrix}
u_{h}^{(1)} \\
u_{h}^{(2)} \\
\vdots \\
\vdots \\
u_{h}^{(n-1)}\\
u_{h}^{(n)}
\end{pmatrix} =
u^{i,j}_{h}.
\end{equation}

\textbf{Bemerkung:}

Für den Fall von eindimensionalen Gittern hätte $\mat{I}$ folgende Darstellung:
\begin{equation}
\mat{I} = \frac {1} {2}
\begin{pmatrix}
1\\
2\\
1 & 1\\
  & 2\\
  & 1\\
  &   & \ddots\\
  &   &          & 1 & 1\\
  &   &          &   & 2\\
  &   &          &   & 1
\end{pmatrix}.
\end{equation}
Hier ist $\mat{I} \in \mathbb{R}^{N \times \tilde N}$.

\bild{interpolation}{12cm}{Bei der Interpolation bedienen sich die Punkte vom feinen Gitter, den gewichteten Punkten des groben Gitters.}{Bei der Interpolation bedienen sich die Punkte vom feinen Gitter, den gewichteten Punkten des groben Gitters.}\label{img.Prolongation}

% \bild{Restriktionsoperator}{14cm}{bla}{bla}\label{img.Restriktion}

Für das Umsetzen in Programmcode ist es natürlich ungünstig eine komplette Matrix-Vektor-Multiplikation zu implementieren, zumal eine Matrix dieser Größe enorm viel Speicherplatz erfordert. Aus diesem Grund lässt sich die Interpolation auch in Komponentenschreibweise fassen:

\begin{eqnarray}
u_{h}^{2i-1,2j-1} = u_{2h}^{i,j} &\hspace{0.5cm}& i,j = 1,...,\tilde N, \\
u_{h}^{2i-1,2j} = \frac{1}{2} (u_{2h}^{i,j} + u_{2h}^{i,j+1}) &\hspace{0.5cm}& i = 1,...,\tilde N; j = 1,...,\tilde N - 1, \\
u_{h}^{2i,2j-1} = \frac{1}{2} (u_{2h}^{i,j} + u_{2h}^{i+1,j}) &\hspace{0.5cm}& i = 1,...,\tilde N - 1; j = 1,...,\tilde N, \\
u_{h}^{2i,2j} = \frac{1}{4} (u_{2h}^{i,j} + u_{2h}^{i,j+1} + u_{2h}^{i+1,j} + u_{2h}^{i+1,j+1}) &\hspace{0.5cm}& i,j = 1,...,\tilde N - 1.
\end{eqnarray}

Diese Vorschrift lässt sich nun relativ komfortabel und effizient programmieren. Somit ist nun der Übergang vom groben zum feinen Gitter bekannt. Nun wollen wir noch die Gegenrichtung betrachten.

\section{Restriktion}

Sei $R_{h}^{2h}: \Omega_{h} \longrightarrow \Omega_{2h}$ mit $R_{h}^{2h}(u_{h}) = \mat{R}u_{h} = u_{2h}$ und $\mat{R} \in \mathbb{R}^{\tilde N^{2} \times (2 \tilde N - 1)^{2}}$. Diese Abbildungsvorschrift nennt man Restriktion. Auch hier gibt es unterschiedliche Methoden, wobei in diesem Abschnitt das Gegenstück zur obigen Interpolation - der Full-Weighting-Operator für die Restriktion - verwendet wird. Auch dieser stellt den exaktesten Übergang zwischen beiden Gittern dar und hat einen speziellen Bezug zur Matrix $\mat{I}$
\begin{equation}
\mat{R} := \frac {1} {4} \mat{I}^{T}
\end{equation}

\subsection{Restriktionsmatrix}

Dadurch ist die Matrixdarstellung gegeben durch:

\begin{equation}
R = \frac{1}{16}
\begin{pmatrix}
I_{1}^{T} & I_{2}^{T}\\
		  & I_{1}^{T} & I_{2}^{T}\\
		  &			  &			  & \ddots\\
		  &			  &			  &		   & I_{1}^{T} & I_{2}^{T}
\end{pmatrix},
\end{equation}

wobei $I_{1}^{T}, I_{2}^{T}$ die transponierten Matrizen von $I_{1}, I_{2}$ darstellen.

\textbf{Beispiel:}

\begin{equation}
\mat{R}u^{i,j}_{h} = \frac {1} {16}
\begin{pmatrix}
I_{1}^{T} & I_{2}^{T}\\
		  & I_{1}^{T} & I_{2}^{T}\\
		  &			  &			  & \ddots\\
		  &			  &			  &		   & I_{1}^{T} & I_{2}^{T}
\end{pmatrix}
\begin{pmatrix}
u_{h}^{(1)} \\
u_{h}^{(2)} \\
\vdots \\
u_{h}^{(n)}
\end{pmatrix} =
\begin{pmatrix}
u_{2h}^{(1)} \\
\vdots \\
u_{2h}^{(m)}
\end{pmatrix} =
u^{i,j}_{2h}.
\end{equation}

Und für die Umsetzung in Code benutzen wir die Komponentenschreibweise:

Für ein $u_{2h}$ auf $\Omega_{2h}$ gilt:
\begin{equation}
u_{2h}^{i,j} = \frac {1} {16} (4u_{h}^{i,j}+2(u_{h}^{i+1,j}+u_{h}^{i-1,j}+u_{h}^{i,j+1}+u_{h}^{i,j-1})+u_{h}^{i-1,j-1}+u_{h}^{i-1,j+1}+u_{h}^{i+1,j+1}+u_{h}^{i+1,j-1})
\end{equation}

Das Vorgehen wird von \autoref{img.restrict} illustriert.

\bild{restrict}{12cm}{Ausgehend von einem Punkt innerhalb des Gitters, ist hier die Gewichtung der Werte veranschaulicht. Jeder Wert wird zusätzlich mit einem Faktor $\frac{1}{16}$ multipliziert}{Ausgehend von einem Punkt innerhalb des Gitters, ist hier die Gewichtung der Werte veranschaulicht. Jeder Wert wird zusätzlich mit einem Faktor $\frac{1}{16}$ multipliziert}\label{img.restrict}

\textbf{Bemerkung:}

Auch hier wollen wir noch die Restriktionsmatrix für den eindimensionalen Fall angeben:

\begin{equation}
\mat{R} = \frac {1} {4}
\begin{pmatrix}
1 & 2 & 1\\
  & 1 & 2 & 1\\
  &   &   &   & \ddots\\
  &   &   &   &        & 1 & 2 & 1
\end{pmatrix}
\end{equation}

\section{Transformation der Matrix}

Zum Abschluss sollte noch die Matrix $\mat{A}$ vom feinen Gitter auf das grobe Gitter transformiert werden. Befänden wir uns nicht auf dem Einheitsquadrat oder hätte $\mat{A}$ eine nicht so regelmäßige Struktur wie beispielsweise $\mat{A}_{2D}$, dann gilt folgende Transformationsvorschrift:

\begin{equation}
\mat{A}_{2h} = \mat{R}\mat{A}_{h}\mat{I},
\end{equation}

mit $\mat{A}_{2h},\mat{A}_{h},\mat{R},\mat{I}$ wie oben.\\
Da wir uns jedoch auf dem Einheitsquadrat befinden und $\mat{A}_{2D}$ eine günstige Struktur hat, wollen wir dieses Kapitel nicht weiter vertiefen. Lediglich soll angegeben werden, wie $\mat{A}_{2h}$ in unserem Fall nach der Transformation aussieht.

\textbf{Beispiel:}

Sei $N = 7$ die Anzahl der Gitterpunkte in beide Richtungen des feinen Gitters und $\tilde N = 3$ die Anzahl der Gitterpunkte in beide Richtungen des groben Gitters. Außerdem seien $\mat{A}_{2h} \in \mathbb{R}^{9 \times 9}$, $\mat{A}_{h} \in \mathbb{R}^{49 \times 49}$, $\mat{I} \in \mathbb{R}^{49 \times 9}$ und $\mat{R} \in \mathbb{R}^{9 \times 49}$. So folgt:

\begin{equation}
\mat{R}\mat{A}_{2D_{h}}\mat{I} = \mat{R}
\begin{pmatrix}
A_{1} & -Id \\
-Id & A_{2} & \ddots \\
 & \ddots & \ddots \\
 & & A_{6} & -Id\\
 & & -Id & A_{7}
\end{pmatrix}
\mat{I} = 
\begin{pmatrix}
A_{1} & -Id \\
-Id & A_{2} & -Id \\
 & -Id & A_{3}
\end{pmatrix}
= \mat{A}_{2D_{2h}}
\end{equation}

Die Matrizen $\mat{A}_{2D_{ih}}$ sind also für alle $i$ stets bekannt.

\section{Einführung in die Mehrgittermethoden}

Wie in\autoref{s.Finite Differenzen} gesehen befinden wir uns bei der Diskretisierung der Poisson-Gleichung auf einem Gebiet $\Omega_{h} = (0,1)^{2}$ der Schrittweite $h = \frac {1} {n}$. Nach der Ausführung von k-Iterationsschritten des Jacobi-Relaxationsverfahren sind auf diesem Gitter die kurzwelligen Fehler $e^{k} = u^{*} - u^{k}$ verschwunden. Nun berechnet man im $k-ten$ Schritt das Residuum $r^{k}$ und führt für das äquivalente lineare Gleichungssystem $\mat{A}e^{k} = r^{k}$, wobei $e^{k} = 0$ gilt, $k$ Iterationsschritte aus. So erhalten wir eine Näherung des Fehlers $e^{k}$. \\
Stellt man \autoref{eq.Fehler} um und berechnet $e^{k} + u^{k}$, so erhält man eine neue Approximation der exakten Lösung. \\
Kombiniert man dieses Vorgehen nun mit dem Wechsel zwischen zwei Gittern der Gitterweite $h$ und $2h$ so erhält man das Zweigitterverfahren.
  % und bringen dieses auf ein gröberes Gitter, z.B. $\Omega_{2h}$ mit der Schrittweite $2h$, könnten wir hier nun mit \autoref{eq.Residuumsgleichung} das Gleichungssystem mit $e_{2h} = 0$ lösen. Der niedrig frequente Fehler in $r^{k}$ wird in $r_{2h}$ so dann zu einem hochfrequenten auf $\Omega_{2h}$ werden. \\
% Führt man einige Iterationsschritte auf dem groben Gitter aus, bringt den verbleidenden Fehler $e_{2h}^{k}$ wieder zurück auf $\Omega_{h}$ und addiert diesen zur iterierten Lösung $u^{k}$ reduziert sich $e^{k}$. Diese Vorgehensweise führt man nun so lange durch, bis $e{k} \approx 0$.

\section{Das Zweigitterverfahren}\label{s.Der Zweigitter-Algorithmus}

Wir wollen das erste Verfahren der Mehrgittermethoden kennen lernen. Der Zweigitter-Algorithmus bildet die Basis für die Mehrgitterverfahren. Die Idee dahinter ist, zunächst die kurzwelligen Fehler durch a priori Fehlerglättung zu eliminieren und auf ein gröberes Gitter zu wechseln. Dort soll dann die Residuumsgleichung gelöst und auf das feinere Gitter zurück gekehrt werden. Hier findet eine a posteriori Fehlerglättung statt. Wiederhole dieses Vorgehen bis zur Konvergenz:

\begin{eqnarray}
\textbf{while }                                 &u^{k} \ne u^{*} \notag \\
&\textit{pre-smooth }                         &\textnormal{JacobiRelaxationMethod} \notag \\
&\textit{calculate residual }        &r^{k} = b - Au^{k} \notag \\
&\textit{restrict }                         &r^{k}_{2h}=Rr^{k}_{h} \notag \\
&                                                                &\mat{A}^{2h} = R\mat{A}^{h}P \notag \\
&\textit{set error }                        &e^{0}_{2h} = 0 \notag \\
&\textit{solve direct }                        &\mat{A}^{2h} e_{2h} = r^{k}_{2h} \notag \\
&\textit{prolongate }                        &e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }                        &u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth (optional) }        &\textnormal{JacobiRelaxationMethod} \notag \\
\textbf{end } \notag
\end{eqnarray}

Es bleiben zwei Fragen nun unbeantwortet:
\begin{description}
\item[1.] Wie soll die Residuumsgleichung auf dem gröberen Gitter gelöst werden?
\item[2.] Wie steht es mit der Konvergenz dieses Verfahrens? Besitzt es die nötige Rechengeschwindigkeit?
\end{description}

Die Antwort auf die Frage nach der Konvergenz werden wir in dieser Arbeit nicht behandeln. Für ein weiteres Studium wird [Saad] empfohlen.\\ \\

Der klare Nachteil dieser Methode liegt natürlich im direkten Lösen der Residuumsgleichung. Wählen wir ein sehr feines Gebiet $\Omega_{h}$ mit $m = 256$, also $h = \frac {1} {256}$, so liefert das Gebiet $\Omega_{2h}$ immer noch ein Gleichungssystem der Dimension $127^{2}$. Ein System dieser Ordnung zu lösen erfordert großen Rechenaufwand, der in dieser Form nicht erwünscht ist. \\

\section{Mehrgitter-Algorithmen}\label{s.Mehrgitteralgorithmus}

Da also das Lösen der Residuumsgleichung auf dem groben Gitter einen direkten Löser erfordert, der zusätzlichen Rechenaufwand bedeutet, ist der Zweigitter-Algorithmus nicht die Variante, die in der Praxis verwendet wird.

Eine bessere Methode ist, das Gitter immer gröber zu machen, bis das System direkt lösbar ist, um dann wieder auf das feinste Gitter zurück zu kehren. Wir erweitern also das Zweigitterverfahren um Rekursion. Denn ruft sich die Funktion in jedem Iterationsschritt selbst auf und löst direkt, sobald sie auf dem gröbsten Gitter befindet, erhalten wir folgende rekursive Funktion:

\begin{eqnarray}
\textbf{V-cycle }(u,b)                         & \notag \\
&\textbf{if }\textit{(coarsest grid) }&\textnormal{return } u_{finest grid}=\mat{A}^{-1}b \notag \\
&\textbf{else }                                                & \notag \\
&\textit{pre-smooth }                                 &\textnormal{JacobiRelaxationMethod} \notag \\
&\textit{calculate residual }                &r^{k} = b - Au^{k} \notag \\
&\textit{restrict }                                 &r^{k}_{2h}=Rr^{k}_{h} \notag \\
&                                                                        &\mat{A}^{2h} = R\mat{A}^{h}P \notag \\
&\textit{recursion }                                &e^{k}_{2h} = \textbf{V-cycle}(0,r^{k}_{2h}) \notag \\
&\textit{prolongate }                                &e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }                                &u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth }                                        &\textnormal{JacobiRelaxationMethod} \notag \\
&                                                                        &\textnormal{return } u_{h} \notag \\
\textbf{end}                                                & \notag
\end{eqnarray}

Dieser Algorithmus ist auch als V-Zyklus bekannt. Wie dieser Name zustande kommt illustriert (Bild V-Zyklus). Nun gibt es eine weitere Variante, den W-Zyklus (illustriert in Bild W-Zyklus):

\begin{eqnarray}
\textbf{W-cycle }(u,b)                         & \notag \\
&\textbf{if }\textit{(coarsest grid) }&\textnormal{return } u_{finest grid}=\mat{A}^{-1}b \notag \\
&\textbf{else }                                                & \notag \\
&\textit{pre-smooth }                                 &\textnormal{JacobiRelaxationMethod} \notag \\
&\textit{calculate residual }                &r^{k} = b - Au^{k} \notag \\
&\textit{restrict }                                 &r^{k}_{2h}=Rr^{k}_{h} \notag \\
&                                                                        &\mat{A}^{2h} = R\mat{A}^{h}P \notag \\
&\textit{recursion }                                &e^{k}_{2h} = \textbf{W-cycle}(0,r^{k}_{2h}) \notag \\
&\textit{recursion }                                &e^{k}_{2h} = \textbf{W-cycle}(0,r^{k}_{2h}) \notag \\
&\textit{prolongate }                                &e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }                                &u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth }                                        &\textnormal{JacobiRelaxationMethod} \notag \\
&                                                                        &\textnormal{return } u_{h} \notag \\
\textbf{end}                                                & \notag
\end{eqnarray}

% Man kann zeigen [Briggs/Saad], dass die Mehrgittermethoden einen Rechenaufwand von $\mathcal{O}(n)$ pro Iterationsschritt haben. Somit sind Mehrgitterverfahren effiziente und häufig verwendete Lösungsalgorithmen für lineare Gleichungssysteme. Im letzten Kapitel werden wir nun noch vergleichen, welcher der bisher vorgestellten Algorithmen für unsere Aufgabenstellung gut geeignet ist.

\chapter{Ein Vergleich zwischen Iterativen- und Mehrgitter-Methoden}\label{c.Vergleich}

In diesem letzten Kapitel wollen wir die Iterativen-Verfahren, speziell das Verfahren der konjugierten Gradienten, und die Mehrgitterverfahren numerisch vergleichen. Dafür betrachten wir folgende Gleichung:

\section{Beispiel einer Poisson Gleichung}\label{s.Beispiel einer Poisson Gleichung}

Seien $f: \Omega \rightarrow \mathbb{R}$ und $g: \partial\Omega \rightarrow \mathbb{R}$ stetige Funktionen mit $f(x,y) = -4.$ und $g(x,y) = x^{2} + y^{2}$. Sei außerdem $\Omega = (0,1)\times(0,1) \in \mathbb{R}^{2}$.Gegeben ist das Randwertproblem
\begin{eqnarray}
        -\Delta u(x,y) &=& f(x,y) = -4 \textnormal{ in } \Omega \\
    u(x,y) &=& g(x,y) = x^{2} + y^{2} \textnormal{ in } \partial \Omega
\end{eqnarray}
Gesucht ist eine Funktion $u(x,y)$, die diese Gleichung löst. \\
Offensichtlich löst der elliptische Paraboloid $u(x,y) = x^{2} + y^{2}$ die partielle Differentialgleichung, da $\partial_{xx}u(x,y) = \partial_{yy}u(x,y) = 2$. Allerdings wollen wir nun diese Lösung auch numerisch erhalten.\\
Die gewünschte Lösung sollte also folgenden Graphen ergeben:

%bild

Zu beachten ist außerdem, dass wir $\Omega_{h} \in [0,1]$ gewählt haben. Es wird also auch nur das Viertel des Graphen ausgegeben, welches sich im Einheitsquadrat befindet.\\

\section{Zur Implementierung in C++}

Das gesamte Programm wurde objektorientiert geschrieben, darum ist stets von Methoden und Klassen, nicht von Funktionen die Rede. In den folgenden Beispielen wollen wir zunächst die Lösung der Poisson Gleichung für verschiedene Verfahren betrachten. Dafür werden die jeweiligen Methoden der Klassen ebenfalls dargestellt. Außerdem wollen wir nicht nur die Iterationsschritte genauer betrachten, sondern auch die Rechenzeit.\\
An manchen Stellen im Code kommt die Vermutung auf, dass es sich um Pseudocode handeln könnte. Dies ist natürlich nicht der Fall. Es wurden lediglich bestimmte Operatoren wie *,+,-,etc. überladen.

\section{Abbruchkriterien}

Um zu verstehen, warum wir Abbruchkriterien benötigen, soll hier ein kurzes Beispiel folgen:\\
Das CG-Verfahren konvergiert bekanntlich nach maximal $n$ Schritten. Wir brauchen also kein Abbruchkriterium, damit wir die optimale Lösung finden. Angenommen die Dimension der Matrix ist $n = 10^{6}$. Dann werden trotz der schnellen Konvergenz eine große Anzahl an Iterationen benötigt. Im schlimmsten Fall eben $10^{6}$. Um dies zu vermeiden, lässt man den Algorithmus abbrechen, sobald eine gewisse Toleranzgrenze erreicht ist. In der Praxis schätzt man das $k-te$ Residuum in der A-Norm oder der 2-Norm gegen eine Grenze ab. In diesem Beispiel wählen wir folgenden Ansatz:

\begin{equation}
\| u^{k} - u^{*} \|_{2} \le 10^{-3} \cdot \| u^{0} - u^{*} \|_{2}.
\end{equation}

Im Allgemeinen ist natürlich die Lösung der partiellen Differentialgleichung nicht bekannt. Hier existiert allerdings die analytische Lösung und wir können das Abbruchkriterium so wählen.

\section{Lösung der Poisson Gleichung (Jacobi-Verfahren)}\label{s.Jacobi mit Beispiel}

Wie wir bereits in \autoref{ss.Spektralradius Jacobi} gesehen haben, konvergiert das Jacobi-Verfahren nur langsam. Es überrascht darum nicht, dass einige Iterationsschritte nötig sind, um das Gleichungssystem zu lösen. Trotz einer effizienten Programmierung benötigt die Methode viel Rechenzeit. Dies illustriert (Tabelle kommt noch).

% \subsection{Jacobi C++-Methode}\label{s.cpu}\lstinputlisting[language=C]{code/Jacobi.c}

% Der \"JacobiMethod\"-Methode wird die Matrix $\mat{A}$, der Iterationsvektor $x$ und der Lösungsvektor $b$ übergeben.

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 40  & 80 & 160 & 320 \\\hline\hline
\multirow{5}* & Schritte & 2092  & 8345 & 33332  & 133227  \\\cline{2-6}
& Rechenzeit in s &  0.45  & 7.28 & 120.31 & 2034.71 \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

\section{Lösung der Poisson Gleichung (Jacobi-Relaxations-Verfahren)}\label{s.JacobiRelax mit Beispiel}

\subsection{Parameter $\omega = \frac {1} {2}$}

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 40  & 80 & 160 & 320 \\\hline\hline
\multirow{5}* & Schritte & 4186  & 16693 & 66667  & 133227  \\\cline{2-6}
& Rechenzeit in s &  0.88  & 14.43 & 237.10 & bla \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

\subsection{Parameter $\omega = \frac {4} {5}$}

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 40  & 80 & 160 & 320 \\\hline\hline
\multirow{5}* & Schritte & 2615  & 10432 & 41666  & 166534  \\\cline{2-6}
& Rechenzeit in s &  0.55  & 9.00 & 147.91 & 3794.98 \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

\section{CG-Verfahren angewandt auf das Beispiel}\label{s.CG mit Beispiel}

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 40  & 80 & 160 & 320 \\\hline\hline
\multirow{5}* & Schritte & 65  & 130 & 262  & 525  \\\cline{2-6}
& Rechenzeit in s &  0.02  & 0.16 & 1.35 & 11.26 \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

\section{PCG-Verfahren angewandt auf das Beispiel}\label{s.PCG mit Beispiel}

\subsection{Unvollständige Cholesky-Zerlegung}

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 40  & 80 & 160 & 320 \\\hline\hline
\multirow{5}* & Schritte & 20  & 40 & 79  & 158  \\\cline{2-6}
& Rechenzeit in s &  0.01  & 0.14 & 1.1 & 8.78 \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

\subsection{Modifizierte unvollständige Cholesky-Zerlegung}

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 40  & 80 & 160 & 320 \\\hline\hline
\multirow{5}* & Schritte & 7  & 10 & 13  & 19  \\\cline{2-6}
& Rechenzeit in s &  0.00  & 0.04 & 0.21 & 1.23 \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

\section{Das Mehrgitterverfahren angewandt auf das Beispiel}\label{s.Multigrid mit Beispiel}

\subsection{V-Zyklus}\label{ss.V-Zyklus mit Beispiel}

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 32  & 64 & 128 & 256 \\\hline\hline
\multirow{5}* & Schritte & 11  & 29 & 63  & 11 \\\cline{2-6}
& Rechenzeit in s &  0.18  & 0.70 & 3.61 & 22.00 \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

\subsection{W-Zyklus}\label{ss.W-Zyklus mit Beispiel}

\begin{table}[H]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{$N$} & 32  & 64 & 128 & 256 \\\hline\hline
\multirow{5}* & Schritte & 2092  & 8345 & 33332  & 133227  \\\cline{2-6}
& Rechenzeit in s &  0.45  & 7.28 & 120.31 & bla \\\hline
\end{tabular}
\caption[Jacobi-Iterationsverfahren]{Je größer $N$ wird, desto mehr Iterationsschritte und Rechenaufwand ist zum Lösen der Gleichung nötig.}
\vspace{2ex}\end{table}

% \begin{table}[H]\vspace{1ex}\centering\begin{tabular}{|l|l|}
% \hline
% Formen & Städte\\
% \hline
% \hline
% Quadrat &  Bunkenstedt \\
% \hline
% Dreieck &  Laggenbeck\\
% \hline
% Kreis &  Peine\\
% \hline
% Raute & Wakaluba \\
% \hline
% \end{tabular}
% \caption{\label{tab.sinnlos}eine sinnlose Tabelle}
% \vspace{2ex}\end{table}

% \begin{table}[H]\vspace{1ex}\centering
% \begin{tabular}{|ll||l|l|l|l|}\hline
% \multicolumn{2}{|c||}{}&\multicolumn{4}{c|}{ dies} \\
% \multicolumn{2}{|c||}{}& von dort  & und dort & über hier & zu Los \\\hline\hline
% \multirow{3}*{\rotatebox{90}{das}} & hier &  bla  & bla  & bla  & bla \\\cline{2-6}
% & dort & bla  & bla & bla  & bla  \\\cline{2-6}
% & da &  bla  & bla & bla & bla \\\hline
% \end{tabular}
% \caption[eine kompliziertere Tabelle]{eine kompliziertere Tabelle mit viel Beschreibungstext, der aber nicht im Tabellenverzeichnis auftauschen soll}
% \vspace{2ex}\end{table}

% Er hörte leise Schritte hinter sich. Das bedeutete nichts Gutes. Wer würde ihm schon folgen, spät in der Nacht und dazu noch in dieser engen Gasse mitten im übel beleumundeten Hafenviertel? Gerade jetzt, wo er das Ding seines Lebens gedreht hatte und mit der Beute verschwinden wollte! Hatte einer seiner zahllosen Kollegen dieselbe Idee gehabt, ihn beobachtet und abgewartet, um ihn nun um die Früchte seiner Arbeit zu erleichtern? Oder gehörten die Schritte hinter ihm zu einem der unzähligen Gesetzeshüter dieser Stadt, und die stählerne Acht um seine Handgelenke würde gleich zuschnappen? Er konnte die Aufforderung stehen zu bleiben schon hören. Gehetzt sah er sich um. Plötzlich erblickte er den schmalen Durchgang. Blitzartig drehte er sich nach rechts und verschwand zwischen den beiden Gebäuden. Beinahe wäre er dabei über den umgestürzten Mülleimer gefallen, der mitten im Weg lag. Er versuchte, sich in der Dunkelheit seinen Weg zu ertasten und erstarrte: Anscheinend gab es keinen anderen Ausweg aus diesem kleinen Hof als den Durchgang, durch den er gekommen war. Die Schritte wurden lauter und lauter, er sah eine dunkle Gestalt um die Ecke biegen. Fieberhaft irrten seine Augen durch die nächtliche Dunkelheit und suchten einen Ausweg. War jetzt wirklich alles vorbei, waren alle Mühe und alle Vorbereitungen umsonst? Er presste sich ganz eng an die Wand hinter ihm und hoffte, der Verfolger würde ihn übersehen, als plötzlich neben ihm mit kaum wahrnehmbarem Quietschen eine Tür im nächtlichen Wind hin und her schwang. Könnte dieses der flehentlich herbeigesehnte Ausweg aus seinem Dilemma sein? Langsam bewegte er sich auf die offene Tür zu, immer dicht an die Mauer gepresst. Würde diese Tür seine Rettung werden?


% Anhang
% \begin{landscape}\begin{multicols}{2}
% \appendix
% \chapter{Anhang}
% \section{Quelltexte}
% \subsubsection*{cpu.c aus Linux 2.6.16}\label{s.cpu}\lstinputlisting[language=C]{code/cpu.c}
% \end{multicols}\end{landscape}


% \bibliographystyle{alphadin_martin}
% \bibliography{bibliographie}


\chapter*{Erklärung}

Hiermit versichere ich, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe, dass alle Stellen der Arbeit, die wörtlich oder sinngemäß aus anderen Quellen übernommen wurden, als solche kenntlich gemacht und dass die Arbeit in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde vorgelegt wurde.

\vspace{3cm}
Ort, Datum \hspace{5cm} Unterschrift\\

\end{document}