\input{arbeit-vorlage-praeambel.tex} % Importiere die Einstellungen aus der Präambel
% hier beginnt der eigentliche Inhalt
\begin{document}
\pagenumbering{Roman} % große Römische Seitenummerierung
\pagestyle{empty}

% Titelseite
\clearscrheadings\clearscrplain

\begin{center}
\begin{Huge}
Fakultät für Mathematik\\
\vspace{3mm}
\end{Huge}{\Large Universität Regensburg}\\

\vspace{20mm}
\begin{Large}
Ein Vergleich des Verfahrens der konjugierten Gradienten und Mehrgittermethoden, angewandt auf die diskretisierte Poisson-Gleichung\\
\end{Large}
\vspace{8mm}
Bachelor-Arbeit\\
\vspace{0.4cm}
\vspace{2 cm}
Michael Bauer \\
Matrikel-Nummer 1528558\\
\vspace{8cm}
\begin{tabular}{ll}
{\bf Erstprüfer}&Prof. Garcke\\
{\bf Zweitprüfer}&Prof. ...\\
\end{tabular}

\end{center}
\clearpage


\pagestyle{useheadings} % normale Kopf- und Fußzeilen für den Rest

\tableofcontents
\listoffigures
\listoftables

\chapter*{Symbolverzeichnis}\label{s.sym}
\addcontentsline{toc}{chapter}{Symbolverzeichnis}
\markboth{Symbolverzeichnis}{Symbolverzeichnis}
\section*{Allgemeine Symbole}\label{s.sym.alg}
\begin{flushleft}\begin{tabularx}{\textwidth}{l|X}
Symbol & Bedeutung\\\hline
$a$ & der Skalar $a$ \\
$\vec{x}$ & der Vektor $\vec{x}$\\
$\mat{A}$ & die Matrix $\mat{A}$\\
\end{tabularx}\end{flushleft}




% richtiger Inhalt
\chapter{Einleitung}\label{c.Einleitung}
\pagenumbering{arabic} % ab jetzt die normale arabische Nummerierung

Viele Prozesse in den Naturwissenschaften, wie Biologie, Chemie und Physik, aber auch der Medizin, Technik und Wirtschaft lassen sich auf partielle Differentialgleichungen (PDG) zurückführen. Das Lösen solcher Gleichungen ist allerdings nicht immer möglich, oder aufwendig. \\
Eine PDG, die vor Allem in der Physik häufige Verwendung findet, ist die Poisson-Gleichung – eine elliptische partielle Differentialgleichung zweiter Ordnung. So genügt diese Gleichung beispielsweise dem elektrostatischen Potential $u$ zu gegebener Ladungsdichte $f$, aber auch dem Gravitationspotential $u$ zu gegebener Massendichte $f$. \\
Methoden aus der numerischen Mathematik ermöglichen uns nun das Lösen von partiellen Differentialgleichungen mittels computerbasierten Algorithmen. Hierbei wird jedoch nicht die Lösung direkt bestimmt, sondern versucht eine exakte Approximation der Lösung zu erhalten. Dabei ist es wichtig, dass der zugrunde liegende Algorithmus effizient ist. \\
Um nun die Lösung einer partiellen Differentialgleichung mittels effizienten Algorithmus bestimmen zu können, müssen wir uns im Vorfeld Gedanken darüber machen, wie wir diese am besten erhalten. Eine der zentralen Methoden der Numerik sind Finite Differenzen. Hierbei diskretisiert man das Gebiet, auf dem die PDG definiert ist und führt die Gleichung auf ein lineares Gleichungssystem zurück. \\
Eine Möglichkeit ein solches lineares Gleichungssystem zu lösen sind iterative Verfahren. Somit hat man ein großartiges Werkzeug, dass eine Lösung innerhalb weniger Iterationsschritte berechnet und auch mit sehr großen Systemen gut zurecht kommt. Natürlich gilt das nicht für jedes Verfahren, weshalb wir über die Verfahren sprechen möchten, die diese Kriterien erfüllen. \\
Abschließend wollen wir uns dann noch mit Mehrgittermethoden beschäftigen. Sie stellen wohl die effizienteste und modernste Methode dar, ein lineares Gleichungssystem zu lösen.


\chapter{Diskretisierung der Poisson-Gleichung im $\mathbb{R}^{2}$}\label{c.Diskretisierte Poisson-Gleichung}
Um die Poisson-Gleichung zu diskretisieren, werden wir diese zunächst definieren. Außerdem werden wir die Methode der finitien Differenzen einführen, um dann ein Gleichungssystem der Form $\mat{A}u = f$ zu erhalten.

\section{Definition (Poisson-Gleichung)}\label{s.Poisson-Gleichung}

Sei $\Omega = (0,1)\times(0,1) \in \mathbb{R}^{2}$ ein beschränktes, offenes Gebiet des $\mathbb{R}^{2}$. Gesucht wird eine Funktion $u(x,y)$, die das Randwertproblem
\begin{eqnarray}
	-\Delta u(x,y) &=& f(x,y) \textnormal{ in } \Omega \\
    u(x,y) &=& g(x,y) \textnormal{ in } \partial \Omega
\end{eqnarray}
löst.
Dabei seien $f: \Omega \rightarrow \mathbb{R}$ und $g: \partial\Omega \rightarrow \mathbb{R}$ stetige Funktionen und es bezeichnet $\Delta := \sum\limits_{k=1}^{n} \frac {\partial^{2}} {\partial x_{k}^{2}}$ den Laplace-Operator. Für die Poisson-Gleichung im $\mathbb{R}^{2}$ gilt dann:
\begin{eqnarray}
	-\Delta u(x,y) &=& \frac {\partial^{2} u(x,y)} {\partial x^{2}} + \frac {\partial^{2} u(x,y)} {\partial y^{2}} = f(x,y) \textnormal{ in } \Omega \\
    u(x,y) &=& g(x,y) \textnormal{ in } \partial \Omega
\end{eqnarray}
(2.2) und (2.4) nennt man Dirichlet-Randbedingung.

\section{Bemerkungen}\label{s.Bemerkungen zur Poisson-Gleichung}

\begin{itemize}
\item Die Funktion $u(x,y)$ ist häufig formelmäßig nicht darstellbar und wird mit Hilfe von numerischen Verfahren in $\Omega$ genähert (Parallele numerische Verfahren/Seite 18 unten)
\item Man kann zeigen, dass, wenn $\partial \Omega$ aus glatten Liniensegmenten (z.B. Geraden) zusammengesetzt ist und $f(x,y) \in C^{1}(\Omega), g \in C(\partial \Omega)$ gilt, die Gleichungen (2.1),(2.2) bzw. (2.3),(2.4) eine eindeutige Lösung besitzen (Dahmen,Reusken Seite 463).
\end{itemize}

Um diese (elliptische) partielle Differentialgleichung nun in $\Omega$ zu diskretisieren, bedarf es der Hilfe der Finiten Differenzen Methode.

\section{Finite Differenzen-Methode für die Poisson-Gleichung}\label{s.Finite Differenzen}

\subsection{(Zentraler) Differenzenquotient zweiter Ordnung}\label{ss.Differenzenquotient zweiter Ordnung}

Wir betrachten ein $(x,y) \in \Omega$ beliebig. Dann gilt für $h > 0$ mit der Taylorformel
\begin{eqnarray}
u(x+h,y) &=& u(x,y) + h \partial_{x} u(x,y) + \frac {h^{2}} {2!} \partial_{xx} u(x,y) + \frac {h^{3}} {3!} \partial_{xxx} u(x,y) + ... \\
u(x-h,y) &=& u(x,y) - h \partial_{x} u(x,y) + \frac {h^{2}} {2!} \partial_{xx} u(x,y) - \frac {h^{3}} {3!} \partial_{xxx} u(x,y) + ...
\end{eqnarray}
Analog können wir diese Betrachtung für $u(x,y+h)$ und $u(x,y-h)$ machen. \\
Löst man nun (2.5) und (2.6) jeweils nach $\partial_{xx} u(x,y)$ auf und addiert die zwei Gleichungen, so erhält man:

\begin{equation}
\partial_{xx} u(x,y) + O(h^{2}) = \frac {u(x-h,y) - 2u(x,y) + u(x+h,y)} {h^{2}}
\end{equation}

Ebenso lösen wir nach $\partial_{yy} u(x,y)$ auf und erhalten analog:
\begin{equation}
\partial_{yy} u(x,y) + O(h^{2}) = \frac {u(x,y-h) - 2u(x,y) + u(x,y+h)} {h^{2}}
\end{equation}

Wobei hier $\partial_{xx} u(x,y) = \frac {\partial^{2} u(x,y)} {\partial x^{2}}$ und $\partial_{yy} u(x,y) = \frac {\partial^{2} u(x,y)} {\partial y^{2}}$ gemeint ist.
Diese Näherungen nennt man auch (zentralen) Differenzenquotienten der zweiten Ableitung. $O(h^{2})$ ist ein Term zweiter Ordnung und wird vernachlässigt.

Somit erhalten wir für $\Delta u(x,y)$ die Näherung
\begin{eqnarray}
\Delta u(x,y) &=& \frac {\partial^{2} u(x,y)} {\partial x^{2}} + \frac {\partial^{2} u(x,y)} {\partial y^{2}} = \partial_{xx} u(x,y) + \partial_{yy} u(x,y) \notag \\
&\approx& \frac {u(x-h,y) + u(x+h,y) - 4u(x,y) + u(x,y-h) + u(x,y+h)} {h^{2}}
\end{eqnarray}

\subsection{Diskretisierung von $\Omega$}\label{ss.Diskretisierung}

Mit einem zweidimensionalen Gitter, der Gitterweite $h$, wobei $h \in \mathbb{Q}$ mit $h = \frac {1} {n}$ und $n \in \mathbb{N}_{>1}$, wird nun das Gebiet $\Omega$ diskretisiert. Die Zahl $(n-1)$ gibt uns an, wie viele Gitterpunkte es jeweils in x- bzw. y-Richtung gibt.\\

Für $i = 1,...,(n-1)$ und $j = 1,...,(n-1)$ kann man dann $u(x,y)$ auch in der Form $u(x_{i},y_{j})$ schreiben:
\begin{equation}
u(x_{i},y_{j}) := u(ih,jh)
\end{equation}

und $\Omega$ fassen wir als $\Omega_{h}$ auf, so dass:
\begin{equation}
\Omega_{h} := \{u(ih, jh) | 1 \le i,j \le (n-1)\}
\end{equation}

Betrachten wir nun noch den Rand von $\Omega_{h}$:
\begin{equation}
\overline \Omega_{h} := \{u(ih, jh) | 0 \le i,j \le n\}
\end{equation}

Mit der Formel (2.9) ergibt sich nun für $\Delta u(x,y)$ die diskretisierte Form:
\begin{eqnarray}
\Delta_{h} u(x,y) &:=& \frac {u(x-h,y) + u(x+h,y) - 4u(x,y) + u(x,y-h) + u(x,y+h)} {h^{2}} \notag \\
&=& \frac {u(x_{i}-h,y_{i}) + u(x_{i}+h,y_{i}) - 4u(x_{i},y_{i}) + u(x_{i},y_{i}-h) + u(x_{i},y_{i}+h)} {h^{2}} \notag \\
&=& \frac {u(ih-h,jh)+u(ih+h,jh)-4u(ih,jh)+u(ih,jh-h)+u(ih,jh+h)} {h^{2}} \notag \\
&=& \frac {1} {h^{2}}
\begin{pmatrix}
\frac {u(x,y+h)} {u(x,y)}, & 1, & \frac {u(x,y-h)} {u(x,y)}
\end{pmatrix}
\begin{pmatrix}
  & 1 & \\
1 & -4 & 1 \\
  & 1 & 
\end{pmatrix}
\begin{pmatrix}
u(x+h,y) \\
u(x,y) \\
u(x-h,y)
\end{pmatrix}
\end{eqnarray}
Diese Approximation wird auch 5-Punkt-Differenzenstern genannt, siehe dazu Abbildung (2.1).

%\bild{diffStar}{8cm}{5-Punkt-Differenzenstern im Gitter}{5-Punkt-Differenzenstern im Gitter}\label{img.5-Point-Star}

%\bild{grid}{8cm}{(Lexikographische) Nummerierung von $\Omega = (0,1)^{2}$ mit $n=5$}{Gitter}\label{img.gridWithNumbers}

Nummeriert man nun alle Gitterpunkte des Gitters fortlaufend von links unten nach rechts oben durch (\autoref{img.gridWithNumbers}) und stellt für jeden dieser Punkte die Gleichung (2.13) auf, so führt dies auf eine $(n-1)^{2} \times (n-1)^{2}$-Matrix der Form:

\begin{equation}
\mat{A} = \frac {1} {h^{2}}
\begin{pmatrix}
A_{1} & -Id \\
-Id & A_{2} & \ddots \\
 & \ddots & \ddots & -Id \\
 & & -Id & A_{n}
\end{pmatrix}
\end{equation}
wobei $\mat{Id} \in \mathbb{R}^{(n-1) \times (n-1)}$ die Identität meint und für alle $i = 0,..,n$ gilt:

\begin{equation}
A_{i} = 
\begin{pmatrix}
4 & -1 & & \\
-1 & 4 & \ddots & \\
 & \ddots & \ddots & -1 \\
 & & -1 & 4
\end{pmatrix}
\end{equation}
mit $A_{i} \in \mathbb{R}^{(n-1) \times (n-1)}$ symmetrisch, positiv definit (s.p.d).

Um nun auf ein lineares Gleichungssystem der Form $\mat{A}u = f$ zu kommen, muss natürlich noch die rechte Seite, also das $f$ aufgestellt werden. Aus der Form der Matrix ist erkennbar, dass nicht alle Punkt aus $\overline \Omega$ in $\mat{A}$ auftauchen. Das liegt daran, dass die Randpunkte die aus der Dirichlet-Randbedingung resultieren ($u(x,y) = f(x,y)$) bereits bekannt sind und somit nicht genähert werden müssen. Jedoch muss zu jeder Komponente in $f$, die einen Randpunkt als Nachbarn hat, dieser aufaddiert werden. Dies führt uns auf folgende rechte Seite:

\begin{equation}
f = 
\begin{pmatrix}
f_{1} \\ f_{2} \\ \vdots \\ f_{n-1}
\end{pmatrix}
\end{equation}

wobei gilt

\begin{equation}
f_{1} = 
\begin{pmatrix}
f(h,h) + h^{-2}(g(h,0)+g(0,h)) \\
f(2h,h) + h^{-2}(g(2h,0)) \\
\vdots \\
f(1-2h,h) + h^{-2}(g(1-2h,0)) \\
f(1-h,h) + h^{-2}(g(1-h,0)+g(0,1-h))
\end{pmatrix}
\end{equation}

\begin{equation}
f_{j} = 
\begin{pmatrix}
f(h,jh) + h^{-2}(g(0,jh)) \\
f(2h,jh) \\
\vdots \\
f(1-2h,jh) \\
f(1-h,jh) + h^{-2}(g(1,jh))
\end{pmatrix}
2 \le j \le n-2,
\end{equation}

\begin{equation}
f_{n-1} = 
\begin{pmatrix}
f(h,1-h) + h^{-2}(g(h,1)+g(0,1-h)) \\
f(2h,1-h) + h^{-2}(g(2h,1)) \\
\vdots \\
f(1-2h,1-h) + h^{-2}(g(1-2h,1)) \\
f(1-h,1-h) + h^{-2}(g(1-h,1)+g(1,1-h))
\end{pmatrix}
\end{equation}

Nun steht das lineare Gleichungssystem der Form $\mat{A}u = f$, wobei $\mat{A}$ und $f$ bekannt sind und $u$ der Lösungsvektor ist, der die Lösung der partielle Differentialgleichung enthält.

% \section{Das Residuum und die Residuumsgleichung}\label{s.Residuum und Resdiuumsgleichung}

% Ein weiterer Begriff, der im Zusammenhang mit linearen Gleichungssystemen oft fällt, ist der des Residuums. Dieses wird uns in \autoref{s.Das Verfahren der konjugierten Gradienten} und \autoref{c.Mehrgitterverfahren} wieder begegnen.

% \subsection{Definition (Residuum)}\label{ss.Das Residuum}

% Sei $\mat{A}u = f$ ein lineares Gleichungssystem mit $A \in \mathbb{R}^{n \times n}$, $u,f \in \mathbb{R}^{n}$. Dann gilt für $r \in \mathbb{R}^{n}$:
% \begin{equation}
% r := f - \mat{A}u
% \end{equation}
% wobei wir $r$ das Residuum nennen.

% \subsection{Die Residuumsgleichung}\label{ss.Resdiuumsgleichung}

% Seien $\mat{A},u,f$ und $r$ wie in \autoref{ss.Das Residuum}. Dann gilt für $e \in \mathbb{R}^{n}$:

\section{Eigenwerte und Eigenvektoren der Matrix $\mat{A}$}\label{s.Eigenwerte und Eigenvektoren}

Der Spektralradius bzw. die Kondition der Matrix $\mat{A}$ spielt in den iterativen Verfahren, wie auch bei den Mehrgittermethoden eine große Rolle. Sie entscheidet über die Konvergenzeigenschaften und die Effizienz des Algorithmus. Darum wollen wir uns in diesem Kapitel mit den Eigenwerten und Eigenvektoren der Matrix $\mat{A}$ genauer befassen. \\
Da $\mat{A}$ symmetrisch ist, existiert eine Orthogonalbasis aus Eigenvektoren für die gilt:
\begin{equation}
\mat{A}x = \lambda x
\end{equation}
Für die diskretisierte Poisson Gleichung des $\mathbb{R}1{2}$, lässt sich dies auch in der Form:
\begin{equation}
\mat{A}x_{ij} = \lambda_{ij} x{ij}
\end{equation}

\subsection{Eigenwerte von $\mat{A}$}\label{ss.Eigenwerte}

Sei $\mat{A}$ die Matrix der 2D Poisson Gleichung. Dann lassen sich die Eigenwerte wie folgt darstellen:
\begin{equation}
\lambda_{ij} = \frac {4} {h^{2}} \left(\sin^{2}(\frac {1} {2} i \pi h) + \sin^{2}(\frac {1} {2} j \pi h) \right) \textnormal{ für } 1 \le i,j \le n-1
\end{equation}
wobei $(n-1)$ die Anzahl der Gitterpunkte in x- und in y-Richtung definiert (siehe oben).

\subsection{Eigenvektoren von $\mat{A}$}\label{ss.Eigenvektoren}

Die Eigenvektoren für die Matrix $\mat{A}$ lassen sich in folgender Form darstellen:
\begin{equation}
u_{ij} = \sin(i \pi h) \sin(j \pi h) \textnormal{ für } i,j \in \mathbb{N}
\end{equation}



\chapter{Iterative Lösungsvefahren für lineare Gleichungssysteme}\label{c.IterativeVerfahren}

Gleichungssysteme, die partielle Differentialgleichungen lösen können sehr groß werden, da man das entsprechende Gitter sehr fein wählen will, um eine möglichst genaue Lösung zu erhalten. Aus diesem Grund sind direkte Verfahren, wie z.B. der Gauß-Algorithmus oder die LR-Zerlegung nicht geeignet. Ihr Rechenaufwand beläuft sich im Allgemeinen auf $\mathcal{O}(n^{3})$ und ist dadurch zu langsam. Fehler - durch Maschinengenauigkeit bedingt - werden außerdem in diesen Verfahren verstärkt. Darum müssen bessere Verfahren gewählt werden. \\
Ein wesentlicher Bestandteil der numerischen Mathematik sind iterative Verfahren zu Lösung linearer Gleichungssysteme. Diese zeichnen sich meist durch eine schnelle Konvergenz und einen geringen Rechenaufwand aus. Wir wollen uns im folgenden dem Jacobi-Verfahren (Gesamtschrittverfahren), Gauß-Seidel-Verfahren (Einzelschrittverfahren), SOR-Verfahren (SOR = successive overrelaxation) und dem CG-Verfahren (ohne und mit Vorkonditionierung) widmen.

\section{Das Jacobi-Verfahren (Gesamtschrittverfahren)}\label{s.Das Jacobi-Iterationsverfahren}

Im folgenden betrachten wir das oben beschriebene Gitter mit $(N-1)$ Gitterpunkten in x- und y-Richtung und erhalten dadurch für die Dimension $n = (N-1) \cdot (N-1)$. \\


\subsection{Das allgemeine Jacobi-Iterationsverfahren}\label{ss.Allgemeines Jacobi-Verfahren}

Sei $\mat{A} \in \mathbb{R}^{n \times n}$ und $f,u \in \mathbb{R}^{n}$, wobei $u$ die Lösung des linearen Gleichungssystems $\mat{A}u = f$ ist. Dann lässt sich $\mat{A}$ wie folgt zerlegen:

\begin{equation}
A = D - L - U
\end{equation}

Dabei sind $D,L,U \in \mathbb{R}^{n \times n}$, wobei $\mat{D}$ die Diagonalelemente von $\mat{A}$ enthält, $\mat{L}$ eine strikte untere und $\mat{U}$ eine strikte obere Dreiecksmatrix sind. \\
Somit ergibt sich für $\mat{A}u = f$:

\begin{equation}
Au = f \Leftrightarrow (D-L-U)u = f \Leftrightarrow Du = (L+U)u + f
\end{equation}

Ist nun $D$ nicht singulär, so gilt für das Jacobi-Verfahren folgende Iterationsvorschrift:
\begin{equation}
Du^{k+1} = (L+U)u^{k} + f \Leftrightarrow u^{k+1} = D^{-1}(L+U)u^{k} + D^{-1}f
\end{equation}

Mit der Iterationsmatrix $T := D^{-1}(L+U)$.
Wobei dies in Komponentenschreibweise wie folgt aussieht: \\
Mit einem Startvektor $u^{0} \in \mathbb{R}^{n}$ und $k=1,2,...$ berechne für $i=1,...,n$:

\begin{equation}
u^{k}_{i} = \frac {1} {a_{ii}} (f_{i} - \sum_{\substack{j = 1 \\ j \ne i}}^{n} a_{ij}u^{k-1}_{i})
\end{equation}

Offensichtlich wird jedes $u^{k}$ durch seinen Vorgänger berechnet. Der Rechenaufwand pro Iterationsschritt beträgt hier $\mathcal{O}(n^{2})$ und entspricht somit einer Matrix-Vektor-Multiplikation.

\subsection{Das Jacobi-Iterationsverfahren für die Poisson-Gleichung}\label{ss.Jacobi-Verfahren der Poisson Gleichung}

Da die Matrix, die durch das diskretisierte Poisson-Problem aufgestellt wird, dünn besetzt ist, können wir den Rechenaufwand für dieses spezielle Problem auf $\mathcal{O}(n)$ pro Iterationsschritt verbessern. Dafür benötigen wir nochmals den 5-Punkt-Differenzenstern und die partielle Differentialgleichung:

\begin{equation}
\frac {u(x_{i-1},y_{j}) + u(x_{i+1},y_{j}) - 4u(x_{i},y_{j}) + u(x_{i},y_{j-1}) + u(x_{i},y_{j+1})} {h^{2}} = f(x,y)
\end{equation}

Löst man diese Gleichung nun nach $u(x_{i},y_{j})$ auf und führt dies für alle $u(x_{i},y_{i})$ durch, so erhält man folgende Iterationsvorschrift für das Jacobi-Verfahren: \\

Für $k = 1,2,...$ berechne mit Startvektor $u^{0} \in \mathbb{R}^{n}$ \\
Für $i = 1,...,N-1$ \\
$hspace{1cm}$ Für $j = 1,...,N-1$
\begin{equation}
u^{k}_{i,j} =   \frac {1} {4} (u^{k}_{i-1,j} + u^{k}_{i+1,j} + u^{k}_{i,j-1} + u^{k}_{i,j+1} - h^{2}f(x_{i},y_{j}))
\end{equation}

Der Rechenaufwand beträgt nun pro Iterationsschritt lediglich $\mathcal{O}((N-1) \cdot (N-1)) = \mathcal{O}(n)$ Schritte.

Ein weiteres Verfahren, welches für die diskretisierte Poisson-Gleichung sogar doppelt so schnell konvergiert, als das Jacobi-Verfahren wollen wir nun im nächsten Abschnitt betrachten.


\section{Das Gauß-Seidel-Verfahren (Einzelschrittverfahren)}\label{s.Das Gauss-Seidel-Verfahren}

\subsection{Das allgemeine Gauss-Seidel-Iterationsverfahren}\label{ss.Allgemeines Gauss-Seidel-Verfahren}

Mit der selben Vorschrift und den selben Überlegungen wie oben, wollen wir die Matrix $\mat{A}$ wie folgt zerlegen:
\begin{equation}
A = D - L - U
\end{equation}

Somit ergibt sich für $\mat{A}u = f$:
\begin{equation}
Au = f \Leftrightarrow (D-L-U)u = f \Leftrightarrow (D-L)u = Uu + f
\end{equation}

Daraus können wir nun folgende Iterationsvorschrift ableiten:
\begin{equation}
(D-L)u^{k+1} = Uu^{k} + f \Leftrightarrow u^{k+1} = (D-L)^{-1}Uu^{k} + (D-L)^{-1}f
\end{equation}

Dies nun in Komponentenschreibweise für $i=1,...,n$:
\begin{equation}
\sum\limits_{j=1}^{i} a_{ij}u_{j}^{k+1} = -\sum\limits_{j=i+1}^{n} a_{ij}u_{j}^{k} + f_{i}\label{eq.GaussSeidel}
\end{equation}

Formt man \autoref{eq.GaussSeidel} um, so erhält man den Algorithmus des Gauss-Seidel-Verfahrens mit Startvektor $u^{0} \in \mathbb{R}$: \\

Für $k = 1,2,...$ berechne für $i = 1,...,n$
\begin{eqnarray}
u_{i}^{k+1} &=& \frac {1} {a_{ii}} (f_{i} - \sum\limits_{j=1}^{i-1} a_{ij}u_{j}^{k+1} - \sum\limits_{j=i+1}^{n} a_{ij}u_{j}^{k})
\end{eqnarray}

Wie zu sehen ist, verwendet dieser Algorithmus dieses mal nicht nur die Werte aus dem vorigen Iterationsschritt, sondern auch welche aus dem der gerade berechnet wird. Das Gauss-Seidel-Verfahren wartet ebenfalls mit einem Rechaufwand von $\mathcal{O}(n^{2})$ auf. Wie beim Jacobi-Verfahren kann man dies auf $\mathcal{O}(n)$ optimieren.

\subsection{Das Gauss-Seidel-Iterationsverfahren für die Poisson-Gleichung}\label{ss.Gauss-Seidel-Verfahren der Poisson Gleichung}

Um nun eine angepasste Formel bzw. Iterationsvorschrift zu erhalten gehen wir wieder mit Hilfe des 5-Punkt-Differenzensterns vor. Allerdings verwendet wie oben beschrieben der Algorithmus auch Werte, die im aktuellen Iterationsschritt berechnet wurden. Um dies zu gewährleisten müssen wir uns nur \autoref{img.gridWithNumbers} ansehen. Hierbei ist ersichtlich, dass $u_{i-1,j}$ und $u_{i,j-1}$ bereits neu berechnet wurden. $u_{i+1,j}$ und $u_{i,j+1}$ stammen noch aus dem vorigen Iterationsschritt.

Daraus ergibt sich der spezielle Gauss-Seidel-Algorithmus für die Poisson-Gleichung:
\begin{equation}
u^{k}_{i,j} =   \frac {1} {4} (u^{k}_{i-1,j} + u^{k}_{i,j-1} + u^{k-1}_{i+1,j} + u^{k-1}_{i,j+1} - h^{2}f(x_{i},y_{j}))
\end{equation}
Auch hier reduziert sich der Rechenaufwand auf $\mathcal{O}(n)$.


\section{Warum Einzelschritt- und Gesamtschrittverfahren?}\label{s.Warum Einzelschritt- und Gesamtschrittverfahren?}
Zum lösen von großen linearen Gleichungssystemen werden das Jacobi-Verfahren und das Gauss-Seidel-Verfahren heute kaum mehr verwendet. Es gibt mittlerweile schnellere, stabilere und effizientere Verfahren wie wir im fortlaufenden sehen werden. Einen großen Vorteil haben allerdings beide Verfahren: Sie löschen große Fehler bereits nach den ersten Iterationsschritten aus. Darum finden sie besonders große Verwendung in den Mehrgittermethoden, die wir später kennen lernen werden.

\section{Die SOR-Methode (SOR = successive overrelaxation)}\label{s.SOR-Methode}

Eine Optimierung von \autoref{eq.GaussSeidel} bietet das SOR-Verfahren oder auch Gauss-Seidel-Relaxationsverfahren. Die Einführung eines Relaxationsparameters $\omega$ kann in einigen Fällen das Iterationsverfahren effizienter machen. Speziell für die Poisson-Gleichung wird nicht nur die Effizienz signifikant verbessert, es ist auch ein optimaler Parameter $\omega$ bekannt (siehe hierzu Dahmen/Reusken).

\subsection{Algorithmus des SOR-Verfahrens}\label{ss.Algorithmus des SOR-Verfahrens}

Für $k = 1,2,...$ berechne für $i = 1,...,n$
\begin{eqnarray}
u_{i}^{k+1} &=& u_{i}^{k} - \frac {\omega} {a_{ii}} (f_{i} - \sum\limits_{j=1}^{i-1} a_{ij}u_{j}^{k+1} - \sum\limits_{j=i}^{n} a_{ij}u_{j}^{k}) \\
&=& (1-\omega)u_{i}^{k} - \frac {\omega} {a_{ii}} (f_{i} - \sum\limits_{j=1}^{i-1} a_{ij}u_{j}^{k+1} - \sum\limits_{j=i+1}^{n} a_{ij}u_{j}^{k})
\end{eqnarray}

Offensichtlich erhält man für $\omega = 1$ wieder das Gauss-Seidel-Verfahren. Außerdem existiert, wie bereits erwähnt, nicht immer ein $\omega$, welches das Verfahren schneller konvergieren lässt. Da für die Poisson Gleichung sogar ein optimaler Parameter existiert, wollen wir uns diesen näher ansehen.

\subsection{Satz (Optimaler Parameter $\omega$)}\label{ss.optimales omega}

Sei $\mu := cos(\pi h)$. Für die diskretisierte Poisson Gleichung $Au = f$ gilt dann für den Relaxationsparameter:

\begin{equation}
\omega_{opt} := 1 + \left( \frac {\mu} {1 + \sqrt{1 - \mu^{2}}} \right)
\end{equation}

Verlgeichen Sie hierzu auch Dahmen/Reusken (Seite 564).

\subsection{SOR in Matrix-Darstellung}\label{ss.Matrixdarstellung}

Der Vollständigkeit halber, wollen wir uns jetzt noch der Matrix-Darstellung der SOR-Methode widmen. Im wesentlichen entspricht sie dem Gauss-Seidel-Verfahren. Lediglich der Relaxationsparameter muss eingebaut werden. Man erhält:

\begin{eqnarray}
(\mat{Id}+\omega\mat{D}^{-1}\mat{L})x^{k+1} &=& [(1-\omega)\mat{I}-\omega\mat{D}^{-1}\mat{R}]x^{k} + \omega\mat{D}^{-1}b \notag \\
\Longleftrightarrow \mat{D}^{-1}(\mat{D}+\omega\mat{L})x^{k+1} &=& \mat{D}^{-1}[(1-\omega)\mat{D}-\omega\mat{R}]x^{k} + \omega\mat{D}^{-1}b \notag \\
\end{eqnarray}

dies ergibt

\begin{equation}
x^{k+1} = (\mat{D}+\omega\mat{L})^{-1}[(1-\omega)\mat{D}-\omega\mat{R}]x^{k}+\omega(\mat{D}+\omega\mat{L})^{-1}b
\end{equation}

Mit dieser Gleichung und einigen weiteren Überlegungen kann gezeigt werden, dass folgender Satz gilt.

\subsection{Satz}\label{ss.konvergenz für omega}

Sei $\mat{A}$ hermitesch und positiv definit, dann konvergiert das Gauß-Seidel-Relaxationsverfahren genau dann, wenn $\omega \in (0,2)$ ist. (aus oranges buch) \\ \\

Einen Beweis hierzu findet man beispielsweise in (oranges buch) \\ \\

Aus \autoref{ss.optimales omega} ist der optimale Paramater für die Poisson Gleichung bereits bekannt. Also konvergiert - wie erwartet - das Verfahren für $\omega = $.

\section{Das Verfahren der konjugierten Gradienten}\label{s.Das Verfahren der konjugierten Gradienten}

Das Verfahren der konjugierten Gradienten wurde 1952 von Heestens und Stiefel erstmals vorgestellt. Das Verfahren zeichnet sich durch Stabilität und schnelle Konvergenz aus. \\
Das Verfahren der konjugierten Gradienten (auch CG-Verfahren) ist eine Krylov-Unterraum-Methode und gehört zu den Projektionsverfahren. Charakterisitisch für das CG-Verfahren ist außerdem die A-Orthogonalität der Basen des Krylovraums. Die Orthogonalität hängt also von der zugrunde liegenden Matrix $\mat{A}$ ab. Aus diesem Grund müssen wir zunächst definieren, was A-orthogonal heißt.

\subsection{Definition (A-orthogonal)}\label{ss.A-orthogonal}
Sei $\mat{A}$ eine symmetrische, nicht singuläre Matrix. Zwei Vektoren $x,y \in \mathbb{R}^{n}$ heißen \underline{\textbf{konjugiert}} oder \underline{\textbf{A-orthogonal}}, wenn $x^{T}Ay = 0$ gilt.

\subsubsection{Bemerkung:}\label{sss.Bemerkung zu A-orthogonal}

\begin{itemize}
\item Es definiert $\langle x,y \rangle _{A} = x^{T}Ay$ ein Skalarprodukt auf dem $\mathbb{R}^{n}$ für $\mat{A}$ s.p.d.
\item Wir nennen $\|x\|_{A} := \sqrt{\langle x, x \rangle _{A}}$ die Energie-Norm.
\end{itemize}

Nun wollen wir noch den Projektionssatz in Abhängigkeit unserer Matrix $\mat{A}$ definieren.

%1.4.Lemma
\subsection{Lemma - (A-orthogonaler) Projektionssatz}\label{s.Projektionssatz}

Sei $U_{k}$ ein k-dimensionaler Teilraum des $\mathbb{R}^{n} \hspace{1mm} (k \le n)$, und $p^{0}, p^{1},...,p^{k-1}$ eine $\textit{A-orthogonale Basis}$ dieses Teilraums, also $\langle p^{i}, p^{j} \rangle _{A} = 0 \hspace{2mm} f\ddot{u}r \hspace{2mm} i \ne j$. Sei $v \in \mathbb{R}^{n}$, dann gilt für $u^{k} \in U_{k}$:

\begin{equation}
\|u^{k} - v\|_{A} = \underset{u \in U_{k}}{\min} \|u - v\|_{A}
\end{equation}

genau dann, wenn $u^{k}$ die $\textit{A-orthogonale Projektion}$ von $v$ auf $U_{k} = span\{p^{0},...,p^{k-1}\}$ ist. Außerdem hat $\textbf{u}^{k}$ die Darstellung

\begin{equation}
P_{U_{k,\langle \cdot,\cdot \rangle}}(v) = \textbf{u}^{k} = \sum_{j=0}^{k-1} \frac {\langle v, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j}
\end{equation}

Der Beweis zu diesem Lemma folgt direkt aus dem Projektionssatz. \\
Was dieser Satz nun aussagt ist, dass man einen Vektor $v$ in $U_{k+1}$ sucht. Nun wählen wir ein beliebiges $v \in U_{k+1}$ und minimiert über alle $u^{k} \in U_{k}$. Die optimale Lösung ist dann der gesucht Vektor $v$.
Bildlich gesprochen, ist $v$ die (A-)orthogonale Projektion auf $U_{k}$. Vergleiche Bild (bla). \\ \\

Da wir nun die Grundlagen für das CG-Verfahren geschaffen haben, wollen wir nun den Algorithmus betrachten.

%1.5. Algorithmus
\subsection{Allgemeiner Algorithmus der konjugierten Gradienten}

Zur Erzeugung der Lösung von $x^{*}$ durch Näherungen $x^{1}, x^{2},...$ definieren wir folgende Teilschritte:

\begin{description}

\item[0.] Definiere den ersten Teilraum und bestimme das (Start-) Residuum mit Startvektor $x^{0}$
\begin{equation}
U_{1} := span\{r^{0}\} \textnormal{, wobei } r^{0} = b - Ax^{0}
\end{equation}

\item[1.] Bestimme eine A-orthogonale Basis
\begin{equation}
p^{0},...,p^{k-1} \textnormal{ von } U_{k}
\end{equation}

\item[2.] Bestimme eine Näherungslösung $x^{k}$, so dass
\begin{equation}
\|x^{k} - x^{*}\|_{A} = \underset{u \in U_{k}}{\min} \|x - x^{*}\|_{A}
\end{equation}
gilt. Mit dem A-orthogonalen Projektionssatz berechnen wir also:
\begin{equation}
x^{k} = \sum_{j=0}^{k-1} \frac {\langle x^{*}, p^{j} \rangle _{A}} {\langle p^{j}, p^{j} \rangle _{A}} p^{j}
\end{equation}

\item[3.] Erweitere den Teilraum $U_{k}$ und berechne das iterierte Residuum
\begin{equation}
U_{k+1} := span\{p^{0},...,p^{k-1},r^{k}\} \textnormal{ wobei } r^{k} := b - Ax^{k}
\end{equation}

\end{description}

Natürlich ist das (noch) kein numerischer Algorithmus, den man in Programmcode umsetzen kann, allerdings sollte man sich die Schritte des CG-Algorithmus klar machen, um die Effizienz dahinter zu verstehen:

\subsubsection{Erklärung zum CG-Algorithmus}\label{Erklärung zum CG-Algorithmus}

Vielleicht das Wichtigste vorab: Der Algorithmus endet nach maximal n Schritten. Da wir die Lösung $u \in \mathbb{R}^{n}$ suchen und unsere Teilräume $U_{k}$ mit $k \le n$ sind, muss nach spätestens n Schritten das Verfahren die optimale Lösung im $\mathbb{R}^{n}$  gefunden haben. Oftmals ist eine gesuchte Näherung in einem Teilraum der Lösung bereits sehr nahe bzw. gleich der Lösung. Dann bricht der Algorithmus vorzeitig ab. \\ \\
Um nun das Verfahren zu erklären, betrachten wir nochmals alle Teilschritte des Algorithmus:
\begin{description}

\item[zu 0.] Was der erste Schritt im wesentlichen aussagt ist, dass man den Algorithmus initialisisert und ein Residuum bestimmen muss. Hierfür ist ein Startvektor $x^{0}$ beliebig zu wählen (\autoref{bla}). Das Residuum wird durch $r^{0} := b - Ax^{0}$ definiert.

\item[zu 1.] Um eine A-orthogonale Basis von den $U_{k}$ zu bestimmen ist im wesentlichen eine (A-) Orthogonalisierungverfahren notwendig, welches hier auch angewandt wird.

\item[zu 2.] Hier wird das in \autoref{s.Projektionssatz} bereits besprochene Verfahren angewandt, um eine neue Näherungslösung in $U_{k}$ zu bestimmen.

\item[zu 3.] Hier soll im wesentlich das Gleiche geschehen in wie in Schritt 0. Lediglich wird nun das $k-te$ Resdiduum durch $r^{k} := b - Ax^{k}$ bestimmt.

\end{description}

All diese Überlegung führen uns nun zu einem numerischen Algorithmus, den man in Programmcode umsetzen kann. Die einzelnen Herleitungen und Beweise sind z.B. in Dahmen/Reusken (Seiten bla) zu finden.

\subsection{Numerischer Algorithmus der konjugierten Gradienten}

Gegeben ist eine symmetrisch positiv definitie Matrix $\mat{A} \in \mathbb{R}^{n}$. Bestimme die (Näherungs-) Lösung $x^{*}$ mit Hilfe eines \textit{beliebigen} Startvektors $x^{0} \in \mathbb{R}^{n}$ zu einer gegebenen rechten Seite $b \in \mathbb{R}^{n}$. Setze $\beta_{-1} := 0$ und berechne das Residuum $r^{0} = b - Ax^{0}$. \\
Für $k = 1,2,...$, falls $r^{k-1} \ne 0$ berechne:

\begin{eqnarray}
p^{k-1} &=& r^{k-1} + \beta_{k-2}p^{k-2}, \textnormal{ wobei } \beta_{k-2} = \frac {\langle r^{k-1}, r^{k-1} \rangle} {\langle r^{k-2}, r^{k-2} \rangle} \textnormal{ mit } (k \ge 2) \notag \\
x^{k} &=& x^{k-1} + \alpha_{k-1}p^{k-1}, \textnormal{ wobei } \alpha_{k-1} = \frac {\langle r^{k-1}, r^{k-1} \rangle} {\langle p^{k-1}, Ap^{k-1} \rangle} \notag \\
r^{k} &=& r^{k-1} - \alpha_{k-1}Ap^{k-1} \notag
\end{eqnarray}

\chapter{Mehrgitterverfahren}\label{c.Mehrgitterverfahren}

In diesem Abschnitt sollen nun die Mehrgittermethoden genauer betrachtet werden. Bevor wir jedoch genauer auf dieses Verfahren eingehen, wollen wir uns nochmal einige Erkenntnisse klar machen:

\section{Grundideen}

\begin{description}

\item[1.] Auslöschung hochfrequenter Fehler \\
Das Gauß-Seidel-Verfahren und das Jacobi-Verfahren löschen hochfrequente Fehler in den ersten Iterationsschritten aus. Niederfrequente Fehler werden nur sehr langsam beseitigt.(siehe \autoref{s.Das Jacobi-Iterationsverfahren} und \autoref{s.Das Gauss-Seidel-Verfahren})
\item[2.] Grobe Fehler nach einer Gittertransformation \\
Niedrig frequente Fehler auf einem feinen Gitter werden zu hochfrequenten Fehlern, wenn sie auf ein gröberes Gitter überführt werden.
\item[3.] Residuumsgleichung \\
Die für diesen Algorithmus wichtige Residuumsgleichung lautet:
\begin{equation}
\mat{A} e^{k} = r^{k}\label{eq.Residuumsgleichung}
\end{equation}
Die Lösung von $\mat{A} e^{k} = r^{k}$ ist äquivalent zur Lösung von $\mat{A}u=b$, wobei $e^{k} = 0$.

\end{description}

\subsection{Beweis der Residuumsgleichung}

Das Residuum ist an der $k-ten$ Stelle definiert als 
\begin{equation}
r^{k} = b - \mat{A}u^{k}
\end{equation}
Der Fehler
\begin{equation}
e = u^{*} - u^{k}\label{eq.Fehler}
\end{equation}
wobei $u^{*}$ die exakte Lösung darstellt, erfüllt ebenso folgende Gleichung:
\begin{equation}
\mat{A}e^{k} = \mat{A}(u^{*} - u^{k}) = \mat{A} u^{*} - \mat{A} u^{k} = b - \mat{A} u^{k} = r^{k}
\end{equation}
Wir kennen zwar den Fehler $e^{k}$ nicht, wissen aber, dass dieser $0$ ist, falls $r^{k} = 0$.
$\Longrightarrow$ Beh.

Gauss-Seidel- und Jacobi-Verfahren löschen also hochfrequente Fehler in den ersten Itertionsschritten aus. Um die nieder frequenten Fehler zu reduzieren sind allerdings wesentlich mehr Iterationsschritte notwendig. Auch aus diesem Grund finden beide Verfahren bei der Lösung großer, linearer Gleichungssysteme wenig Anwendung. \\
Auch wenn die hohe Anzahl an notwendigen Iterationen ein deutlicher Nachteil ist, wollen wir im folgenden die Vorteile dieser Methoden ausnutzen. \\%Die Mehrgittermethoden bedienen sich der Auslöschung hochfrequenter Fehler und liefern so eine gute und schnelle Approximation der Lösung von $\mat{A}u=b$. \\
Wie in\autoref{s.Finite Differenzen} gesehen befinden wir uns bei der Diskretisierung der Poisson-Gleichung auf einem Gebiet $\Omega_{h} = (0,1)^{2}$ der Schrittweite $h = \frac {1} {n}$. Nach der Ausführung von k-Iterationsschritten von Einzel- oder Gesamtschrittverfahren sind auf diesem Gitter die hochfrequenten Fehler $e^{k} = u^{*} - u^{k}$ verschwunden. Nun berechnet man im $k-ten$ Schritt das Residuum $r^{k}$ und führt für das äquivalente lineare Gleichungssystem $\mat{A}e^{k} = r^{k}$, wobei $e^{k} = 0$ gilt, $l$ Iterationsschritte aus. So erhalten wir eine Näherung des Fehlers $e^{k}$. \\
Stellt man \autoref{eq.Fehler} um, berechnet also $e^{k} + u^{k}$, so erhält man eine neue Näherung der exakten Lösung. \\
Kombiniert man dieses Vorgehen nun mit dem Wechsel zwischen zwei Gittern der Gitterweite $h$ und $2h$ so erhält man das Zweigitterverfahren:
  % und bringen dieses auf ein gröberes Gitter, z.B. $\Omega_{2h}$ mit der Schrittweite $2h$, könnten wir hier nun mit \autoref{eq.Residuumsgleichung} das Gleichungssystem mit $e_{2h} = 0$ lösen. Der niedrig frequente Fehler in $r^{k}$ wird in $r_{2h}$ so dann zu einem hochfrequenten auf $\Omega_{2h}$ werden. \\
% Führt man einige Iterationsschritte auf dem groben Gitter aus, bringt den verbleidenden Fehler $e_{2h}^{k}$ wieder zurück auf $\Omega_{h}$ und addiert diesen zur iterierten Lösung $u^{k}$ reduziert sich $e^{k}$. Diese Vorgehensweise führt man nun so lange durch, bis $e{k} \approx 0$.

\section{Der Zweigitter-Algorithmus}\label{s.Der Zweigitter-Algorithmus}

Der Zweigitter-Algorithmus findet in der Praxis zwar wenig Verwendung, allerdings wird die Idee dahinter die Basis für das Mehrgitter-Verfahren sein.

\begin{eqnarray}
\textbf{while } 				&u^{k} \ne u^{*} \notag \\
&\textit{pre-smooth } 			&\textnormal{Jacobi-/Gauss-Seidel-Steps} \notag \\
&\textit{calculate residual }	&r^{k} = b - Au^{k} \notag \\
&\textit{restrict } 			&r^{k}_{2h}=Rr^{k}_{h} \notag \\
&								&\mat{A}^{2h} = R\mat{A}^{h}P \notag \\
&\textit{set error }			&e^{0}_{2h} = 0 \notag \\
&\textit{solve direct }			&\mat{A}^{2h} e_{2h} = r^{k}_{2h} \notag \\
&\textit{prolongate }			&e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }			&u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth (optional) }	&\textnormal{Jacobi-/Gauss-Seidel-Steps} \notag \\
\textbf{end } \notag
\end{eqnarray}

Zunächst sei erwähnt, dass das Nachglätten optional ist, allerdings einige Vorteile bietet, auf die wir hier nicht genauer eingehen möchten. Bei der Implementierung sollte also auf Nachglättung geachtet werden. \\
Der klare Nachteil dieser Methode liegt natürlich im direkten Lösen der Residuumsgleichung. Wählen wir ein sehr feines Gebiet $\Omega_{h}$ mit $n = 256$, also $h = \frac {1} {256}$, so liefert das Gebiet $\Omega_{2h}$ immer noch ein Gleichungssystem der Dimension $127^{2}$. Ein System dieser Ordnung zu lösen erfordert Rechenaufwand, der hier nicht erwünscht ist. \\

\section{Der Mehrgitter-Algorithmus}

Eine bessere Methode ist, das Gitter immer feiner zu machen, bis das System direkt lösbar ist, um dann wieder auf das feinste Gitter zurück zu kehren. Genauer:

\begin{eqnarray}
\textbf{Multigrid }(u,b) 			& \notag \\
&\textbf{if }\textit{(finest grid) }&\textnormal{return } u_{finest grid}=\mat{A}^{-1}b \notag \\
&\textbf{else }						& \notag \\
&\textit{pre-smooth } 				&\textnormal{Jacobi-/Gauss-Seidel-Steps} \notag \\
&\textit{calculate residual }		&r^{k} = b - Au^{k} \notag \\
&\textit{restrict } 				&r^{k}_{2h}=Rr^{k}_{h} \notag \\
&									&\mat{A}^{2h} = R\mat{A}^{h}P \notag \\
&\textit{recursion }				&e^{k}_{2h} = \textbf{Multigrid }(0,r^{k}_{2h}) \notag \\
&\textit{prolongate }				&e^{k}_{h} = Pe^{k}_{2h} \notag \\
&\textit{add error }				&u^{k}_{h} = u^{k-1}_{h} + e^{k}_{h} \notag \\
&\textit{smooth }					&\textnormal{Jacobi-/Gauss-Seidel-Steps} \notag \\
&									&\textnormal{return } u_{h} \notag \\
\textbf{end}						& \notag
\end{eqnarray}

Nun wollen wir die Idee, die wir in \autoref{s.Der Zweigitter-Algorithmus} entwickelt haben formulieren. Hier unterscheiden wir dann zwischen zwei Methoden: dem \textbf{V-Zyklus} und dem \textbf{W-Zyklus}. \\
Bei einem V-Zyklus wird geht man pro Iterationsschritt vom feinsten zum gröbsten Gitter und zurück (siehe Bild bla). Bei einem W-Zyklus geht man vom feinsten auf das gröbste Gitter über, prolongiert 

\chapter{Ein Vergleich zwischen Iterativen- und Mehrgitter-Methoden}\label{c.Vergleich}

In diesem letzten Kapitel wollen wir die Iterativen-Verfahren, speziell das Verfahren der konjugierten Gradienten, und die Mehrgitterverfahren numerisch vergleichen. Dafür betrachten wir folgende Gleichung:

\section{Beispiel einer Poisson Gleichung}\label{s.Beispiel einer Poisson Gleichung}

Seien $f: \Omega \rightarrow \mathbb{R}$ und $g: \partial\Omega \rightarrow \mathbb{R}$ stetige Funktionen mit $f(x,y) = -4.$ und $g(x,y) = x^{2} + y^{2}$. Sei außerdem $\Omega = (0,1)\times(0,1) \in \mathbb{R}^{2}$.Gegeben ist das Randwertproblem
\begin{eqnarray}
	-\Delta u(x,y) &=& f(x,y) = -4 \textnormal{ in } \Omega \\
    u(x,y) &=& g(x,y) = x^{2} + y^{2} \textnormal{ in } \partial \Omega
\end{eqnarray}
Gesucht ist eine Funktion $u(x,y)$, die diese Gleichung löst. \\
Offensichtlich löst der elliptische Paraboloid $u(x,y) = x^{2} + y^{2}$ die partielle Differentialgleichung, da $\partial_{xx}u(x,y) = \partial_{yy}u(x,y) = 2$. Allerdings wollen wir nun diese Lösung auch numerisch erhalten.

\section{Jacobi-Verfahren angewandt auf das Beispiel}\label{s.Jacobi mit Beispiel}

\section{Gauss-Seidel-Verfahren angewandt auf das Beispiel}\label{s.Gauss-Seidel mit Beispiel}

\section{SOR-Verfahren angewandt auf das Beispiel}\label{s.SOR mit Beispiel}

\section{CG-Verfahren angewandt auf das Beispiel}\label{s.CG mit Beispiel}

\section{PCG-Verfahren angewandt auf das Beispiel}\label{s.PCG mit Beispiel}

\section{Das Mehrgitterverfahren angewandt auf das Beispiel}\label{s.Multigrid mit Beispiel}

\subsection{V-Zyklus}\label{ss.V-Zyklus mit Beispiel}

\subsection{W-Zyklus}\label{ss.W-Zyklus mit Beispiel}

\begin{table}[!hbt]\vspace{1ex}\centering\begin{tabular}{|l|l|}
\hline
Formen & Städte\\
\hline
\hline
Quadrat &  Bunkenstedt \\
\hline
Dreieck &  Laggenbeck\\
\hline
Kreis &  Peine\\
\hline
Raute & Wakaluba \\
\hline
\end{tabular}
\caption{\label{tab.sinnlos}eine sinnlose Tabelle}
\vspace{2ex}\end{table}

\begin{table}[!hbt]\vspace{1ex}\centering
\begin{tabular}{|ll||l|l|l|l|}\hline
\multicolumn{2}{|c||}{}&\multicolumn{4}{c|}{ dies} \\
\multicolumn{2}{|c||}{}& von dort  & und dort & über hier & zu Los \\\hline\hline
\multirow{3}*{\rotatebox{90}{das}} & hier &  bla  & bla  & bla  & bla \\\cline{2-6}
& dort & bla  & bla & bla  & bla  \\\cline{2-6}
& da &  bla  & bla & bla & bla \\\hline
\end{tabular}
\caption[eine kompliziertere Tabelle]{eine kompliziertere Tabelle mit viel Beschreibungstext, der aber nicht im Tabellenverzeichnis auftauschen soll}
\vspace{2ex}\end{table}

Er hörte leise Schritte hinter sich. Das bedeutete nichts Gutes. Wer würde ihm schon folgen, spät in der Nacht und dazu noch in dieser engen Gasse mitten im übel beleumundeten Hafenviertel? Gerade jetzt, wo er das Ding seines Lebens gedreht hatte und mit der Beute verschwinden wollte! Hatte einer seiner zahllosen Kollegen dieselbe Idee gehabt, ihn beobachtet und abgewartet, um ihn nun um die Früchte seiner Arbeit zu erleichtern? Oder gehörten die Schritte hinter ihm zu einem der unzähligen Gesetzeshüter dieser Stadt, und die stählerne Acht um seine Handgelenke würde gleich zuschnappen? Er konnte die Aufforderung stehen zu bleiben schon hören. Gehetzt sah er sich um. Plötzlich erblickte er den schmalen Durchgang. Blitzartig drehte er sich nach rechts und verschwand zwischen den beiden Gebäuden. Beinahe wäre er dabei über den umgestürzten Mülleimer gefallen, der mitten im Weg lag. Er versuchte, sich in der Dunkelheit seinen Weg zu ertasten und erstarrte: Anscheinend gab es keinen anderen Ausweg aus diesem kleinen Hof als den Durchgang, durch den er gekommen war. Die Schritte wurden lauter und lauter, er sah eine dunkle Gestalt um die Ecke biegen. Fieberhaft irrten seine Augen durch die nächtliche Dunkelheit und suchten einen Ausweg. War jetzt wirklich alles vorbei, waren alle Mühe und alle Vorbereitungen umsonst? Er presste sich ganz eng an die Wand hinter ihm und hoffte, der Verfolger würde ihn übersehen, als plötzlich neben ihm mit kaum wahrnehmbarem Quietschen eine Tür im nächtlichen Wind hin und her schwang. Könnte dieses der flehentlich herbeigesehnte Ausweg aus seinem Dilemma sein? Langsam bewegte er sich auf die offene Tür zu, immer dicht an die Mauer gepresst. Würde diese Tür seine Rettung werden?


% Anhang
\begin{landscape}\begin{multicols}{2}
\appendix
\chapter{Anhang}
\section{Quelltexte}
\subsubsection*{cpu.c aus Linux 2.6.16}\label{s.cpu}\lstinputlisting[language=C]{code/cpu.c}
\end{multicols}\end{landscape}


\bibliographystyle{alphadin_martin}
\bibliography{bibliographie}


\chapter*{Erklärung}

Hiermit versichere ich, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe, dass alle Stellen der Arbeit, die wörtlich oder sinngemäß aus anderen Quellen übernommen wurden, als solche kenntlich gemacht und dass die Arbeit in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde vorgelegt wurde.

\vspace{3cm}
Ort, Datum \hspace{5cm} Unterschrift\\

\end{document}